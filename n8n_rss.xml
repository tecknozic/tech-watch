<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[n8n Blog]]></title><description><![CDATA[Automate without limits]]></description><link>https://blog.n8n.io/</link><image><url>https://blog.n8n.io/favicon.png</url><title>n8n Blog</title><link>https://blog.n8n.io/</link></image><generator>Ghost 6.10</generator><lastBuildDate>Mon, 05 Jan 2026 11:28:00 GMT</lastBuildDate><atom:link href="https://blog.n8n.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[A practical guide to building your RAG pipeline in n8n]]></title><description><![CDATA[Explore how to build a full RAG pipeline in n8n without heavy frameworks. Compare code-first approaches to visual workflows for faster iteration and easier maintenance.]]></description><link>https://blog.n8n.io/rag-pipeline/</link><guid isPermaLink="false">694175713a6d780001f76436</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Nyior Clement]]></dc:creator><pubDate>Mon, 22 Dec 2025 13:51:25 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/12/workflow-template--2-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/12/workflow-template--2-.png" alt="A practical guide to building your RAG pipeline in n8n"><p>Building a RAG pipeline often starts with a simple goal, but quickly becomes harder than expected. A small feature can turn into a collection of services, scripts, and configuration files, where minor changes cause frequent failures. What should be an easy way to ground a model in your own data ends up buried under glue code and deployment overhead, making the core idea harder to work with.</p><p>This is where <a href="https://n8n.io/rag/">n8n, with its RAG capabilities</a>, becomes interesting.</p><p>You build the entire RAG pipeline in one visual workflow, choose your models and vector stores, and avoid glue code altogether. The result is a simpler, more reliable way to ground AI in your own data.</p><p>Sounds interesting? Let&#x2019;s take a closer look at how it works!</p><h2 id="why-does-rag-exist-in-the-first-place">Why does RAG exist in the first place?</h2><p>Before discussing the Retrieval-Augmented Generation (RAG) pipeline, it helps to ask a simple question: what exactly goes wrong when you use a foundation model on its own?</p><p>Most teams see familiar patterns:</p><ul><li>The model hallucinated details that do not match reality.</li><li>It did not know the internal data.</li><li>You could not easily update its knowledge without retraining a model.</li></ul><p>Imagine your company has product docs, support tickets, and internal guides. You ask a foundation model a question like &#x201C;Does our enterprise plan support SSO with provider X?&#x201D; The model has no idea what your plan actually includes, so it guesses based on patterns from the general internet. Sometimes it is close. Sometimes it is dangerously wrong.</p><p>You need a way to give the model fresh, trusted context when the question is asked. You also need a way to do this without retraining a model every time your documentation changes.</p><p>This is the idea behind RAG pipelines. </p><h2 id="what-is-a-rag-pipeline">What is a RAG pipeline?</h2><p>A RAG pipeline (Retrieval-Augmented Generation pipeline) is a system that helps an AI model answer questions using your own data, not just what it learned during training.</p><p>Instead of asking the model to &#x201C;know everything,&#x201D; you let it:</p><ol><li>Retrieve the most relevant pieces of your own data for a given question, on the fly.</li><li>Augment the prompt so the model answers with that context in front of it.</li></ol><p>You can think of it as a librarian for your language model. Ingestion is when you bring books into the library. Retrieval is the process of finding the right pages. Augmentation is where you hand those pages to the model.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In n8n, each of these stages exists as nodes in a single workflow, rather than scattered scripts across services.</div></div><h2 id="key-stages-of-a-rag-pipeline">Key stages of a RAG pipeline</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/rag-pipeline.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1920" height="1963" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/rag-pipeline.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/rag-pipeline.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/rag-pipeline.png 1600w, https://blog.n8n.io/content/images/2025/12/rag-pipeline.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Key stages of a RAG pipeline - n8n</span></figcaption></figure><h3 id="stage-1-data-ingestion">Stage 1: Data ingestion</h3><p>This stage answers the question &#x201C;What information should my model have access to?&#x201D;</p><p>Typical sources include product documentation, knowledge base articles, Notion pages, Confluence spaces, PDFs in cloud storage, or support tickets. During ingestion, you:</p><ol><li><strong>Load data:</strong> This is the stage where you connect to your chosen source and pull in the documents you want the system to work with. It could be files, pages, or any other text-based content your pipeline relies on.</li><li><strong>Split data:</strong> Long documents are split into smaller segments to make them easier for the model to process. These pieces are usually kept below a specific size, for example, around 500 characters, to make retrieval more precise.</li><li><strong>Embed data:</strong> Each chunk of text is transformed into a vector using an embedding model. This converts the text&apos;s meaning into a numerical form that the system can compare and work with.</li><li><strong>Store data:</strong> The vectors are then placed into a vector database. This allows users to quickly find the most relevant chunks when they ask a question.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/rag-ingestion.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/rag-ingestion.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/rag-ingestion.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/rag-ingestion.png 1600w, https://blog.n8n.io/content/images/2025/12/rag-ingestion.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n RAG: Stage 1: Data ingestion</span></figcaption></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In code-centric setups, this usually requires several scripts and services. In n8n, many of these steps exist as ready-to-use nodes.</div></div><h3 id="stage-2-retrieval-augmentation-and-generation">Stage 2: Retrieval, augmentation, and generation</h3><ol><li><strong>Retrieval:</strong> When a user asks a question, the system converts that question into a vector using the same embedding model used during ingestion. This query vector is then compared against all vectors in the database to find the closest matches. These matches represent the pieces of text most likely to contain helpful information for answering the question.</li><li><strong>Generation:</strong> The language model receives two things. The user&#x2019;s question and the relevant text retrieved from the vector database. It combines both inputs to produce a grounded response, using the retrieved information as context for the answer.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/rag-retrieval-generation.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/rag-retrieval-generation.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/rag-retrieval-generation.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/rag-retrieval-generation.png 1600w, https://blog.n8n.io/content/images/2025/12/rag-retrieval-generation.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n RAG: Stage 2: Retrieval, augmentation, and generation</span></figcaption></figure><h2 id="how-to-build-a-rag-pipeline-in-n8n">How to build a RAG pipeline in n8n?</h2><p>We&#x2019;ll use n8n to illustrate a practical, production-ready approach to building <a href="https://n8n.io/workflows/categories/ai-rag/"><u>RAG workflows</u></a>. Instead of focusing on isolated components, n8n lets us design the entire pipeline, from data ingestion and embeddings to retrieval, generation, and post-answer actions, in one place.</p><p>Here&apos;s an n8n workflow that listens for new or updated documents in Google Drive, processes them automatically, stores their embeddings in Pinecone, and uses Google&#x2019;s Gemini models to answer employee questions based on those documents. Everything lives inside one visual workflow. You configure it. You do not write boilerplate code.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(2753, document.currentScript);
</script>
<!--kg-card-end: html-->
<p>Here is what happens behind the scenes:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/data-src-image-f2306434-2e0a-4935-b080-667d6b0ab4af.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1600" height="808" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/data-src-image-f2306434-2e0a-4935-b080-667d6b0ab4af.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/data-src-image-f2306434-2e0a-4935-b080-667d6b0ab4af.png 1000w, https://blog.n8n.io/content/images/2025/12/data-src-image-f2306434-2e0a-4935-b080-667d6b0ab4af.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">How to build a RAG pipeline in n8n: Part 1</span></figcaption></figure><ol><li>Two <strong>Google Drive Trigger</strong> nodes monitor a folder. One detects new files, the other detects updates.</li><li>When a file is detected, a <strong>Google Drive</strong> node downloads it.</li><li>A <strong>Default Data Loader</strong> node extracts the document text.</li><li>A <strong>Recursive Character Text Splitter</strong> node breaks the content into smaller chunks for better retrieval.</li><li>A<strong> Google Gemini</strong> <strong>Embeddings</strong> node creates embeddings for each text chunk using the text-embedding-004 model.</li><li>A <strong>Pinecone Vector Store</strong> node indexes both the chunks and their embeddings into your Pinecone index.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/data-src-image-ffadb7a3-a4cb-404a-a821-406cf0458f3c.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1600" height="776" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/data-src-image-ffadb7a3-a4cb-404a-a821-406cf0458f3c.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/data-src-image-ffadb7a3-a4cb-404a-a821-406cf0458f3c.png 1000w, https://blog.n8n.io/content/images/2025/12/data-src-image-ffadb7a3-a4cb-404a-a821-406cf0458f3c.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">How to build a RAG pipeline in n8n: Part 2</span></figcaption></figure><ol start="7"><li>A <strong>Chat Trigger</strong> node receives employee questions.</li><li>The question is passed to an <strong>AI Agent</strong> node.</li><li>The agent uses a <strong>Vector Store Tool</strong> connected to Pinecone in query mode to retrieve relevant chunks.</li><li>The agent forwards the question and the retrieved chunks to the <strong>Google Gemini Chat Model</strong>.</li><li>Gemini generates a grounded response using the retrieved text.</li><li>A <strong>Window Buffer Memory</strong> node allows for short-term conversation memory, so follow-up questions feel natural.</li></ol><p>In short, the workflow keeps your document index up to date and uses it to power an intelligent, context-aware chatbot.</p><p>To run this RAG workflow in your own n8n instance, you will complete a few setup steps. Each step activates part of the pipeline you saw above.</p><h3 id="step-1-prepare-your-accounts">Step 1: Prepare your accounts</h3><p>You will need three services set up.</p><p><strong>Google Cloud Project and Vertex AI API</strong></p><ul><li>Create a Google Cloud project &#x2192;<a href="https://console.cloud.google.com/projectcreaten8n"> <u>https://console.cloud.google.com/projectcreate</u></a></li><li>Enable required services inside the project:<ul><li>The Vertex AI API (for embeddings and chat models).</li><li>Enable the Google Drive API (for loading and monitoring documents from Drive).</li></ul></li></ul><p>If all goes well, you should see the Vertex AI API and the Google Drive API in your list of enabled services.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/data-src-image-63787f78-5f0d-494d-8ea8-1ce24a73d7b7.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1600" height="839" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/data-src-image-63787f78-5f0d-494d-8ea8-1ce24a73d7b7.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/data-src-image-63787f78-5f0d-494d-8ea8-1ce24a73d7b7.png 1000w, https://blog.n8n.io/content/images/2025/12/data-src-image-63787f78-5f0d-494d-8ea8-1ce24a73d7b7.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Google Cloud Project and Vertex AI API</span></figcaption></figure><p><strong>Google AI API key</strong></p><ul><li>Get your API key from Google AI Studio &#x2192;<a href="https://aistudio.google.com/"> <u>https://aistudio.google.com/</u></a></li><li>This key will authenticate all Gemini model calls from n8n.</li></ul><p><strong>Google Drive OAuth2 credentials</strong></p><ul><li>In your Google Cloud project, create an OAuth2 Client ID.</li><li>Add the correct redirect URI for your n8n instance.</li><li>Use this OAuth2 credential in n8n to give the workflow permission to read your Google Drive folder.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/data-src-image-4ce8bf78-5d4a-442d-bce6-7e078d8195cc.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="1600" height="739" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/data-src-image-4ce8bf78-5d4a-442d-bce6-7e078d8195cc.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/data-src-image-4ce8bf78-5d4a-442d-bce6-7e078d8195cc.png 1000w, https://blog.n8n.io/content/images/2025/12/data-src-image-4ce8bf78-5d4a-442d-bce6-7e078d8195cc.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Google Drive OAuth2 credentials</span></figcaption></figure><p><strong>Pinecone account</strong></p><ul><li>Create a free Pinecone account &#x2192;<a href="https://www.pinecone.io"> <u>https://www.pinecone.io</u></a></li><li>You will see an existing default API key. Copy it.</li><li>Create an index named company-files to store your embeddings and text chunks.</li></ul><h3 id="step-2-prepare-your-google-drive-folder">Step 2: Prepare your Google Drive folder</h3><p>Create a dedicated folder in Google Drive. This folder will hold all the documents your chatbot should use as references. The workflow will automatically monitor this folder.</p><h3 id="step-3-add-your-credentials-to-n8n">Step 3: Add your credentials to n8n</h3><p>Before the workflow can run, n8n needs permission to talk to external services. You do this by creating credentials. Generally, to add any credential in n8n:</p><ol><li>Open your n8n instance.</li><li>Click <strong>Create</strong> <strong>credential</strong>.</li><li>Select the service you want to connect to. In the screenshot below, I selected the <strong>Google Drive OAuth2 </strong>service.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/n8n-new-credential.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="879" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/n8n-new-credential.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/n8n-new-credential.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/n8n-new-credential.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/n8n-new-credential.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Select the service you want to connect to</span></figcaption></figure><p>For this guide, we need to create credentials for three services:</p><ol><li><strong><u>Google Drive OAuth2</u></strong></li></ol><p>This credential allows n8n to read and monitor files in your Google Drive.</p><ul><li>Create a new credential of type <strong>Google Drive OAuth2, </strong>following the steps above.</li><li>Enter the <strong>Client ID</strong> and <strong>Client Secret</strong> from your Google Cloud project.</li><li>Click <strong>Connect</strong> and complete the Google authorisation flow.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/google-oauth2-api-credentials.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="1193" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/google-oauth2-api-credentials.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/google-oauth2-api-credentials.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/google-oauth2-api-credentials.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/google-oauth2-api-credentials.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Google Drive OAuth2 API credentials - n8n</span></figcaption></figure><ol start="2"><li><strong><u>Google Gemini (PaLM) API</u></strong></li></ol><p>This credential is used for embeddings and chat generation with Gemini models.</p><ul><li>Create a new credential of type <strong>Google Gemini (PaLM) API</strong>.</li><li>Paste your <strong>Google AI API key</strong> from Google AI Studio.</li><li>Save the credential.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/google-gemini-credential.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="1193" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/google-gemini-credential.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/google-gemini-credential.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/google-gemini-credential.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/google-gemini-credential.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Google Gemini (PaLM) API credential - n8n</span></figcaption></figure><ol start="3"><li><strong><u>Pinecone API</u></strong></li></ol><p>This credential allows n8n to store and retrieve vectors from your Pinecone index.</p><ul><li>Create a new credential of type <strong>Pinecone API</strong>.</li><li>Paste your <strong>Pinecone API key</strong>.</li><li>Save the credential.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/pinecone-credential.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="1193" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/pinecone-credential.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/pinecone-credential.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/pinecone-credential.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/pinecone-credential.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Pinecone API credential - n8n</span></figcaption></figure><p>Once created, you can select the credential from any compatible node. For a deeper overview, see the <a href="https://docs.n8n.io/integrations/builtin/credentials/" rel="noreferrer">official n8n documentation on credentials</a>.</p><h3 id="step-4-import-the-rag-workflow">Step 4: Import the RAG workflow</h3><p><a href="https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/"><u>Download or copy the workflow</u></a> and import it into your n8n instance. You will now see the full RAG pipeline inside the editor with all nodes connected.</p><h3 id="step-5-configure-the-nodes">Step 5: Configure the nodes</h3><p>Make the workflow yours by updating a few nodes.</p><ul><li>Update both <strong>Google Drive Trigger</strong> nodes to watch the folder you created.</li><li>Open the <strong>Pinecone Vector Store</strong> nodes and point them to your company-files index.</li><li>Confirm the embedding model settings in the <strong>Embeddings Google Gemini</strong> node.</li><li>Confirm the chat model selection in the <strong>Google Gemini Chat Model</strong> node.</li></ul><p>At this point, your workflow is fully wired to your accounts, your Drive folder, and your vector index.</p><h3 id="step-6-test-the-rag-pipeline">Step 6: Test the RAG pipeline</h3><p>Add or update a document in your Google Drive folder to trigger the indexing flow. Then ask a question through the chat entry point and observe how the agent retrieves relevant text and generates an answer. Every step is visible in n8n, so you can easily inspect and debug.</p><h3 id="step-7-activate-the-workflow">Step 7: Activate the workflow</h3><p>Enable the workflow in n8n Cloud or run it in your self-hosted environment. Your RAG chatbot is now live, indexing new company documents automatically and answering employee questions with grounded, up-to-date information.</p><h2 id="what-are-5-rag-pipeline-examples-in-n8n">What are 5 RAG pipeline examples in n8n?</h2><p>Now that you&#x2019;ve seen how a RAG pipeline fits together in n8n, it helps to look at real examples. The following workflows show different ways teams use RAG in practice, from simple starters to more advanced, automated setups.</p><h3 id="rag-starter-template-using-simple-vector-stores-and-form-trigger">RAG starter template using simple vector stores and form trigger</h3><p><a href="https://n8n.io/workflows/5010-rag-starter-template-using-simple-vector-stores-form-trigger-and-openai"><u>A beginner-friendly RAG workflow</u></a> that shows how to give an agent knowledge from a PDF or document. Upload a file, generate embeddings, and start chatting with your content using a simple vector store:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/screenshot-JTV0QNYw-1.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="953" height="523" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/screenshot-JTV0QNYw-1.png 600w, https://blog.n8n.io/content/images/2025/12/screenshot-JTV0QNYw-1.png 953w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n: RAG starter template using simple vector stores and form trigger</span></figcaption></figure><h3 id="build-custom-workflows-automatically-with-gpt-4o-rag-and-web-search">Build custom workflows automatically with GPT-4o, RAG, and web search</h3><p><a href="https://n8n.io/workflows/5024-build-custom-workflows-automatically-with-gpt-4o-rag-and-web-search"><u>This template shows</u></a> how to convert a one-line request into an automated n8n workflow with RAG and web search capabilities. It&#x2019;s ideal for quickly prototyping complex automations with little manual wiring.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/custom-workflows.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="1476" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/custom-workflows.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/custom-workflows.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/custom-workflows.png 1600w, https://blog.n8n.io/content/images/2025/12/custom-workflows.png 2122w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n: Template for custom workflows with GPT-4o, RAG and web search</span></figcaption></figure><h3 id="create-a-documentation-expert-bot-with-rag-gemini-and-supabase">Create a documentation expert bot with RAG, Gemini, and Supabase</h3><p><a href="https://n8n.io/workflows/5993-create-a-documentation-expert-bot-with-rag-gemini-and-supabase"><u>A hands-on workflow</u></a> that builds a RAG chatbot knowledgeable about a specific topic by indexing documentation and serving as an &#x201C;expert librarian&#x201D; that answers questions with grounded context.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/documentation-expert-bot.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="1433" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/documentation-expert-bot.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/documentation-expert-bot.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/documentation-expert-bot.png 1600w, https://blog.n8n.io/content/images/2025/12/documentation-expert-bot.png 2122w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n: Workflow template for a documentation expert bot</span></figcaption></figure><h3 id="basic-rag-chat">Basic RAG chat</h3><p><a href="https://n8n.io/workflows/5028-basic-rag-chat/"><u>A simpler RAG example</u></a> that demonstrates an end-to-end pipeline using an in-memory vector store for quick prototyping. It shows data ingestion, embeddings with an external provider, retrieval, and chat generation.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/simple-rag-example.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="880" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/simple-rag-example.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/simple-rag-example.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/simple-rag-example.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/simple-rag-example.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n: A simple RAG workflow template</span></figcaption></figure><h3 id="local-chatbot-with-retrieval-augmented-generation-rag">Local chatbot with Retrieval Augmented Generation (RAG)</h3><p><a href="https://n8n.io/workflows/5148-local-chatbot-with-retrieval-augmented-generation-rag"><u>This workflow</u></a> shows how to run a fully local RAG chatbot using n8n with Ollama and Qdrant. It ingests PDF files into Qdrant, retrieves relevant chunks at query time, and answers using a local model, which is useful when you want RAG without sending data to external APIs.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/local-rag-chatbot.png" class="kg-image" alt="A practical guide to building your RAG pipeline in n8n" loading="lazy" width="2000" height="880" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/local-rag-chatbot.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/local-rag-chatbot.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/local-rag-chatbot.png 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/local-rag-chatbot.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n: Workflow template for a local RAG chatbot with n9n, Ollama and Qdrant</span></figcaption></figure><h2 id="benefits-and-challenges-of-rag">Benefits and challenges of RAG</h2><p>RAG offers clear benefits, from reducing hallucinations to making knowledge reusable across teams, but it also introduces new challenges around data quality, performance, and security. Understanding these trade-offs is essential before building, and n8n provides a practical way to manage them in one system.</p><h3 id="benefits">Benefits</h3><ul><li>RAG reduces hallucinations by grounding answers in your real data.</li><li>It enables easy updates without retraining.</li><li>It makes your knowledge reusable by letting multiple teams pull from the same indexed documents.&#xA0;</li><li>It speeds up experimentation by letting you change models or data sources without rewriting code.</li></ul><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In n8n, these benefits are amplified because the entire pipeline lives in a single interface.</div></div><h3 id="challenges">Challenges</h3><ul><li>RAG depends on the quality of your data.</li><li>Chunking and retrieval may need tuning when your documents vary in structure or when the retrieved text is not specific enough to answer the question.</li><li>The pipeline can introduce latency when documents are large or when your vector store is slow to respond.</li><li>You must also consider security because your embeddings and stored chunks may contain sensitive internal information that must be protected.</li></ul><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">n8n gives you a single place to observe and adjust these trade-offs.</div></div><h2 id="frequently-asked-questions-about-rag-pipelines">Frequently asked questions about RAG pipelines</h2><h3 id="how-does-a-rag-pipeline-in-langchain-compare-to-building-one-in-n8n">How does a RAG pipeline in LangChain compare to building one in n8n?</h3><p>LangChain is excellent when you want complete control through code. It gives you fine-grained tools for chunking, embedding, retrieval, and orchestration. n8n gives you the same core pattern in a visual flow, with little to no code.</p><h3 id="can-i-still-use-python-if-i-build-my-rag-pipeline-in-n8n">Can I still use Python if I build my RAG pipeline in n8n?</h3><p>Yes. You can keep Python for the pieces that genuinely need it. n8n takes over the routine parts of ingestion, embeddings, vector search, and model calls, so you write fewer maintenance scripts. When a custom transformation or scoring function is needed, you can use the <strong>Code</strong> node to run a small Python snippet and feed the result back into the workflow.</p><h3 id="do-i-need-code-at-all-to-build-a-rag-pipeline">Do I need code at all to build a RAG pipeline?</h3><p>You do not need code to build the core pipeline. Ingestion, splitting, embeddings, vector storage, retrieval, prompting, and generation can all run visually in n8n. Code becomes optional. You add it only for advanced logic specific to your organisation.</p><h3 id="how-does-a-haystack-based-rag-pipeline-fit-with-n8n">How does a Haystack-based RAG pipeline fit with n8n?</h3><p>Haystack is a strong framework for retrieval, ranking, and search in Python. You can keep Haystack for specific retrieval logic and let n8n handle the surrounding orchestration. n8n can trigger Haystack jobs, pass documents or queries into the pipeline, handle retries, and connect results to downstream systems. Some teams replace Haystack entirely with visual nodes to simplify maintenance.</p><h2 id="wrap-up">Wrap up</h2><p>RAG exists because foundation models alone cannot reliably answer questions about your internal data.</p><p>In code-heavy setups, a RAG pipeline requires many custom services and scripts. In n8n, you use ready templates and visual nodes to build and deploy a RAG pipeline with little or no boilerplate code. You keep control, clarity, and flexibility without drowning in setup.</p>
<!--kg-card-begin: html-->
<div class="content-banner">
  <div>
    <h3>Create your own RAG workflows</h3>
    <p>Build, test, and deploy grounded AI workflows faster&#x2014;without maintaining scattered services or boilerplate code</p>
  </div>
  <a href="https://app.n8n.cloud/register" class="global-button blog-banner-signup">Try n8n now</a>
</div>
<!--kg-card-end: html-->
<h2 id="whats-next">What&apos;s next?</h2><p>If you want to dive deeper into RAG pipelines and n8n, please <a href="https://docs.n8n.io/advanced-ai/rag-in-n8n/"><u>see our RAG documentation</u></a>. Additionally, the resources below go beyond the basics. They walk you through full pipelines, show real setups, and explore more advanced automation patterns.</p><ul><li>There&#x2019;s a broader shift toward <a href="https://blog.n8n.io/agentic-rag/" rel="noreferrer"><strong>agentic RAG</strong></a><strong> workflows </strong>&#x2014; systems that don&#x2019;t just retrieve and answer, but verify, refine, and improve their own results. This guide focuses on the foundation, but once that&#x2019;s stable, the next step is teaching your pipeline to evaluate and strengthen its own output.</li><li><a href="https://blog.n8n.io/rag-chatbot/"><u>Build custom RAG chatbots with n8n</u>: </a>A detailed article that explains how to connect any knowledge source, index it in a vector database, and build an AI-powered chatbot using n8n&#x2019;s visual workflows.</li><li><a href="https://n8n.io/workflows/4552-index-documents-from-google-drive-to-pinecone-with-openai-embeddings-for-rag"><u>Index documents from Google Drive to Pinecone with n8n</u></a>: A ready-to-use workflow template that watches a Drive folder and automatically indexes new files into a Pinecone vector store. Great starting point for doc-based RAG systems.</li><li><a href="https://www.youtube.com/watch?v=FlGBvZ-6R-Q"><u>Creating a RAG Agent in n8n for Beginners</u></a>: A comprehensive step-by-step guide.</li></ul><p>The best RAG pipeline is the one shaped by your data and your needs. These resources give you a toolkit for building, improving, and scaling. n8n makes it possible, without overwhelming boilerplate!</p><p></p>]]></content:encoded></item><item><title><![CDATA[Multi-agent systems: Frameworks & step-by-step tutorial]]></title><description><![CDATA[Discover multi-agent AI patterns, communication, costs, risks, and real-world use cases. Compare visual builders like n8n with code-first SDKs.]]></description><link>https://blog.n8n.io/multi-agent-systems/</link><guid isPermaLink="false">692e13ac4c216b0001b12dc2</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Yulia Dmitrievna]]></dc:creator><pubDate>Mon, 22 Dec 2025 13:11:34 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/12/multi-agent-systems-b--1-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/12/multi-agent-systems-b--1-.jpg" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial"><p>In our <a href="https://blog.n8n.io/ai-agent-orchestration-frameworks/"><u>previous article on AI agent orchestration frameworks</u></a>, we explored why multi-agent systems work: specialized agents perform certain tasks better than a generalist.</p><p>However, this specialization comes at a price. <a href="https://www.anthropic.com/engineering/multi-agent-research-system"><u>Research by Antropic</u></a> shows that multi-agent systems outperformed single agents by 90.2%. They also consumed 15&#xD7; more tokens. Token usage alone explained 80% of the performance differences in Anthropic&apos;s internal tests.</p><p>The trade-off is real. Multi-agent systems quickly burn through API budgets, coordination becomes complex, and debugging becomes more difficult.</p><p>So when does it make sense?</p><ul><li>when your task involves multiple domains that require deep expertise,</li><li>when you need parallel processing across different data sources,</li><li>when a single context window can&apos;t hold everything</li></ul><p>This guide breaks down multi-agent system architectures: what they are, how they work, when to use them, and which frameworks support different patterns. We&apos;ll cover real-world applications, common failure modes, and practical implementation in <a href="https://n8n.io/ai/"><u>n8n</u></a>.</p><ul>
<li><a href="#what-is-a-multi-agent-system">What is a multi-agent system?</a>
<ul>
<li><a href="#whats-the-difference-between-multi-agent-ai-and-single-agent-ai">What&apos;s the difference between multi-agent AI and single-agent AI?</a></li>
</ul>
</li>
<li><a href="#how-do-multi-agent-systems-work">How do multi-agent systems work?</a>
<ul>
<li><a href="#how-do-agents-communicate-in-multi-agent-systems">How do agents communicate in multi-agent systems?</a></li>
</ul>
</li>
<li><a href="#examples-of-multi-agent-systems-applications">Examples of multi-agent systems applications</a></li>
<li><a href="#popular-frameworks-for-multi-agent-systems">Popular frameworks for multi-agent systems</a>
<ul>
<li><a href="#visual-builders-and-low-code-platforms">Visual builders and low-code platforms</a></li>
<li><a href="#code-first-frameworks-and-sdks">Code-first frameworks and SDKs</a></li>
</ul>
</li>
<li><a href="#how-to-build-a-multi-agent-system-in-n8n">How to build a multi-agent system in n8n?</a>
<ul>
<li><a href="#step-1-set-up-the-main-agent-as-the-coordinator">Step 1. Set up the main agent as the coordinator</a></li>
<li><a href="#step-2-add-and-configure-the-email-sub-agent">Step 2. Add and configure the email sub-agent</a></li>
<li><a href="#step-3-build-a-rag-sub-agent-to-modularize-data-access">Step 3. Build a RAG sub-agent to modularize data access</a></li>
<li><a href="#step-4-optimize-task-handover-between-agents">Step 4. Optimize task handover between agents</a></li>
</ul>
</li>
<li><a href="#advantages-of-multi-agent-systems">Advantages of multi-agent systems</a>
<ul>
<li><a href="#task-specialization-reduces-token-consumption">Task specialization reduces token consumption</a></li>
<li><a href="#parallel-execution-improves-throughput">Parallel execution improves throughput</a></li>
<li><a href="#isolated-failures-improve-reliability">Isolated failures improve reliability</a></li>
<li><a href="#modular-updates-reduce-deployment-risk">Modular updates reduce deployment risk</a></li>
</ul>
</li>
<li><a href="#challenges-of-multi-agent-systems">Challenges of multi-agent systems</a>
<ul>
<li><a href="#coordination-overhead-scales-with-agent-count">Coordination overhead scales with agent count</a></li>
<li><a href="#token-costs-multiply-across-the-system">Token costs multiply across the system</a></li>
<li><a href="#quality-drift-compounds-through-agent-chains">Quality drift compounds through agent chains</a></li>
<li><a href="#security-depends-on-your-threat-model">Security depends on your threat model</a></li>
</ul>
</li>
<li><a href="#faq">FAQ</a>
<ul>
<li><a href="#faq-1">What are common multi-agent architecture patterns?</a></li>
<li><a href="#faq-2">How do I evaluate multi-agent system performance?</a></li>
<li><a href="#faq-3">What are the best tools for building multi-agent systems?</a></li>
<li><a href="#faq-4">When should I use multi-agent instead of single-agent systems?</a></li>
<li><a href="#faq-5">What security risks do multi-agent systems face?</a></li>
</ul>
</li>
<li><a href="#wrap-up">Wrap up</a></li>
<li><a href="#what%E2%80%99s-next">What&#x2019;s next?</a></li>
</ul>
<h2 id="what-is-a-multi-agent-system">What is a multi-agent system?</h2><p>A multi-agent system (MAS) consists of several autonomous AI agents that interact within a shared environment to accomplish tasks. Each agent specializes in a specific domain &#x2013; data analysis, content generation, API integration &#x2013; rather than one agent handling everything.</p><p>These agents coordinate through communication protocols, share context through memory systems, and hand off tasks based on their specialization.</p><h3 id="whats-the-difference-between-multi-agent-ai-and-single-agent-ai">What&apos;s the difference between multi-agent AI and single-agent AI?</h3><p>Single agents use one model with one system prompt. Multi-agent systems distribute work across specialized agents with different models, prompts, and tools. Trade-offs: multi-agent offers better specialization and parallel execution but requires coordination logic and uses more tokens.</p><table>
<thead>
<tr>
<th>Aspect</th>
<th>Single-agent</th>
<th>Multi-agent</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Architecture</strong></td>
<td>Monolithic</td>
<td>Distributed</td>
</tr>
<tr>
<td><strong>Specialization</strong></td>
<td>Generalist</td>
<td>Multiple specialists</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Limited (vertical only)</td>
<td>High (horizontal scaling)</td>
</tr>
<tr>
<td><strong>Cost</strong></td>
<td>Requires expensive models</td>
<td>Mix of model sizes but more tokens</td>
</tr>
<tr>
<td><strong>Failure Mode</strong></td>
<td>Single point of failure</td>
<td>Isolated failures</td>
</tr>
</tbody>
</table>
<p>Instead of packing more instructions into one system prompt, you build specialized agents that excel at narrow tasks and coordinate when necessary.</p><h2 id="how-do-multi-agent-systems-work">How do multi-agent systems work?</h2><p>Individual agents still follow the basic <a href="https://blog.n8n.io/ai-agents/#how-do-ai-agents-work"><u>perception &#x2192; decision &#x2192; action cycle</u></a>. Multi-agent systems add a coordination layer on top.</p><p>To see how this works in practice, let&apos;s look at a customer support system:</p><ol><li>The router agent reads the incoming message</li><li>Based on keywords, it determines that it&#x2019;s a billing question</li><li>It forwards the message with the full conversation context to the billing specialist</li><li>The billing agent queries the database, checks the account status</li><li>It generates a response and forwards it to the email agent</li><li>The email agent formats the message and sends the email back to the customer</li></ol><p>This requires three critical additions that go beyond the capabilities of a single agent:</p><p><strong>Agent-to-agent communication</strong>: passing data and context between specialized agents without loss of information.</p><p><strong>Shared memory</strong>: maintaining state across handoffs so the billing agent knows what the router agent has already discussed.</p><p><strong>Orchestration logic</strong>: deciding which agent processes what, when to hand off, and how to merge results from multiple agents.</p><h3 id="how-do-agents-communicate-in-multi-agent-systems">How do agents communicate in multi-agent systems?</h3><p>Agents can coordinate through standardized protocols or framework-specific methods:</p><ul><li><strong>Model Context Protocol (MCP)</strong>: developed by Anthropic, standardizes how agents access tools and external resources</li><li><strong>Agent-to-Agent (A2A)</strong>: Google&apos;s protocol for peer-to-peer agent collaboration</li><li><strong>Custom approaches</strong>: framework-specific communication like <a href="https://langchain-opentutorial.gitbook.io/langchain-opentutorial/17-langgraph/02-structures/08-langgraph-multi-agent-structures-01#hands-off"><u>LangGraph&apos;s state handover</u></a> or <a href="https://docs.crewai.com/en/concepts/collaboration#how-agent-collaboration-works"><u>task delegation from CrewAI</u></a></li></ul><p>Most production systems use a mix of standard protocols for tool access and custom logic for workflow-specific coordination.Communication can be synchronous (agent waits for response) or asynchronous (message queues), depending on the architecture pattern.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The way agents coordinate depends on your workflow pattern. Learn about <a href="https://blog.n8n.io/ai-agentic-workflows/"><u>4 practical AI agentic workflow patterns in n8n</u></a> &#x2013; from simple chained requests to complex multi-agent teams with distributed decision-making. Different coordination approaches are suitable for different problems.</div></div><h2 id="examples-of-multi-agent-systems-applications">Examples of multi-agent systems applications</h2><p>The coordination mechanisms we just discussed - agent-to-agent communication, shared memory, orchestration logic - are already working in production systems.</p><p>Multi-agent architecture is increasingly a built-in feature rather than something you have to build from scratch. Instead of discussing theoretical examples, we&#x2019;ve focused on systems that you can access today as well as research with proven implementations.</p><p>Here are some common categories where AI multi-agent systems already exist:</p><ul><li><strong>Customer support</strong>: platforms route inquiries through specialized agents: well-known examples include <a href="https://www.intercom.com/blog/headlines-from-pioneer-2025/"><u>Intercom Fin 3</u></a>, <a href="https://respond.io"><u>Respond.io</u></a>, <a href="https://inkeep.com/blog/AI-Customer-Experience"><u>Inkeep</u></a>.</li><li><strong>Deep research</strong>: these systems parallelize information gathering with the subsequent&#xA0; re-ranking / summary: <a href="https://www.perplexity.ai/api-platform/resources/architecting-and-evaluating-an-ai-first-search-api"><u>Perplexity</u></a>, <a href="https://github.com/assafelovic/gpt-researcher"><u>GPT Researcher</u></a>, and <a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"><u>Tongyi Deep Research</u></a>.</li><li><strong>Software development</strong>: <a href="https://cursor.com/blog/2-0"><u>Cursor 2.0</u></a> runs up to 8 parallel coding agents, <a href="https://dev.to/bredmond1019/multi-agent-orchestration-running-10-claude-instances-in-parallel-part-3-29da"><u>Claude Code</u></a> enables 10+ simultaneous instances for coordinated development.</li><li><strong>Data analytics</strong>: organizations deploy agents that query databases on behalf of users. <a href="https://www.firstround.com/ai/shopify"><u>Shopify built internal tools</u></a> using LibreChat with 30+ MCP servers. <a href="https://chat.cbioportal.org/"><u>cBioAgent</u></a> enables researchers to query cancer genomics through plain text using a similar tech stack.</li><li><strong>Content creation</strong>: research papers show sequential refinement (<a href="https://arxiv.org/abs/2509.10761"><u>EditDuet</u></a>) and 4-agent pipelines (<a href="https://huggingface.co/papers/2506.10540"><u>AniMaker</u></a>) for video and animation production.</li></ul><table>
<thead>
<tr>
<th>Use Case</th>
<th>Application</th>
<th>Pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Customer support</strong></td>
<td>Intercom Fin 3</td>
<td>Procedures + Simulations</td>
</tr>
<tr>
<td></td>
<td>Respond.io</td>
<td>Role-based routing</td>
</tr>
<tr>
<td><strong>Research</strong></td>
<td>Perplexity</td>
<td>Parallel retrieval</td>
</tr>
<tr>
<td></td>
<td>GPT Researcher</td>
<td>Planner + executor</td>
</tr>
<tr>
<td></td>
<td>Tongyi</td>
<td>Hierarchical agents</td>
</tr>
<tr>
<td><strong>Software development</strong></td>
<td>Cursor 2.0</td>
<td>Up to 8 parallel agents</td>
</tr>
<tr>
<td></td>
<td>Claude Code</td>
<td>Multi-instance</td>
</tr>
<tr>
<td><strong>Data analytics</strong><br><strong>(LibreChat examples)</strong></td>
<td>Shopify</td>
<td>30+ MCP tool servers</td>
</tr>
<tr>
<td></td>
<td>cBioPortal</td>
<td>Database query agents</td>
</tr>
<tr>
<td></td>
<td>Fetch FAST</td>
<td>BI intelligence agents</td>
</tr>
<tr>
<td><strong>Content creation</strong></td>
<td>EditDuet</td>
<td>Editor + Critic</td>
</tr>
<tr>
<td></td>
<td>AniMaker</td>
<td>4-agent pipeline</td>
</tr>
</tbody>
</table>
<p>These examples show three recurring coordination patterns:</p><ul><li><strong>Handoff-based</strong>: specialized agents pass on the context between stages (customer support and data analytics)</li><li><strong>Parallel execution</strong>: multiple agents work simultaneously and then combine the results (research, software development)</li><li><strong>Sequential refinement</strong>: agents process in stages, each building on the previous output (content creation)</li></ul><h2 id="popular-frameworks-for-multi-agent-systems">Popular frameworks for multi-agent systems</h2><p>Now that you&apos;ve seen what multi-agent systems can accomplish, let&apos;s look at how you can build them. We can divide the landscape of possible solutions into two categories: visual builders for rapid development and code-first frameworks for detailed control.</p><h3 id="visual-builders-and-low-code-platforms">Visual builders and low-code platforms</h3><p>These platforms let you design agent workflows using graphical interfaces. Some offer a code fallback when visual tools hit limits.</p><table>
<thead>
<tr>
<th>Builder</th>
<th>Overview</th>
<th>MAS use case examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>n8n</strong></td>
<td>Hybrid low-code/full-code<br>platform with 1000+<br>integrations and MCP support.<br>Visual workflows with JavaScript<br>customization when needed.</td>
<td>Customer support routing,<br>document processing<br>pipelines, data enrichment<br>workflows and much more</td>
</tr>
<tr>
<td><strong>Flowise</strong></td>
<td>Visual builder on<br>LangChain/LlamaIndex with<br>Agentflow for multi-agent<br>systems. Quick prototyping<br>with RAG capabilities.</td>
<td>Chatbot prototypes, RAG<br>applications, LangChain<br>workflow visualization</td>
</tr>
<tr>
<td><strong>Zapier Agents</strong></td>
<td>No-code extension of Zapier&apos;s<br>8000+ app ecosystem. Limited<br>to prompting, no code<br>customization.</td>
<td>Simple business automation,<br>data syncing between apps,<br>scheduled tasks</td>
</tr>
<tr>
<td><strong>OpenAI AgentKit</strong></td>
<td>Emerging product based on<br>OpenAI Agents SDK. Combines<br>visual builder interface with<br>SDK export for self-hosting.<br>OpenAI models only.</td>
<td>OpenAI-native applications,<br>quick agent prototyping<br>with SDK flexibility</td>
</tr>
<tr>
<td><strong>Vertex AI Agent Builder</strong></td>
<td>Google Cloud managed<br>platform with no-code<br>interface and enterprise<br>data integration.</td>
<td>Google Cloud workflows,<br>enterprise RAG,<br>Gemini-based agents</td>
</tr>
</tbody>
</table>
<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Visual tools work well when you need fast iteration, have non-developer team members involved, or want to combine AI agents with existing business automation.</div></div><h3 id="code-first-frameworks-and-sdks">Code-first frameworks and SDKs</h3><p>These frameworks enable you to programmatically control agent behavior, state management, and coordination patterns. Better suited for complex custom logic.</p><table>
<thead>
<tr>
<th>Framework</th>
<th>Overview</th>
<th>MAS use case examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LangGraph</strong></td>
<td>Graph-based state management with<br>explicit control over agent workflows.<br>Advanced checkpointing and<br>human-in-the-loop.</td>
<td>Complex multi-step workflows,<br>conditional routing, state-dependent<br>agent coordination</td>
</tr>
<tr>
<td><strong>CrewAI</strong></td>
<td>Role-based teams framework<br>independent of LangChain. Crews<br>(autonomous) + Flows (event-driven).</td>
<td>Collaborative research teams,<br>content creation pipelines,<br>sequential task execution</td>
</tr>
<tr>
<td><strong>AutoGen</strong></td>
<td>Conversational multi-agent across<br>Python/C#/Java/JS. Group chat<br>capabilities with integrated code<br>execution.</td>
<td>Code generation systems,<br>conversational debugging, peer<br>agent collaboration</td>
</tr>
<tr>
<td><strong>Google ADK</strong></td>
<td>Workflow-based framework with A2A<br>protocol support and native Vertex AI<br>integration.</td>
<td>Google Cloud workflows, sequential/<br>parallel patterns, loop-based<br>processing</td>
</tr>
<tr>
<td><strong>Semantic<br>Kernel Agent<br>Framework</strong></td>
<td>Skill-based architecture for<br>C#/Python/Java with Azure<br>integration. Hierarchical agent<br>patterns.</td>
<td>Enterprise .NET applications, Azure<br>workflows, plugin-based systems</td>
</tr>
</tbody>
</table>
<p>SDK frameworks work best when you need precise control over agent behavior, have complex state management requirements, or are developing systems that require extensive customization.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">For detailed comparisons, see our <a href="https://blog.n8n.io/ai-agent-orchestration-frameworks/"><u>guide on AI agent orchestration frameworks</u></a>.</div></div><h2 id="how-to-build-a-multi-agent-system-in-n8n">How to build a multi-agent system in n8n?</h2><p><a href="https://n8n.io/"><u>n8n</u></a> is a node-based AI workflow automation builder that allows you to start simple and add complexity only as needed. We can easily demonstrate in n8n how to connect several services, triggers, and sequential steps in a single automation.&#xA0;</p><p>We&#x2019;ll build a hierarchical multi-agent system in which a main agent coordinates two specialized sub-agents: one for email operations, another for document search and summarization. This represents a pattern from a <a href="https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers"><u>broader set of multi-agent architectures</u></a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/MAS7.png" class="kg-image" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial" loading="lazy" width="1065" height="712" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/MAS7.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/MAS7.png 1000w, https://blog.n8n.io/content/images/2025/12/MAS7.png 1065w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">A hierarchical multiagent system: main agent routes requests to specialised sub-agents</span></figcaption></figure><p>Our example focuses on the supervisor pattern as it&#x2019;s practical for most business automation scenarios. We assume that you already have some experience in building AI agents and focus mainly on a few useful techniques. If you just start, we have multiple videos and tutorials on developing AI agents. <a href="https://community.n8n.io/c/tutorials/28"><u>The community forum</u></a> is the best place to start learning.</p><h3 id="step-1-set-up-the-main-agent-as-the-coordinator">Step 1. Set up the main agent as the coordinator</h3><p>The AI Agent node acts as a central coordinator with Simple Memory to maintain the conversation context.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/MAS10.png" class="kg-image" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial" loading="lazy" width="700" height="600" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/MAS10.png 600w, https://blog.n8n.io/content/images/2025/12/MAS10.png 700w"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Main supervisor agent with several connected sub-nodes</em></i></figcaption></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The model selection depends on your specific needs. You can reserve expensive reasoning models for the planning logic of the main agent and use a cheaper model for simple sub-agent operations. Or you can invert it - use a fast model for routing at the top level and deploy more capable models in sub-agents for complex domain-specific reasoning. n8n makes testing both configurations easy.</div></div><h3 id="step-2-add-and-configure-the-email-sub-agent">Step 2. Add and configure the email sub-agent</h3><p>The email sub-agent includes several Gmail operations (retrieving multiple messages with filtering, preparing drafts, sending replies, and reading the whole content of a single email). When a user requests the latest emails from a specific user, the main agent delegates to the sub-agent, which executes the necessary Gmail API calls and returns the results.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/MAS8.png" class="kg-image" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial" loading="lazy" width="1029" height="644" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/MAS8.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/MAS8.png 1000w, https://blog.n8n.io/content/images/2025/12/MAS8.png 1029w" sizes="(min-width: 720px) 720px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Email sub-agent with several connected nodes</em></i></figcaption></figure><ul><li>Each of the <a href="https://community.n8n.io/t/review-node-as-tools-is-finally-here/57539"><u>Gmail sub-nodes</u></a> is similar to the standalone Gmail node with two key differences:</li><li>The sub-nodes are only connected to the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/"><u>root AI nodes</u></a></li><li>Each sub-node has <a href="https://docs.n8n.io/advanced-ai/examples/using-the-fromai-function/"><u>dynamic tool parameters</u></a>. Dynamic parameters are filled in during LLM runtime. Your only task is to provide clear descriptions for each field.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/MAS4.png" class="kg-image" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial" loading="lazy" width="1730" height="920" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/MAS4.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/MAS4.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/MAS4.png 1600w, https://blog.n8n.io/content/images/2025/12/MAS4.png 1730w" sizes="(min-width: 720px) 720px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Example settings of the Gmail sub-nodes</em></i></figcaption></figure><p>Learn more about the AI Agent Tool node:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/lW5xEm7iSXk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="n8n Just Made Multi Agent AI Way Easier: New AI Agent Tool"></iframe></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">If you&#x2019;re wondering why we use AI sub-agents instead of the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolmcp/"><u>MCP tool</u></a>, the difference is that the AI sub-agent has its own system prompt, separate LLM node and even memory. MCP hides the complexity and can be easier to set up (a single node instead of multiple tools), but it only provides tool access. It is possible to connect the MCP tool node to the sub-agent if you need a special system prompt or another LLM.</div></div><h3 id="step-3-build-a-rag-sub-agent-to-modularize-data-access">Step 3. Build a RAG sub-agent to modularize data access</h3><p>Finally, we create a dedicated RAG sub-agent which handles all document operations - searching embeddings, retrieving relevant content and summarising the whole document.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/MAS9.png" class="kg-image" alt="Multi-agent systems: Frameworks &amp; step-by-step tutorial" loading="lazy" width="780" height="525" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/MAS9.png 600w, https://blog.n8n.io/content/images/2025/12/MAS9.png 780w" sizes="(min-width: 720px) 720px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">RAG-sub-agent fetches document chunks via Qdrant vector store, and is also able to get the whole document summary</em></i></figcaption></figure><p>We&#x2019;ve already prepared <a href="https://drive.google.com/drive/u/2/folders/1BevhU5qdgNDFbK4D9oAYGeK0Dt5sEaxQ" rel="noreferrer">the Qdrant collection</a> which you can import into a free Qdrant cloud or self-hosted account.</p><p>The document processing workflow is useful for capturing the context of the entire document (rather than just the chunk from a vector store). It includes predefined steps: download file &#x2192; extract text &#x2192; convert to markdown &#x2192; prepare a summary text &#x2192; send back to the sub-agent. These sequences are wrapped in a sub-workflow, making them reusable across different agents and reducing execution overhead. When you need to change document processing logic, you update one subworkflow instead of modifying multiple agents.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">When working with sub-agents that process large files, it may be useful to pass the file identifier between agents instead of passing the file content itself. We&apos;ve illustrated a special case of this in our tutorial for an agent sub-workflow. A sub-workflow extracts the file from Google Drive using the file ID it receives from the AI agent. The agent receives the ID by querying the vector store. <br><br>Alternatively, you can use the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/"><u>Memory Manager node</u></a> to load the file content into the chat history once and connect the same memory node to all sub-agents. This ensures that the file content is not lost when agents interact with each other.</div></div><p><a href="https://n8n.io/workflows/11525-build-a-multi-agent-system-with-n8n-qdrant-gmail-and-openai/" rel="noreferrer">Grab</a> the free n8n template and adjust this multi-agent system to your needs.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(11525, document.currentScript);
</script>
<!--kg-card-end: html-->
<h3 id="step-4-optimize-task-handover-between-agents">Step 4. Optimize task handover between agents</h3><p>Currently, agents track their intermediate steps in a scratchpad and only pass the final messages to each other. This significantly reduces the overall token consumption but some of the context is lost.</p><p>There are two strategies to mitigate this:</p><ul><li>First, <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/"><u>add critical intermediate results to shared memory</u></a> so other agents can access them.</li><li>Second, for large data transfers between agents, pass file identifiers or URLs instead of the full content. This way multiple agents can read the same source data without creating lengthy outputs in their communication.</li></ul><p>In the example setup, sub-agents report only to the main agent. But you can configure peer-to-peer communication if your workflow requires agents to coordinate directly without going through the supervisor.</p><p>The complete workflow demonstrates how to convert a monolithic agent (all tools are directly attached) into a modular multi-agent system. Swap out the email sub-agent for Slack operations, add a database query sub-agent, change models per agent based on the task complexity - the architecture supports these modifications without rebuilding from scratch.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">For an excellent video-tutorial with a different example of MAS in n8n, check out <a href="https://www.youtube.com/watch?v=0iUNOmeU7O4"><u>this popular video from our community</u></a>.</div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/0iUNOmeU7O4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Multi-Agent Systems Have NEVER Been EASIER to Build (n8n, no code)"></iframe></figure><h2 id="advantages-of-multi-agent-systems">Advantages of multi-agent systems</h2><p>The specialization approach creates four core benefits that matter in production:</p><h3 id="task-specialization-reduces-token-consumption">Task specialization reduces token consumption</h3><p>A generalist agent that processes a simple data validation task uses the same costly model as complex multi-step reasoning. Multi-agent systems let you match model size to the task complexity. Simple tasks (data validation, format checking) can run on smaller models. Complex synthesis and decision-making use larger models only when necessary.</p><h3 id="parallel-execution-improves-throughput">Parallel execution improves throughput</h3><p>Single agents process sequentially - they finish one task and then start the next. Multi-agent systems can perform independent operations simultaneously. A research system can query three data sources at once, rather than one after another. This is important when response time directly impacts user experience or when you&apos;re processing large amounts of data.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In n8n, AI agents are built on top of the LangChain library. By default, multi-agent systems work sequentially. If you need to parallelize certain steps, we recommend using <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executeworkflow/" rel="noreferrer">the Execute sub-workflow node</a> or the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolhttprequest/" rel="noreferrer">HTTP Request tool node</a> and sending multiple requests at once. Alternatively, you can also write custom code in the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/"><u>LangChain Code node</u></a>. This allows you to improve the multitasking performance of your system.</div></div><h3 id="isolated-failures-improve-reliability">Isolated failures improve reliability</h3><p>When a single-agent system fails, everything stops. Multi-agent systems contain failures to specific components. Your billing agent crashes? Customer support and technical support agents keep working. The system degrades gracefully instead of going completely offline. This also speeds up debugging - you know exactly which agent has failed.</p><h3 id="modular-updates-reduce-deployment-risk">Modular updates reduce deployment risk</h3><p>To update the behavior of a single agent, you don&apos;t have to redeploy the entire system. You can test changes to a specialist agent without risking cascading issues. This becomes critical as systems scale. A customer support system with 10 specialized agents can update its billing logic without affecting order tracking, account management, or technical support agents.</p><h2 id="challenges-of-multi-agent-systems">Challenges of multi-agent systems</h2><p>Multi-agent systems introduce additional complexity that does not arise with single agents. Here are the four most common challenges:</p><h3 id="coordination-overhead-scales-with-agent-count">Coordination overhead scales with agent count</h3><p>The communication complexity grows exponentially with the number of agents. Three agents coordinate three relationships. Ten agents need forty-five. This manifests itself in increased latency, higher token consumption due to context sharing, and more failure points in the coordination chain.</p><p><strong>Mitigation</strong>: Choose architecture patterns that limit connections. Hierarchical structures with supervisor agents reduce direct agent-to-agent communication. Sequential pipelines eliminate parallel coordination overhead.</p><h3 id="token-costs-multiply-across-the-system">Token costs multiply across the system</h3><p>Multi-agent systems use significantly more tokens than single-agent approaches. This seems to contradict the &quot;reduced token consumption&quot; advantage. The paradox resolves when you consider the allocation strategy: you&apos;re using more tokens overall, but they&#x2019;re distributed more efficiently.</p><p><strong>Mitigation</strong>: Mix different models based on the task complexity. Implement prompt caching to reuse repeated context across agent interactions. Monitor token usage per workflow and optimize high-cost handoffs.</p><h3 id="quality-drift-compounds-through-agent-chains">Quality drift compounds through agent chains</h3><p>An error made by one agent affects downstream processes. A data extraction agent misreads a field, the validation agent approves it based on an incorrect schema, and the reporting agent presents incorrect information to users.</p><p><strong>Mitigation</strong>: Set up validation checkpoints between agents for critical operations. These checks can be LLM-powered or use simple regex rules. Alternatively, you can run parallel agents for the same task and select the mode value (the most frequent result).</p><h3 id="security-depends-on-your-threat-model">Security depends on your threat model</h3><p>Internal systems that operate with trusted data are exposed to different risks than client-facing agents processing external input.</p><p>Client-facing systems are vulnerable to prompt injection attacks where malicious instructions manipulate agent behavior. <a href="https://brave.com/blog/comet-prompt-injection/"><u>Brave&apos;s research on Perplexity Comet</u></a> demonstrated how hidden instructions in webpage content can steal credentials and exfiltrate sensitive data - completely bypassing traditional web security mechanisms.</p><p><strong>Mitigation</strong>: Treat all external input as untrustworthy for customer-facing agents, require explicit user confirmation for sensitive actions, and isolate the agent&apos;s functions from regular operations.</p><h2 id="faq">FAQ</h2>
<!--kg-card-begin: html-->
<details>
 <summary>
        <h3 style="display: inline-block; margin: 0 0 0.5em 0;" id="faq-1">What are common multi-agent architecture patterns?</h3>
    </summary>
<!--kg-card-end: html-->
<p>Hierarchical (supervisor manages worker agents), sequential (pipeline where each agent processes then hands off), parallel (multiple agents work simultaneously), handoff (dynamic routing based on context), and network (agents communicate peer-to-peer).</p>

<!--kg-card-begin: html-->
</details>
<!--kg-card-end: html-->

<!--kg-card-begin: html-->
<details>
 <summary>
        <h3 style="display: inline-block; margin: 0 0 0.5em 0;" id="faq-2">How do I evaluate multi-agent system performance?</h3>
    </summary>
<!--kg-card-end: html-->
<p>Track overall efficiency with the built-in <a href="https://docs.n8n.io/advanced-ai/evaluations/overview/">n8n Evals feature</a>, save task completion time, token usage per workflow, error propagation rates, and overall cost per task. Use tracing tools like LangSmith or LangFuse to visualize agent interactions and identify bottlenecks.</p>

<!--kg-card-begin: html-->
</details>
<!--kg-card-end: html-->

<!--kg-card-begin: html-->
<details>
 <summary>
        <h3 style="display: inline-block; margin: 0 0 0.5em 0;" id="faq-3">What are the best tools for building multi-agent systems?</h3>
    </summary>
<!--kg-card-end: html-->
<p>n8n works well for hybrid visual/code systems with 1000+ built-in integrations. Code-first SDKs (LangGraph, AutoGen, Semantic Kernel) offer precise control over state management and coordination patterns - choose based on your team&apos;s language preferences (Python, C#, Java, JavaScript). If you&apos;re locked into AWS, Google Cloud, or Azure ecosystems, their managed platforms provide native integrations.</p>

<!--kg-card-begin: html-->
</details>
<!--kg-card-end: html-->

<!--kg-card-begin: html-->
<details>
 <summary>
        <h3 style="display: inline-block; margin: 0 0 0.5em 0;" id="faq-4">When should I use multi-agent instead of single-agent systems?</h3>
    </summary>
<!--kg-card-end: html-->
<p>When tasks span multiple domains that require deep expertise, when you need parallel processing across data sources, or when a single context window can&apos;t hold everything. Don&apos;t use multi-agent for simple tasks where the coordination effort exceeds the benefit.</p>

<!--kg-card-begin: html-->
</details>
<!--kg-card-end: html-->

<!--kg-card-begin: html-->
<details>
 <summary>
        <h3 style="display: inline-block; margin: 0 0 0.5em 0;" id="faq-5">What security risks do multi-agent systems face?</h3>
    </summary>
<!--kg-card-end: html-->
<p>Internal systems face credential management challenges across multiple agents with different permissions. Client-facing systems face prompt injection issues where malicious instructions in external data manipulate agent behavior. More agents mean more access points to secure and more potential attack surfaces.</p>

<!--kg-card-begin: html-->
</details>
<!--kg-card-end: html-->
<h2 id="wrap-up">Wrap up</h2><p>Today we&apos;ve covered multi-agent system architectures, coordination patterns, real-world applications, and available frameworks. We&apos;ve also examined the core trade-offs - specialization benefits versus coordination complexity, parallel execution versus token costs.</p><p>There are three distinct ways of creating such systems:</p><ul><li><strong>Visual building</strong>: n8n provides the hybrid option - visual workflow design, code fallback when needed, and no vendor lock-in. Better than pure no-code (Zapier), faster than code-first frameworks for non-developers.</li><li><strong>Code-first development</strong>: various SDKs give precise control over state management and coordination logic. Choose based on your team&apos;s language. Be aware that Google ADK and Microsoft Semantic Kernel optimize for their cloud ecosystems.</li><li><strong>Enterprise platforms</strong>: AWS Bedrock, Google Vertex AI, and Azure offer managed infrastructure. Such SDKs tend to be&#xA0; vendor lock-in in exchange for managed convenience. Evaluate against your multi-cloud strategy.</li></ul><p>In the practical part, we&#x2019;ve demonstrated how to build a multi-agent system using n8n&apos;s sub-agents for task delegation, individual tool nodes for agent capabilities, and workflows-as-tools for controlled multi-step operations. This shows hierarchical patterns in action.</p>
<!--kg-card-begin: html-->
<div class="content-banner">
  <div>
    <h3>Create your own multi-agent systems</h3>
    <p>Build, test and swap specialized agents in minutes</p>
  </div>
  <a href="https://app.n8n.cloud/register" class="global-button blog-banner-signup">Try n8n now</a>
</div>
<!--kg-card-end: html-->
<h2 id="what%E2%80%99s-next">What&#x2019;s next?</h2><p>Ready to build multi-agent systems? Here&apos;s where to go from here:</p><ul><li>Start with <a href="https://blog.n8n.io/ai-agents/"><u>AI agent fundamentals</u></a> if you&apos;re new to agent architectures - covers perception, decision-making, and action cycles.</li><li>Review <a href="https://blog.n8n.io/ai-agentic-workflows/"><u>4 practical AI agentic workflow patterns</u></a> to understand coordination approaches before building complex systems.</li><li>Compare <a href="https://blog.n8n.io/ai-agent-orchestration-frameworks/"><u>AI agent orchestration frameworks</u></a> in detail - includes deployment options, pricing, and trade-offs we didn&apos;t cover here.</li><li>Explore <a href="https://n8n.io/workflows/categories/ai/"><u>production-ready AI workflows</u></a> from the n8n community to see multi-agent patterns implemented.</li></ul><p>Finally, <a href="https://n8n.io/ai-agents/"><u>try n8n&apos;s AI capabilities for free</u></a> and check the <a href="https://n8n.io/integrations/categories/ai/"><u>AI integrations catalog</u></a> to see what tools your agents can connect to.</p>]]></content:encoded></item><item><title><![CDATA[Building your own LLM evaluation framework with n8n]]></title><description><![CDATA[In this hands-on tutorial, we'll guide you through creating a low-code LLM evaluation framework using n8n. Learn key concepts like LLM-as-a-Judge and build a custom evaluation path that ensures you can deploy updates, test new models, and maintain quality with total confidence.]]></description><link>https://blog.n8n.io/llm-evaluation-framework/</link><guid isPermaLink="false">693e8b64da9f1400015cc727</guid><category><![CDATA[AI]]></category><category><![CDATA[Tutorial]]></category><dc:creator><![CDATA[Mihai Farcas]]></dc:creator><pubDate>Mon, 15 Dec 2025 14:37:14 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/12/workflow-template--1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/12/workflow-template--1-.png" alt="Building your own LLM evaluation framework with n8n"><p>If you&#x2019;ve ever built an application powered by Generative AI, you know the feeling: one small change to a prompt, a model swap, or a slight tweak to a node can turn a perfectly functional workflow into an unpredictable mess. Unlike deterministic code, AI outputs introduce an element of delightful, yet frustrating, chaos.</p><p>This unpredictability is exactly why you can&apos;t just rely on guesswork when deploying AI. You need a dedicated, repeatable testing mechanism: <strong>an LLM evaluation framework</strong>.</p><p>In this hands-on tutorial, we&apos;ll guide you through the why and how of creating a low-code <a href="https://blog.n8n.io/introducing-evaluations-for-ai-workflows/"><u>AI Evaluation Framework using n8n</u></a>. You&apos;ll learn the key concepts, understand techniques such as &#x201C;LLM-as-a-Judge&#x201D;, and build a custom evaluation path that ensures you can deploy updates, test new models, and maintain quality with total confidence.</p><h2 id="why-do-you-need-an-evaluation-framework-for-your-ai-workflows">Why do you need an evaluation framework for your AI workflows?</h2><p>An evaluation framework is the foundational practice that shifts your development process from relying on guesswork to relying on concrete, measurable evidence. Here are the five main reasons to build an evaluation framework into your workflows:</p><ol><li><strong>Deploy with confidence:</strong> An evaluation framework acts as a dedicated testing path. By running tests against a consistent dataset, you ensure <strong>long-term reliability and high-quality outputs</strong>. This allows you to catch regressions or new issues before your end users do. It also lets you test against &quot;edge cases&quot; to ensure your system handles unexpected data gracefully.</li><li><strong>Validate changes objectively</strong>: When you tweak a prompt, did it actually improve the output, or did it just change the writing style? Without a framework, the answer is purely subjective. Evaluations give you evidence. You can definitely see if a prompt tweak or a fix for a specific error actually improved results or if it introduced new problems.</li><li><strong>Experiment and iterate faster:</strong> Fear of breaking production often slows down experimentation. Evaluations provide a safe sandbox. You can test radical changes to your logic or prompts and see the quantified impact immediately without affecting real users. This allows for rapid A/B testing, like comparing two different system prompts, to see which scores better against your benchmarks.</li><li><strong>Make data-driven decisions on models:</strong> New models are released constantly. An evaluation framework lets you quickly compare them. You can decide if switching to a new model makes sense for speed or cost-efficiency, or if a smaller model (like a Gemini Flash Lite) can perform just as well as a larger one for your specific task.</li></ol><h2 id="why-use-n8n-for-llm-evaluation">Why use n8n for LLM evaluation?</h2><p>We&#x2019;ll use <a href="https://n8n.io/ai/" rel="noreferrer">n8n</a> as an example for building your own LLM evaluation framework because it treats evaluation as a continuous, workflow-native practice rather than a one-off benchmark. With built-in and custom metrics, automated feedback loops, and ongoing monitoring, it shows how evaluation can directly support iterative improvement and production reliability.</p><p>Here are the key aspects of n8n&#x2019;s flexible evaluation approach:</p><h3 id="1-straightforward-implementation-on-the-canvas">1. Straightforward implementation on the canvas</h3><p>Traditional monitoring and testing tools, such as LangSmith, often come with a steep learning curve that requires configuring APIs, logging libraries, and external infrastructure. n8n eliminates this friction by bringing AI evaluation directly to its canvas.</p><p>This approach allows for straightforward and less error-prone implementations, relying on the visual, drag-and-drop interface you know and trust. You don&apos;t need to write custom Python scripts; you just need to connect nodes.</p><h3 id="2-evaluation-as-a-dedicated-workflow-path">2. Evaluation as a dedicated workflow path</h3><p>In n8n, an evaluation sequence is structured as a dedicated path within your existing workflow. This design is crucial because it ensures:</p><ul><li><strong>Separation of concerns:</strong> You can execute the testing sequence separately from your production triggers (like webhooks or schedules).</li><li><strong>Focus on iteration:</strong> Developers can focus purely on testing, analysis, and metric calculation without disrupting the production logic that serves end-users.</li></ul><h3 id="3-customizable-inputs-and-metrics">3. Customizable inputs and metrics</h3><p>The framework is highly flexible, allowing you to run a range of test inputs against your workflow and observe the outputs. Critically, the metrics applied are completely customizable. You can measure anything relevant to your specific use case, for example:</p><ul><li><strong>Output correctness:</strong> Is the generated answer factually accurate based on predefined criteria?</li><li><strong>Safety and fairness:</strong> The presence of toxicity, bias, or alignment with safety guidelines.</li><li><strong>Tool calling:</strong> Whether the AI agent correctly invoked the right external tool or function (essential for complex agents).</li><li><strong>Deterministic metrics:</strong> Efficiency measurements like execution time or token count.</li></ul><p>This collective data is essential for analyzing the effect of specific changes (e.g., swapping a model, modifying a system prompt) and comparing overall performance over time.</p><h2 id="key-ai-evaluation-metrics-and-methods-you-can-implement-with-n8n">Key AI evaluation metrics and methods you can implement with n8n</h2><p>LLM evaluation requires a nuanced approach, combining qualitative, context-aware assessments with quantifiable hard data. The flexibility of n8n allows you to implement both, often just by configuring the dedicated <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#evaluation-node"><u>Evaluation node</u></a>.</p><p>Here are the key methods you can deploy directly on your canvas:</p><h3 id="1-llm-as-a-judge-the-gold-standard-for-open-ended-tasks">1. LLM-as-a-Judge (the gold standard for open-ended tasks)</h3><p>This is the standard approach for open-ended tasks where traditional metrics fail (e.g., creative writing or summarization). It involves using a highly capable model (like GPT-5 or Claude 4.5 Sonnet) to evaluate the quality of outputs generated by a target model (often a smaller, more efficient model).</p><p>How to implement it in n8n?</p><p>You no longer need to manually configure a &quot;Judge&quot; LLM and parse JSON responses. Instead, open the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#set-metrics"><u>Evaluation node</u></a> and select one of the AI-based metrics:</p><ul><li><em>Correctness (AI-based)</em><strong>:</strong> automatically scores (1-5) whether the answer&#x2019;s meaning is consistent with your reference answer.</li><li><em>Helpfulness (AI-based)</em><strong>:</strong> scores (1-5) whether the response successfully addresses the initial query.</li><li><em>Custom Metrics</em><strong>:</strong> If you need to test for something specific, like &quot;Did the AI adopt a pirate persona?&quot;, you can use the <em>Custom Metrics</em> option to define your own criteria.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-144457.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="1042" height="901" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-05-144457.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-05-144457.webp 1000w, https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-144457.webp 1042w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n&apos;s Set Metrics node settings</span></figcaption></figure><h3 id="2-evaluating-complex-agent-workflows-rag-and-tool-use">2. Evaluating complex agent workflows (RAG and tool use)</h3><p>If your workflow uses Retrieval-Augmented Generation (RAG) or relies on the LLM to call external tools, you need to evaluate the entire system, not just the final text generation.</p><p>How to implement it in n8n<strong>:</strong></p><ul><li>Tool usage<strong>:</strong> Use the built-in <em>Tools Used</em> metric in the Evaluation node. This returns a score checking if the agent correctly triggered a tool call when expected.</li><li>RAG faithfulness<strong>:</strong> You can use the <em>Correctness (AI-based)</em> metric to verify that the generated answer aligns with the ground truth found in your documents.</li></ul><h3 id="3-quantitative-metrics">3. Quantitative metrics</h3><p>These provide unambiguous data points that complement the qualitative assessments from the LLM-as-a-Judge.</p><p><strong>How to implement it in n8n:</strong></p><ul><li>Deterministic Metrics: These are tracked automatically by n8n&#x2019;s evaluation process:<ul><li><em>Token Count:</em> Essential for tracking cost.</li><li><em>Execution Time:</em> Critical for monitoring user experience latency.</li></ul></li><li><em>Categorization</em><strong>:</strong> Perfect for classification tasks (like our sentiment analysis example). It checks if the output exactly matches the expected class (returning 1 for a match, 0 for a miss).</li><li><em>String similarity</em><strong>:</strong> Measures the character-by-character distance between the result and the expectation. This is useful when you want to catch minor formatting errors or typos without penalizing the model for a valid answer.</li><li>Safety &amp; performance<strong>:</strong> For metrics not yet built-in (like specific Toxicity checks or detailed execution latency), you can simply define them using the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#set-metrics" rel="noreferrer"><strong>Custom Metrics</strong> feature within the Evaluation node</a>.</li><li>Traditional ML Metrics: For structured tasks (like entity extraction), use the Custom Metrics feature to compare the output against a ground truth using classic metrics like <em>Accuracy</em>, <em>Precision</em>, <em>Recall</em>, or <em>F1 Score</em>.</li></ul><h3 id="4-policy-and-safety-evaluation-with-the-guardrails-node">4. Policy and safety evaluation with the guardrails node</h3><p>For enforcing safety, security, and content policies in real-time, the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.guardrails/"><u>Guardrails node</u></a> is essential. You can use it to validate user input <em>before</em> sending it to an AI model, or to check the <em>output</em> from an AI model before it&apos;s used further in your workflow. This allows teams to validate AI responses in real-time, checking for content quality, safety, or custom rules before routing failures to fallback agents or human review.</p><p>The node offers two primary operations:</p><ul><li>The check text for violations<strong>:</strong> Any violation sends items to a &#x201C;Fail&#x201D; branch, which is ideal for evaluation where you want to halt the workflow on an issue.</li><li>Sanitize text<strong>:</strong> Detects and replaces violations like URLs, secret keys, or personally identifiable information (PII) with placeholders. This is useful for cleaning data within the workflow.</li></ul><p>The true power of this approach lies in combination. You can set up a single Evaluation node to check for Categorization (accuracy), Tools Used (logic), Helpfulness (quality) as well as safety, while simultaneously giving you an overall view of performance.</p><h2 id="how-to-build-an-llm-evaluation-framework-for-a-sentiment-analysis-workflow-with-n8n">How to build an LLM evaluation framework for a sentiment analysis workflow with n8n?</h2><p>To illustrate the capabilities of n8n&#x2019;s evaluation features, we are building a workflow that performs sentiment analysis on incoming emails, categorizes them as <strong>Positive</strong>, <strong>Neutral</strong>, or <strong>Negative</strong>, and routes them to the appropriate sales team.</p><p>We want to ensure the categorization works properly. To do that, we will build an evaluation workflow, feed it some tricky use cases, and compare different models to find the cheapest one that performs the task reliably. We will test <strong>Gemini 3 Pro</strong>, <strong>Gemini 2.5 Flash</strong>, and <strong>Gemini 2.5 Flash Lite</strong>. The latter is the most affordable option. If it can handle our workflow, that is a huge win.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-04-175427.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="2000" height="965" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-04-175427.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-04-175427.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/Screenshot-2025-12-04-175427.webp 1600w, https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-04-175427.webp 2101w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">AI evaluation workflow example in n8n</span></figcaption></figure>
<!--kg-card-begin: html-->
<script>
  workflowBanner(11832, document.currentScript);
</script>
<!--kg-card-end: html-->
<p>One of the best parts of the n8n implementation is that it allows you to house the evaluation logic directly alongside your actual workflow.</p><h3 id="step-1-setting-up-the-ground-truths-using-data-tables">Step 1: Setting up the ground truths using Data Tables</h3><p>To run evaluations, we first need to establish test cases and ground truths. The new <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.datatable/"><u>Data Table feature</u></a> in n8n is perfect for this. Think of it as a database table that lives directly in n8n, which workflows can read from and write to.</p><p>For this example, I created 10 test cases. The goal is to find the smallest (and cheapest!) model that performs correctly, guiding us to tweak the system prompt for perfect accuracy.</p><p>To stress-test the models, the test cases are tricky in a few ways:</p><ol><li><strong>Competitor frustration:</strong> The text might list frustrations with a competitor&#x2019;s solution. While it contains negative words, the intent is actually <em>positive</em> for us (they want to switch). Traditional ML often fails here, but LLMs should catch the nuance. We want to see if the smaller Flash Lite model can match the accuracy of Gemini 3 Pro on this.</li><li><strong>Sarcasm:</strong> Phrases like &quot;I was thrilled to see my project pipeline freeze for six hours yesterday&quot; should be classified as <strong>Negative</strong>. Only a capable LLM will catch this tone.</li><li><strong>Mixed signals:</strong> Combining a small compliment with a major complaint. This should be classified as <strong>Negative</strong> overall.</li></ol><p>Once the data table is set up, it looks like this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-03-200613.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="2000" height="592" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-03-200613.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-03-200613.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/Screenshot-2025-12-03-200613.webp 1600w, https://blog.n8n.io/content/images/size/w2400/2025/12/Screenshot-2025-12-03-200613.webp 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Data Table for storing evaluation ground truths and results</span></figcaption></figure><p>The <em>expected </em>column contains our ground truth, and the <em>result </em>column starts empty, this is where we will store the latest evaluation output.</p><h3 id="step-2-creating-the-evaluation-workflow">Step 2: Creating the evaluation workflow</h3><p>Now, let&#x2019;s build the evaluation workflow. We start by fetching all records from the data table and looping over them.</p><p>Inside the loop, we pass the data to the <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.sentimentanalysis/?utm_source=n8n_app&amp;utm_medium=node_settings_modal-credential_link&amp;utm_campaign=%40n8n%2Fn8n-nodes-langchain.sentimentAnalysis"><u>Sentiment Analysis node</u></a>, configured to categorize emails into three buckets: Positive, Neutral, or Negative.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/data-src-image-a65d6876-9ff8-46e8-96b9-dac040db0705.png" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="1600" height="1126" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/data-src-image-a65d6876-9ff8-46e8-96b9-dac040db0705.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/data-src-image-a65d6876-9ff8-46e8-96b9-dac040db0705.png 1000w, https://blog.n8n.io/content/images/2025/12/data-src-image-a65d6876-9ff8-46e8-96b9-dac040db0705.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Sentiment Analysis node details</span></figcaption></figure><p>Normally, the workflow forwards the email to the appropriate team based on the category. However, we don&apos;t want to send real emails during an evaluation. To solve this, we use the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#check-if-evaluating"><u>Check if Evaluating node</u></a>. This splits the workflow into two paths: one for the active evaluation, and one for the normal production run.</p><p>On the evaluation path, we use the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#set-outputs"><u>Set Outputs option of the Evaluation node</u></a>. We select our &quot;Sentiment Analysis Evaluation&quot; table and map the output of the analysis node to the <em>result </em>column.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-092257.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="2000" height="1094" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-05-092257.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-05-092257.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/Screenshot-2025-12-05-092257.webp 1600w, https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-092257.webp 2370w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Save evaluation results back to the Data Table</span></figcaption></figure><h3 id="step-3-computing-metrics">Step 3: Computing metrics</h3><p>Computing metrics is vital to understanding performance at a glance. We do this using the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#set-metrics"><u>Set Metrics option of the Evaluation node</u></a>. We can select the built-in <strong>Categorization</strong> metric, which is designed specifically for this use case.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-092749.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="2000" height="1466" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-05-092749.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-05-092749.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/Screenshot-2025-12-05-092749.webp 1600w, https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-092749.webp 2359w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Computing evaluation metrics using the Set Metrics node</span></figcaption></figure><p>This metric simply compares the expected answer with the actual one. It returns a 0 for a mismatch or a 1 for a match, exactly what we need.&#xA0;</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">You can also calculate advanced metrics like precision, recall, or F1 score using the <b><strong style="white-space: pre-wrap;">Custom Metrics</strong></b> option.</div></div><h3 id="step-4-running-the-tests">Step 4: Running the tests</h3><p>Now we can run the evaluation directly from the canvas to test it. Alternatively, we can use the new <strong>Evaluations</strong> tab at the top of the canvas. Runs started here are saved, providing a visual chart of metrics over time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-110956.webp" class="kg-image" alt="Building your own LLM evaluation framework with n8n" loading="lazy" width="2000" height="803" srcset="https://blog.n8n.io/content/images/size/w600/2025/12/Screenshot-2025-12-05-110956.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/12/Screenshot-2025-12-05-110956.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/12/Screenshot-2025-12-05-110956.webp 1600w, https://blog.n8n.io/content/images/2025/12/Screenshot-2025-12-05-110956.webp 2076w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">AI evaluation results chart in n8n</span></figcaption></figure><p>We ran this evaluation three times, yielding valuable insights: All three models handled the test cases perfectly, despite the tricky edge cases. However, the performance differed significantly:</p><ul><li><strong>Gemini 3 Pro</strong> took over 30 seconds.</li><li><strong>Gemini 2.5 Flash</strong> took about 1.6 seconds.</li><li><strong>Gemini 2.5 Flash Lite</strong> finished in just 650 milliseconds.</li></ul><p>This makes the decision a no-brainer: <strong>Gemini 2.5 Flash Lite</strong> is accurate enough for this task and is the fastest and cheapest option. This is the clear benefit of having an evaluation framework!</p><h2 id="best-practices-for-building-your-llm-evaluation-framework-in-n8n">Best practices for building your LLM evaluation framework in n8n</h2><p>Building an LLM evaluation framework is as much about process as it is about the tools. Here are five best practices to ensure your evaluations remain reliable and scalable:</p><ol><li><strong>Always separate evaluation logic:</strong> Never mix testing logic with production actions. Always use the <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/#check-if-evaluating"><u><strong>Check if Evaluating</strong> node</u></a> to create a clean separation. This prevents &quot;test pollution&quot;&#x2014;like sending 50 test emails to your sales team&#x2014;and ensures your metrics are calculated only during actual test runs.</li><li><strong>Curate a &quot;Golden Dataset&quot;:</strong> Your evaluation is only as good as your data. Don&apos;t just generate random strings; build a <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.datatable/"><strong><u>Data Table</u></strong></a> containing real-world edge cases, previous failure points, and tricky adversarial inputs (like the sarcasm example). As you discover new failures in production, add them to this table to prevent future regressions.</li><li><strong>Combine qualitative and quantitative metrics:</strong> Reliance on a single metric can be misleading. A model might be fast (low latency) but hallucinate facts (low correctness). Always pair deterministic metrics (like <strong>Execution Time</strong> or <strong>JSON Validity</strong>) with qualitative ones (like <strong>LLM-as-a-Judge</strong>) to get the full picture.</li><li><strong>Isolate variables during testing:</strong> When comparing performance, change only one variable at a time. If you swap the model <em>and</em> change the prompt simultaneously, you won&apos;t know which change caused the improvement (or regression). Test a prompt change on the <em>same</em> model first, then test different models with that <em>fixed</em> prompt.</li><li><strong>Keep human-in-the-loop for the &quot;Judge&quot;:</strong> While &quot;LLM-as-a-Judge&quot; is powerful, it isn&apos;t infallible. Periodically audit the decisions made by your Judge node, especially for subjective metrics like &quot;Helpfulness.&quot; If the Judge is consistently misinterpreting your criteria, you may need to refine its system prompt just as you would for your main agent</li></ol><h2 id="wrap-up">Wrap up</h2><p>We have moved from the &quot;delightful chaos&quot; of unpredictable AI outputs to a structured, engineering-grade process. By building an evaluation framework directly in n8n, you have shifted from guessing to knowing.</p><p>You now have a system that allows you to:</p><ul><li><strong>Catch regressions</strong> before they hit production.</li><li><strong>Quantify the impact</strong> of every prompt tweak.</li><li><strong>Compare models</strong> objectively to optimize for cost and speed.</li></ul><p>This framework is your safety net, allowing you to innovate faster and deploy with the confidence that your AI agents will perform exactly as expected.</p><h2 id="what%E2%80%99s-next">What&#x2019;s next?</h2><p>Now that you understand the concepts, the best way to learn is to see these workflows in action. We highly recommend watching these tutorials from the community to deepen your understanding:</p><ul><li><a href="https://www.youtube.com/watch?v=-zFd1nPn6U0"><u>Beginner&apos;s Guide to Workflow Evaluation in n8n (Stop Guessing!)</u></a> &#x2013; A fantastic overview of why evaluation matters and how to set up your first &quot;exam&quot; for your AI.</li><li><a href="https://www.youtube.com/watch?v=-4LXYOhQ-Z0"><u>The Beginner&#x2019;s Guide to n8n Evaluations (Optimize Your AI Agents)</u></a> &#x2013; A deep dive into optimizing agents using the evaluation tools we discussed.</li><li><a href="https://www.youtube.com/watch?v=EgWJiTV45AA"><u>Evaluate Your RAG System with N8N</u></a> &#x2013; Essential viewing if you are building Retrieval-Augmented Generation workflows and need to test factual accuracy.</li></ul><p>Start small, build your first test dataset, and happy automating!</p>]]></content:encoded></item><item><title><![CDATA[Introducing n8n 2.0]]></title><description><![CDATA[n8n 2.0 focuses on secure-by-default execution with better reliability and performance.]]></description><link>https://blog.n8n.io/introducing-n8n-2-0/</link><guid isPermaLink="false">6932fd5cf28fba0001469ffb</guid><dc:creator><![CDATA[Ophir Prusak]]></dc:creator><pubDate>Mon, 08 Dec 2025 18:27:42 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/12/8f4e827c-b2fd-4f94-a3b8-690d3d090c21.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="today-we%E2%80%99re-releasing-n8n-version-200-beta">Today we&#x2019;re releasing n8n version 2.0.0 BETA</h3><img src="https://blog.n8n.io/content/images/2025/12/8f4e827c-b2fd-4f94-a3b8-690d3d090c21.jpg" alt="Introducing n8n 2.0"><p>If you&apos;ve been around software long enough, you know that major version bumps usually mean shiny new features, dramatic redesigns, the works. And with over two years since we released n8n 1.0, a lot of people were expecting something similar.</p><p>But that&apos;s not what this release is about. And honestly? That&apos;s a good thing.</p><h2 id="why-20">Why 2.0?</h2><p>n8n follows <a href="https://semver.org/">semantic versioning</a>. That means not shipping breaking changes (updates that might require you to change your workflows or configuration) without incrementing the major version number.</p><p>Since 1.0 launched in July 2023, we&apos;ve accumulated a list of improvements we&apos;ve been eager to make: security hardening, reliability fixes, and deprecations of features that were causing instability. Every one of these required a breaking change. Every one of them had to wait.</p><p>Version 2.0 incorporates all of those improvements and strengthens n8n&apos;s position as an enterprise-grade platform, with enhanced security, reliability, and scalability for mission-critical workflows.</p><p>Going forward, we won&apos;t be waiting this long between major releases. We&apos;re planning to ship one to two major versions per year, allowing us to iterate faster and ship improvements without long delays.</p><h2 id="the-focus-of-20-security-reliability-and-performance">The focus of 2.0: Security, reliability, and performance</h2><h3 id="security">Security</h3><p>We&apos;ve tightened defaults across the board to make n8n significantly more secure out of the box.</p><p>The biggest change: task runners are now enabled by default, meaning all Code node executions run in isolated environments with limited access. We&apos;ve also blocked environment variables from Code nodes and disabled nodes that allow arbitrary command execution by default.</p><p>The theme is &quot;secure by default.&quot; If your workflows rely on any of the previous, more permissive behaviors, you can still enable them, but you&apos;ll need to do so explicitly.</p><h3 id="reliability">Reliability</h3><p>We&apos;re simplifying the platform by removing legacy options and fixing a few cases that caused confusion or edge-case bugs.</p><p>For example, sub workflows with Wait nodes now correctly return data from the end of the workflow (instead of the input to the Wait node), and we&apos;ve removed nodes for services that no longer exist. Less optionality, fewer edge cases, more predictable behavior.</p><h3 id="performance">Performance</h3><p>We&apos;re not claiming dramatic speed improvements, but we are removing things that caused slowdowns. The new SQLite pooling driver alone can be up to 10x faster in our benchmarks. Filesystem-based binary data handling is more predictable under load. And task runners, while primarily a security feature, also provide better isolation and resource management.</p><div class="kg-card kg-cta-card kg-cta-bg-yellow kg-cta-minimal    " data-layout="minimal">
            
            <div class="kg-cta-content">
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p><span style="white-space: pre-wrap;">The full list of </span><b><strong style="white-space: pre-wrap;">breaking changes</strong></b><span style="white-space: pre-wrap;"> is documented in our </span><a href="https://docs.n8n.io/2-0-breaking-changes/" rel="noopener noreferrer" class="cta-link-color"><span style="white-space: pre-wrap;">migration guide</span></a><span style="white-space: pre-wrap;"> which walks through each change and how to handle it. Be sure to review it before upgrading.</span></p>
                        </div>
                    
                    
                    </div>
                
            </div>
        </div><h2 id="improvements-you%E2%80%99ll-see-right-away">Improvements you&#x2019;ll see right away</h2><p>While the core of 2.0 is under-the-hood, we&#x2019;re also shipping a safer way to update live workflows and a few UI/UX improvements:</p><p><strong>Publish / Save</strong><br>n8n v2.0 introduces a new deliberate, safer paradigm for pushing workflow changes live to production: <strong>Publish / Save.</strong></p><p>In versions 1.x, saving an activated workflow instantly updated production. In v2.0, the <code>Save</code> button preserves your edits without changing what&#x2019;s live. We&#x2019;ve added a new <code>Publish</code> button as a separate, explicit action to update the live version when you&#x2019;re ready.</p><p>Check out <a href="https://docs.n8n.io/workflows/publish/" rel="noreferrer">our docs</a> for a detailed explanation.</p><p><em>This change also lays the groundwork for our Autosave feature which is coming in a few weeks (January 2026).</em></p><p><strong>Improved canvas look and feel.</strong> <br>We made some subtle refinements to the workflow editor canvas so it looks even better, and are working on further improvements.</p><p><strong>Updated sidebar navigation.</strong> <br>We&apos;ve reorganized the sidebar to make it easier to find what you need. Small change, big quality-of-life improvement.</p><h2 id="check-if-youre-ready-the-migration-report"><strong>Check if you&apos;re ready: the Migration Report</strong></h2><p>We&apos;ve built a tool to take the guesswork out of upgrading where you can see exactly which workflows and configurations need attention before you upgrade.</p><p>The report organizes issues into two categories: workflow-level issues (specific nodes or behaviors that will break) and instance-level issues (environment variables and server configuration). Each issue is tagged by severity &#x2014; critical issues will break workflows, so fix those first. Medium and low severity items can wait, but you&apos;ll want to address them eventually.</p><p>Once you&apos;ve worked through the list and hit refresh, a clean report means you&apos;re ready to upgrade.</p><p>Details in the <a href="https://docs.n8n.io/migration-tool-v2/">migration tool docs</a>.</p><div class="kg-card kg-cta-card kg-cta-bg-yellow kg-cta-minimal    " data-layout="minimal">
            
            <div class="kg-cta-content">
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p><span style="white-space: pre-wrap;">The migration tool is only available to global admins on versions 1.119.0 and higher.</span></p>
                        </div>
                    
                    
                    </div>
                
            </div>
        </div><h2 id="reflecting-on-the-journey-10-%E2%86%92-20">Reflecting on the journey: 1.0 &#x2192; 2.0</h2><p>A major version release feels like the right moment to pause and acknowledge how far we&apos;ve come.</p><p>When we released n8n 1.0 in July 2023, it was a milestone: the moment n8n stepped onto the stage as a production-ready platform. Since then:</p><ul><li><a href="https://github.com/n8n-io/n8n" rel="noreferrer"><strong>GitHub</strong></a><strong> stars:</strong> from ~30,000 to over 160,000 stars &#x1F31F;</li><li><a href="https://community.n8n.io/"><strong>Community</strong></a>: from 6,267 to 115,192 members in our community forum.</li><li><a href="https://n8n.io/careers/" rel="noreferrer"><strong>Team</strong></a><strong>:</strong> We&apos;ve grown from 30 to over 190 people</li><li><a href="https://docs.n8n.io/release-notes/" rel="noreferrer"><strong>Releases</strong></a>: Over 120 new releases (almost every week)</li></ul><p>None of this happens without you: the community members who file issues, build nodes, answer questions in the forum, create tutorials, and push n8n to be better every day. This release, like every release, is built on your feedback and contributions.</p><p>And of course our engineering team of over 50 people who have been working tirelessly to make this happen.</p><p>Thank you all.</p><p><strong>We have a lot of exciting new features coming soon so stay tuned!</strong></p><h2 id="faq-what-you-need-to-know">FAQ: What you need to know</h2><p><br><strong>When is this happening?</strong></p><ul><li>Beta (2.0.0): December 8th</li><li>Stable (2.0.x): December 15th</li></ul><p>Follow our <a href="https://docs.n8n.io/release-notes/">release notes page</a> for the latest updates.</p><p><strong>What are the breaking changes?</strong></p><p>We&apos;ve documented all breaking changes and migration paths in our dedicated <a href="https://docs.n8n.io/2-0-breaking-changes/">2.0 breaking changes guide</a>.</p><p><strong>How do I know if I&apos;m affected by the breaking changes?</strong></p><p>We&apos;ve built a Migration Report tool that shows workflow-level and instance-level issues you need to address before upgrading. Find it in your n8n instance under <strong>Settings &#x2192; Migration Report</strong> (available since version 1.121.0, visible to global admins only). </p><p>Details in the <a href="https://docs.n8n.io/migration-tool-v2/">migration tool docs</a>.</p><p><strong>Does the community edition get all 2.0 updates?</strong></p><p>Yes. All changes in 2.0 apply to all versions of n8n: Self-hosted (Community), Cloud, and Enterprise versions.</p><p><strong>What happens to version 1.x?</strong></p><p>Version 1.x will continue to be supported for 3 months after the 2.0 release. During this period, it will receive security and bug fixes only &#x2014; no new features will be added.</p><p><strong>What if I have questions?</strong></p><p>Ask in the <a href="https://community.n8n.io/t/announcing-n8n-version-2-0-coming-soon/226475">community forum</a> &#x2014; reply to our announcement post and we&apos;ll help you out.</p><hr><h2 id="get-ready-for-20">Get ready for 2.0</h2><ol><li><a href="https://docs.n8n.io/2-0-breaking-changes/"><strong>Review the breaking changes.</strong></a></li><li><a href="https://docs.n8n.io/migration-tool-v2/"><strong>Check your Migration Report.</strong></a></li><li><strong>Questions?</strong> <a href="https://community.n8n.io/t/announcing-n8n-version-2-0-coming-soon/226475"><strong>Ask in the community.</strong></a></li></ol><p><strong>Here&apos;s to what&apos;s next</strong><a href="https://blog.n8n.io/content/images/size/w2400/2025/12/Gemini_Generated_Image_547x8t547x8t547x-updatge.png" rel="noreferrer"><strong>!</strong></a></p><p>&#x2014; The entire n8n Team</p>]]></content:encoded></item><item><title><![CDATA[The 9 best AI workflow automation tools in 2026]]></title><description><![CDATA[Looking for AI workflow automation tools? Compare n8n, Zapier, Make, and 6 other top platforms. Find the best fit for beginners, developers, and enterprise teams.]]></description><link>https://blog.n8n.io/best-ai-workflow-automation-tools/</link><guid isPermaLink="false">691d4291d361450001dd3abf</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Maddy Osman]]></dc:creator><pubDate>Tue, 25 Nov 2025 13:54:00 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/12/best-asi-wf-automation-toolsB--1-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/12/best-asi-wf-automation-toolsB--1-.jpg" alt="The 9 best AI workflow automation tools in 2026"><p>Automating tasks has long been a proven strategy for enhancing efficiency. With the addition of AI, we can take productivity to an entirely new level.</p><p>Building a business and working with a team forced me to think through the logic of delegating tasks and building repeatable workflows. This foundation in process-driven thinking has proven invaluable as AI workflow tools have evolved &#x2014; they enable me to use and expand upon the same logical approach that worked for delegation, powering sophisticated AI automations.</p><p>After experimenting with several AI automation tools, such as <a href="https://n8n.io/ai/">n8n</a>, I&#x2019;ve started to get a feel for what works well for different technical skill levels, business needs across sales, marketing, and operations, and varying business sizes, ranging from freelancers to enterprise teams. Let&#x2019;s dig into the best AI workflow automation tools on the market in 2026.</p><h2 id="what-is-an-ai-automation-tool">What is an AI automation tool?</h2><p>An AI automation tool combines the power of artificial intelligence with the standardization of automated workflows. Instead of just moving data from one app to another (traditional automation), an AI automation tool can interpret, decide, generate, and adapt as it runs. AI automation tools can also help you integrate with other third-party applications that power up your workflows.</p><p>A major benefit of incorporating AI with automation is intelligent handling of edge cases. With proper setup, AI workflows can gracefully manage unexpected scenarios &#x2014; even those you haven&apos;t explicitly anticipated &#x2014; whereas traditional programming requires you to predefine every possible path.</p><p>That said, the term &#x201C;intelligence&#x201D; in AI is somewhat of a misnomer. Rather than outsourcing reasoning completely, you can engineer AI workflows to set you up for success, then implement a &#x201C;<a href="https://cloud.google.com/discover/human-in-the-loop">human-in-the-loop</a>&#x201D; (HITL) review of outputs before taking final actions.</p><h2 id="how-we-selected-the-best-ai-workflow-automation-tools">How we selected the best AI workflow automation tools</h2><p>With new AI automation tools emerging rapidly, it&#x2019;s essential to establish clear criteria for evaluating quality.</p><p>Specifically, the AI automation tools on our list combine some (if not most) of the following key characteristics:</p><ul><li><strong>Highly customizable/flexible</strong>: Even the more specialized tools on this list are flexible enough to suit workflows across different departments and needs. Some AI workflow automation platforms are more open, offering code fallback options (such as JavaScript and Python) and source-available licensing, making them ideal for developers who need deep customization. Others provide a more managed, closed environment that simplifies the experience but limits technical flexibility.</li><li><strong>Extensibility</strong>: Each option we&#x2019;ve covered is extensible via connections to third-party tools and services. We&#x2019;ve noted major limitations, as well as built-in integrations.</li><li><strong>Accessible pricing depending on the use case</strong>: Most AI workflow automation tools offer limited free plans that make it possible to do your own testing before committing, as well as reasonable monthly pricing that scales nicely and allows for budget-friendly access, whether you&#x2019;re a freelancer or in-house team. However, some of the more enterprise-focused platforms come with steep price tags that make them unrealistic for smaller players. Ultimately, we&#x2019;ve provided a range of options for effectively addressing varying needs.</li><li><strong>Good visual workflow design</strong>: While certain platforms are better suited for more technical users, an approachable user interface (UI) is a crucial foundation for building any AI automation workflow. Note that although all platforms mentioned have a learning curve, the steepness varies significantly.</li><li><strong>Enterprise-level security and compliance</strong>: Moving data and connecting external tools necessitates robust security, particularly for enterprise businesses with stringent compliance requirements. Platform requirements typically include SOC 2 compliance, secret management, and role-based access controls. Enterprise also brings with it a larger need for overarching workflow observability and monitoring.</li><li><strong>Technical scalability</strong>: Beyond just features, true scalability requires considering how workflows perform at scale, whether pricing remains reasonable as usage grows, and if the platform can handle increasingly complex automations without degradation. Tools built for different technical audiences will scale differently &#x2014; a platform optimized for simplicity may hit walls that a more technical platform can push through.</li></ul><p>With an understanding of these foundational criteria, let&#x2019;s explore each of the best AI workflow automation tools in-depth.</p><h2 id="the-9-best-ai-workflow-automation-tools-of-2026">The 9 best AI workflow automation tools of 2026</h2><table>
<thead>
<tr>
<th>Tool</th>
<th>Best for</th>
<th>Standout<br>Features</th>
<th>Pricing</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>n8n</strong></td>
<td>All-around flexibility<br>for technical + enterprise users</td>
<td>Source-available,<br>self-hosting,<br>JS/Python steps</td>
<td>Free self-hosted;<br>Cloud from <strong>$20/mo</strong></td>
</tr>
<tr>
<td><strong>Zapier</strong></td>
<td>Non-technical users<br>connecting popular apps</td>
<td>8,000+ integrations,<br>beginner UI</td>
<td>Free tier;<br>Pro <strong>$19.99/mo</strong></td>
</tr>
<tr>
<td><strong>Make</strong></td>
<td>Scenario building<br>&amp; data transformation</td>
<td>Visual builder,<br>detailed control</td>
<td>Free tier;<br>Core <strong>$9/mo</strong></td>
</tr>
<tr>
<td><strong>Gumloop</strong></td>
<td>Simple no-code<br>AI workflows</td>
<td>Templates,<br>MCP support</td>
<td>Free tier;<br>Solo <strong>$30/mo</strong></td>
</tr>
<tr>
<td><strong>Lindy.ai</strong></td>
<td>Straightforward<br>automations</td>
<td>4,000+ integrations,<br>AI voice</td>
<td>Free tier;<br>Pro <strong>$39.99/mo</strong></td>
</tr>
<tr>
<td><strong>Agentforce</strong></td>
<td>Salesforce users</td>
<td>Deep SFDC integration,<br>AI voice</td>
<td>Flex credits<br>from <strong>$500</strong></td>
</tr>
<tr>
<td><strong>Workato</strong></td>
<td>Enterprise automation</td>
<td>RBAC,<br>governance,<br>AIRO</td>
<td>Enterprise only</td>
</tr>
<tr>
<td><strong>AirOps</strong></td>
<td>SEO + content</td>
<td>Semrush/Moz,<br>Power Agents</td>
<td>Free 1k tasks;<br>Paid via sales</td>
</tr>
<tr>
<td><strong>ChatGPT Agent Builder</strong></td>
<td>OpenAI users</td>
<td>Built into UI,<br>drag-and-drop</td>
<td>Included in<br><strong>Plus ($20/mo)</strong></td>
</tr>
</tbody>
</table>
<h3 id="1-n8n-for-all-around-flexibility-and-scalability-for-technical-and-enterprise-users">&#xA0;1. n8n for all-around flexibility and scalability for technical and enterprise users</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/n8n---s-AI-workflow-automation-tool-1.jpg" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="636" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/n8n---s-AI-workflow-automation-tool-1.jpg 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/n8n---s-AI-workflow-automation-tool-1.jpg 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/n8n---s-AI-workflow-automation-tool-1.jpg 1600w, https://blog.n8n.io/content/images/2025/11/n8n---s-AI-workflow-automation-tool-1.jpg 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n&#x2019;s AI workflow automation tool: a workflow for generating social posts from relevant RSS news items</span></figcaption></figure><p><strong>Best for</strong>: Technical users and enterprise teams seeking ultimate flexibility and cost-effective scalability.</p><p><strong>n8n key features</strong>:</p><p>n8n is getting a lot of buzz, and for good reason. In tandem with the AI assistant, the recently released <a href="https://docs.n8n.io/release-notes/#ai-workflow-builder-beta">AI workflow builder</a> has made getting started easier than ever, alongside a robust user community that has contributed <a href="https://n8n.io/workflows/categories/ai/">4,000+ starter templates</a> on n8n &#x2014; with even more options available to import as JSON from GitHub. n8n offers several node options (workflow steps) that include both built-in integrations and the flexibility for custom code fallback via Python and JavaScript.</p><p>n8n isn&#x2019;t the easiest tool on this list to use when getting started, but once you invest the time to learn it, it&#x2019;s a powerful force multiplier. Taking Morten Rand-Hendriksen&#x2019;s <a href="https://www.linkedin.com/learning/build-ai-agents-and-automate-workflows-with-n8n/making-ai-agents-work-for-you?u=0">Build AI Agents and Automate Workflows with n8n</a> course on LinkedIn Learning helped me speed through the learning curve and made the platform click for me. But it wasn&#x2019;t until I rebuilt a workflow on n8n that I originally started on Zapier that I came to appreciate the flexibility and robust determinism that I can achieve with n8n&#x2019;s nodes, especially when using the code steps.</p><p>Hands-on users will find the platform even more rewarding as they dive deeper. For example, after working with both the managed and self-hosted versions of n8n (including setups on DigitalOcean Droplets), I&#x2019;ve found the <a href="https://docs.n8n.io/sustainable-use-license/">fair-code</a> AI workflow automation tool&#x2019;s flexibility to be a major advantage.</p><p>I&#x2019;ve even built multi-agent workflows for clients that generate social posts from relevant news items and utilize <a href="https://blog.n8n.io/rag-chatbot/">retrieval-augmented generation (RAG)</a> to craft replies referencing specific podcast transcripts and URLs stored as metadata (see the screenshot in this section). This illustrates just how far you can push automation with n8n.</p><p>Here are some n8n workflow templates I&#x2019;ve either tested myself or are next on my build list:</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner([6270, 5407, 5948, 1418], document.currentScript);
</script>
<!--kg-card-end: html-->
<p>Enterprise users will appreciate the source-available code, SOC 2 compliance, secret-management via AWS/GCP/Azure/Vault, logging, debugging, and role-based access control (RBAC).</p><p><a href="https://n8n.io/pricing/"><strong>n8n pricing</strong></a>:</p><p>n8n is the only AI workflow automation tool on this list that offers a <a href="https://docs.n8n.io/hosting/">free self-hosted option</a> for technical users to take advantage of. The cloud platform starts at $20/month (billed annually) for 2,500 workflow executions with unlimited steps, which is notably different than others listed that charge variable credits per step.</p><h3 id="2-zapier-for-non-technical-users-connecting-popular-apps">2. Zapier for non-technical users connecting popular apps</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Zapier---s-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1060" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Zapier---s-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Zapier---s-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Zapier---s-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Zapier---s-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Zapier&#x2019;s AI workflow automation tool: a workflow for generating journalist request responses from existing content</span></figcaption></figure><p><strong>Best for</strong>: Beginners building straightforward workflows that connect to popular work apps.</p><p><strong>Zapier key features</strong>:</p><p>Zapier is a user-friendly AI workflow automation tool designed for beginners and non-technical users. It&#x2019;s most well-known as a workflow automation tool but has added a number of AI features across its platform, including Agents, AI automation steps, and MCP. To help users go from idea to execution, Zapier offers an AI copilot for building across each tool, as well as <a href="https://zapier.com/templates">starter templates</a> for each product. Notably, Zapier offers 8000+ built-in integrations with popular work apps (the most on this list), as well as some flexibility to add code steps and integrate third-party APIs.</p><p>Zapier shines as a simple <a href="https://blog.n8n.io/no-code-automation-tools/">no-code platform</a> for building functional apps (thanks to features like Interfaces and Tables). The UI is very approachable, making it a strong contender for teams that want to build AI automations but lack budget or access to developer support.</p><p>While these users benefit from the managed experience, technical teams may find it less flexible than other popular options for their needs. For instance, although connected apps are managed centrally, custom API connections and secrets must be configured individually for each workflow, rather than being reused across automations. This can impact both efficiency and security governance for enterprise implementations.</p><p>I often use Zapier when I start building out the logic for a workflow. I find their customer support to be top-notch; however, there have been times when I&#x2019;ve had to abandon workflows I&#x2019;ve spent hours on, even after customer support has intervened. &#xA0;It&#x2019;s frustrating to invest so much time and still end up without a workable solution.</p><p><a href="https://zapier.com/pricing"><strong>Zapier pricing</strong></a>:</p><p>Zapier separates pricing for AI Orchestration (which includes their Zaps (automations), MCP, Tables, and Interfaces products), Agents, and Chatbots &#x2014; the latter two options are available as add-ons to AI Orchestration plans. AI workflow automation free options include AI Orchestration Free includes 100 tasks/month, Agents Free includes 400 activities/month, and Chatbots Free includes 2 chatbots.</p><p>The AI Orchestration Professional starts at $19.99/month (billed annually) and includes 750 tasks per month, as well as additional features such as multi-step Zaps, webhooks, and email and chat support. Agents Pro starts at $33.33/month (billed annually) and includes 1500 activities. Chatbots Pro starts at $13.33/month (billed annually) and consists of 5 chatbots, as well as additional features such as an expanded knowledge base and 100k Table records.</p><h3 id="3-make-for-scenario-building-and-powerful-data-transformation">3. Make for scenario building and powerful data transformation</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Make---s-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1156" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Make---s-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Make---s-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Make---s-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Make---s-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Make&#x2019;s AI workflow automation tool: a workflow for summarizing Gmail emails with OpenAI, then sending results to Slack</span></figcaption></figure><p><strong>Best for</strong>: Technical teams with automation experts.&#xA0;</p><p><strong>Make key features</strong>:</p><p>Make is often compared to Zapier &#x2014; the differences between these AI automation tools are nuanced. Like Zapier, it offers tools for building automations with AI, as well as an AI agent feature. Enterprise users will appreciate the &#x201C;<a href="https://www.make.com/en/grid">Grid</a>&#x201D; for <a href="https://blog.n8n.io/ai-agent-orchestration-frameworks/">AI orchestration</a>, which provides a high-level view of a brand&#x2019;s agents, apps, and workflows, aiding in observability, improving performance, and simplifying debugging. Like Zapier, secret management is not comprehensive, and it&#x2019;s also worth noting that Make offers limited role-based access control.</p><p>As with many other options on this list, there are several built-in integrations, including multiple <a href="https://www.make.com/en/integrations/category/ai">AI models</a>, pre-built <a href="https://www.make.com/en/templates">workflow templates</a>, and an <a href="https://www.make.com/en/blog/mcp-client">MCP server option</a>. The UI is relatively clean and straightforward, but this perceived ease of use may cause a false sense of security. Make notably involves more setup than some of the more beginner-friendly options shared here.</p><p><a href="https://www.make.com/en/pricing"><strong>Make pricing</strong></a>:</p><p>Make offers a limited Free plan that includes 1,000 credits/month. Paid plans, which start at $9/month (billed annually) for Core, include 10,000 credits/month and features such as unlimited active scenarios (automations), increased data transfer limits, and access to the Make API. Notably, when paid annually, the total credits for the year are available immediately for use, not limited by month. It&#x2019;s worth noting that you must pay (use credits) for each step in a workflow, including error handling. Some steps consume more than one credit (especially AI steps), so scaling should include focusing on how to optimize costs.</p><h3 id="4-gumloop-for-simple-no-code-and-niche-ai-workflows">4. Gumloop for simple no-code and niche AI workflows</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Gumloop---s-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1079" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Gumloop---s-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Gumloop---s-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Gumloop---s-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Gumloop---s-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Gumloop&#x2019;s AI workflow automation tool: a workflow for social media listening</span></figcaption></figure><p><strong>Best for</strong>: Technical users with niche needs in target industries (sales, marketing, operations, support, engineering).</p><p><strong>Gumloop key features</strong>:</p><p>You&#x2019;ll find it either refreshing or limiting that Gumloop, as a platform, is the <a href="https://www.gumloop.com/blog/why-we-built-gumloop">newest company on this list</a> of the best AI workflow automation tools. But what it lacks in tenure, it makes up for in robust niche use cases.</p><p>Gumloop offers a number of useful resources for new users, including <a href="https://www.gumloop.com/templates">templates</a>, a self-paced course and guided learning cohort, a <a href="https://www.gumloop.com/browser-extension">Chrome extension</a> for building, MCP, and its AI building assistant, &#x201C;Gummie.&#x201D; It&#x2019;s clear that Gumloop does a lot to educate users, but in my experience, it&#x2019;s not quite intuitive &#x2014; even with extensive user guidance. In fact, even after watching some of their educational videos, I found the UI to be a bit cluttered and overwhelming, as well as difficult to navigate via keyboard and mouse without taking undesired actions.</p><p>Compared to other options on this list, the scope of use cases is narrower, as are the built-in integrations (100+); however, there are options to add custom integrations and APIs.</p><p><a href="https://www.gumloop.com/pricing"><strong>Gumloop pricing</strong></a>:</p><p>Gumloop&#x2019;s Free plan includes 24,000 credits/year. The Solo plan starts at $30/month (billed annually), which includes 120,000 credits/year, plus unlimited triggers, 4 concurrent runs, email support, <a href="https://blog.n8n.io/webhooks-for-workflow-automation/">webhooks</a>, and the ability to bring your own API keys.</p><p>As with Make, Gumloop&#x2019;s variable credit system, based on workflow complexity, can make it difficult to predict costs.</p><h3 id="5-lindyai-for-common-and-straightforward-workflow-automations">5. Lindy.ai for common and straightforward workflow automations</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Lindy.ai-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="976" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Lindy.ai-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Lindy.ai-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Lindy.ai-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Lindy.ai-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Lindy.ai&#x2019;s AI workflow automation tool: a workflow for brand monitoring</span></figcaption></figure><p><strong>Best for</strong>: Lightweight business automations with common work apps &#x2014; especially email, meeting, and sales tasks.</p><p><strong>Lindy.ai key features</strong>:</p><p>Lindy.ai is not the most robust or extensible AI workflow automation tool. Still, it&#x2019;s a favorite of many users precisely because it&#x2019;s uncomplicated &#x2014; especially those in tech, finance, real estate, and healthcare. It <a href="https://www.lindy.ai/integrations">integrates</a> with many of the most widely used business apps (100 in the Free Plan, 4,000+ on paid plans) and makes it easy for new users to get started with a library of <a href="https://www.lindy.ai/templates">pre-built templates</a>. It notably offers <a href="https://www.lindy.ai/solutions/phone">AI voice</a> features and a <a href="https://www.lindy.ai/medical-scribe">HIPAA-compliant healthcare notetaker</a>. Check out Lindy.ai&#x2019;s <a href="https://community.lindy.ai/">community</a> for ideas and support.</p><p>Lindy.ai provides a straightforward approach to getting started with building simple AI workflows using connected tools. But the simplicity can be a double-edged sword &#x2014; it&#x2019;s not particularly suitable for advanced or custom workflows. Lindy&#x2019;s own literature mentions that it&#x2019;s not particularly strong for non-AI integrations. For enterprise users, there&#x2019;s no secret management and role-based access control, and its simplistic nature also means there are no custom code fallback options.</p><p>I used a template to set up a simple brand monitoring agent on Lindy.ai, but the outputs weren&#x2019;t super useful &#x2014; Lindy.ai surfaced outdated information for a workflow that&#x2019;s supposed to run regularly to stay on top of timely news.</p><p><a href="https://www.lindy.ai/pricing"><strong>Lindy.ai pricing</strong></a>:</p><p>Lindy.ai&#x2019;s Free plan includes 400 credits per month (up to 40 tasks), with limited integrations. Pro starts at $39.99/month (billed annually), and offers 5,000 credits per month, 1,500 tasks, 30 phone calls, and 4,000+ integrations.</p><h3 id="6-agentforce-for-salesforce-users-and-sales-use-cases">6. Agentforce for Salesforce users and sales use cases</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Agentforce-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1306" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Agentforce-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Agentforce-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Agentforce-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Agentforce-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Agentforce&#x2019;s AI workflow automation tool: how it works</span></figcaption></figure><p><strong>Best for</strong>: Salesforce users who want to build AI automations with integrated data.</p><p><strong>Agentforce key features</strong>:</p><p>Agentforce is a niche AI workflow automation tool that builds on data and features provided in the <a href="https://blog.n8n.io/salesforce-data-import/">Salesforce platform</a>. Like other options on this list, it offers MCP support and <a href="https://www.salesforce.com/agentforce/agentexchange/">pre-built agent templates</a>. Notably, it includes functionality for multi-agent orchestration. Similar to Lindy.ai, it also provides built-in functionality for <a href="https://www.salesforce.com/agentforce/voice/">AI voice</a> agents.</p><p>It&apos;s worth noting Salesforce&apos;s positioning around AI agents, particularly given their recent organizational changes and their own research findings. A <a href="https://www.theregister.com/2025/06/16/salesforce_llm_agents_benchmark/">Salesforce-led study</a> revealed that multi-step agents frequently struggle to complete objectives reliably, which provides useful context for evaluating any <a href="https://blog.n8n.io/best-ai-agent-builders/">agent-based platforms</a> (including Salesforce&#x2019;s) promises.</p><p><a href="https://www.salesforce.com/agentforce/pricing/"><strong>Agentforce pricing</strong></a>:</p><p>Agentforce leans more towards enterprise in terms of features and pricing. Generally speaking, Flex Credits (pay-per-action) start at $500 for 100,000 credits. Conversations start at $2 per conversation. Using Agentforce makes the most sense for existing Salesforce enterprise users.</p><h3 id="7-workato-for-enterprise-marketing-and-sales-tasks">7. Workato for enterprise marketing and sales tasks</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/Workato-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/Workato-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/Workato-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/Workato-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/Workato-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Workato&#x2019;s AI workflow automation tool: how it works</span></figcaption></figure><p><strong>Best for</strong>:IT professionals building integrations for enterprise sales and marketing teams.</p><p><strong>Workato key features</strong>:</p><p>Workato is a powerful workflow automation tool for cross-departmental use cases, featuring several useful AI-driven features. It <a href="https://www.workato.com/integrations">integrates with 1200+ apps</a>, offers MCP functionality, an Agent Library with prebuilt agents (called &#x201C;Genies&#x201D;), and their AI copilot for workflow building (&#x201C;AIRO&#x201D;). It&#x2019;s especially ideal for enterprise users requiring SOC 2 Type II compliance, comprehensive role-based access control, centralized governance dashboards, and guaranteed SLAs for uptime and support.</p><p>Like Agentforce, Workato is a powerful AI workflow automation tool for enterprise and niche use cases. It requires more technical resources to set up properly, which could cause bottlenecks when actual users require updates. Because it&#x2019;s completely proprietary, there&#x2019;s also limited inline code customization and no source-available licensing.</p><p><a href="https://www.workato.com/pricing"><strong>Workato pricing</strong></a>:</p><p>Workato doesn&#x2019;t publicly publish its pricing, prompting interested parties to contact sales &#x2014; but note that it&#x2019;s an enterprise tool, so pricing will start significantly higher than that of many other options on this list. Pricing is a factor of tasks, the use of advanced connectors, and the number of users on an account.</p><h3 id="8-airops-for-specialized-seo-and-content-marketing-workflows">8. AirOps for specialized SEO and content marketing workflows</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/AirOps-AI-workflow-automation-tool.jpg" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1164" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/AirOps-AI-workflow-automation-tool.jpg 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/AirOps-AI-workflow-automation-tool.jpg 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/AirOps-AI-workflow-automation-tool.jpg 1600w, https://blog.n8n.io/content/images/2025/11/AirOps-AI-workflow-automation-tool.jpg 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">AirOps&#x2019; AI workflow automation tool: a workflow for transforming AI notetaker transcripts into outlines and action items</span></figcaption></figure><p><strong>Best for</strong>: Technical SEOs and content marketers.</p><p><strong>AirOps key features</strong>:</p><p>AirOps is an emerging AI workflow automation tool designed for a specialized audience of content marketers/SEOs who require access to industry-standard data connectors, such as Semrush and Moz, as well as integrations with popular workflow tools like Slack, Google Docs, and content management systems (CMSes) like WordPress. It offers the flexibility to add custom code steps and your own API keys, which I used when creating a workflow that transformed transcript data originating from my AI notetaker, Fireflies.ai (see screenshot).</p><p>AirOps invests a significant amount of resources in building community and educational content, including live cohorts and the self-paced <a href="http://airops.com/university">AirOps University</a> course. I&#x2019;m a fan of their many prebuilt steps and Power Agents, which serve as useful templates for common content marketing/SEO workflows &#x2014; there&#x2019;s no need to start from scratch. Like many other options shared here, they offer an AI builder copilot. Notably, in line with its niche focus, AirOps also offers a knowledge base and brand kit features for customizing outputs in a useful way.</p><p>Compared to some of the beginner-friendly options on this list, AirOps comes with a learning curve and can be complicated to use. Though custom integrations are possible, there are limited built-in integrations. It&#x2019;s also not necessarily built for enterprise-grade security and offers limited role-based access control.</p><p><a href="https://www.airops.com/pricing"><strong>AirOps pricing</strong></a>:</p><p>AirOps offers a limited Free plan that includes 1,000 tasks/month. Pricing for paid plans with additional tasks and data integrations requires talking to sales, but you can test paid features with a free trial.</p><h3 id="9-chatgpt-agent-builder-for-openai-enthusiasts-and-existing-subscribers">9. ChatGPT Agent Builder for OpenAI enthusiasts and existing subscribers</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/11/ChatGPT-Agent-Builder-AI-workflow-automation-tool.png" class="kg-image" alt="The 9 best AI workflow automation tools in 2026" loading="lazy" width="1920" height="1060" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/ChatGPT-Agent-Builder-AI-workflow-automation-tool.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/ChatGPT-Agent-Builder-AI-workflow-automation-tool.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/11/ChatGPT-Agent-Builder-AI-workflow-automation-tool.png 1600w, https://blog.n8n.io/content/images/2025/11/ChatGPT-Agent-Builder-AI-workflow-automation-tool.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">ChatGPT Agent Builder&#x2019;s AI workflow automation tool: the UI for building a new workflow</span></figcaption></figure><p><strong>Best for</strong>:<br>Existing ChatGPT Plus/Pro subscribers and teams already invested in the <a href="https://blog.n8n.io/how-to-use-openai-node-with-n8n/">OpenAI ecosystem</a>.</p><p><strong>ChatGPT Agent Builder key features</strong>:</p><p>ChatGPT Agent Builder is one of the newest options on this list, offering additional value and functionality for existing ChatGPT users. The drag-and-drop user interface looks fairly straightforward, though it&#x2019;s more technical and complicated to use than it appears at first glance. However, unlike the other options on this list, no separate infrastructure is required to get started.</p><p>ChatGPT Agent Builder offers exciting possibilities, but its first iteration is somewhat limited and clunky to use compared to some of the more beginner-friendly platforms. Although I consider myself an intermediate to advanced user of many of the AI workflow software on this list, I still couldn&#x2019;t quite grasp how to get started with ChatGPT Agent Builder, even after watching a walkthrough on LinkedIn Learning.</p><p>Note that you&#x2019;re limited to OpenAI models and their strengths. Also, it doesn&#x2019;t (currently) offer triggers or scheduling, and the outputs are less deterministic than many other options on this list. The MCP is limited to built-in connectors without third-party options. It&#x2019;s not the best choice for enterprises, as it offers limited security, governance, and compliance features.</p><p>&#xA0;<a href="https://chatgpt.com/pricing/"><strong>ChatGPT Agent Builder pricing</strong></a>:</p><p>Unlike other options on this list, ChatGPT Agent Builder is not available for free but is included with paid ChatGPT plans, starting at Plus ($20/month). If you&#x2019;re already a paid subscriber of ChatGPT, it offers built-in value without the need for additional investments in tools.</p><h2 id="explore-5-high-impact-ai-workflow-automation-tools-examples-with-n8n">Explore 5 high-impact AI workflow automation tools examples with n8n</h2><p>Get started with n8n with our library of more than 4,000 pre-built expert templates. Here are some top picks that span popular tools and use cases:</p><h3 id="rag-chatbot-for-company-documents-using-google-drive-and-gemini">RAG Chatbot for Company Documents using Google Drive and Gemini</h3><p>Get started with RAG (retrieval augmented generation), a technique for grounding AI results in reality. This n8n template uses Google Drive as a knowledge base for responses.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(2753, document.currentScript);
</script>
<!--kg-card-end: html-->
<h3 id="voice-based-appointment-booking-system-with-elevenlabs-ai-and-calcom">Voice-Based Appointment Booking System with ElevenLabs AI and Cal.com</h3><p>Build an AI voice agent for booking customer appointments. This n8n template uses ElevenLabs voices with a <a href="http://cal.com">Cal.com</a> scheduler.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(5670, document.currentScript);
</script>
<!--kg-card-end: html-->
<h3 id="extract-and-analyze-google-ai-overviews-with-llm-for-seo-recommendations">Extract and Analyze Google AI Overviews with LLM for SEO Recommendations&#xA0;</h3><p>Create a vibe marketing workflow to help make progress with your GEO (generative engine optimization) approach. This n8n workflow extracts details about Google AI Overviews for target keywords and generates strategic recommendations.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(4822, document.currentScript);
</script>
<!--kg-card-end: html-->
<h3 id="automate-social-media-posts-with-ai-content-and-images-across-twitter-linkedin-facebook">Automate Social Media Posts with AI Content and Images across Twitter, LinkedIn &amp; Facebook</h3><p>Take the guesswork out of what to post next on your social channels. This n8n template connects with OpenAI to generate posts, saves a record in Google Sheets, and posts automatically to social media platforms.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(5841, document.currentScript);
</script>
<!--kg-card-end: html-->
<h3 id="scrape-recent-news-about-a-company-before-a-call">Scrape recent news about a company before a call</h3><p>Augment your sales process with intelligent data about the prospects you&#x2019;re planning to meet with. This n8n workflow scans your calendar for meetings, uses <a href="http://newsapi.org">newsapi.org</a> to fetch relevant news, then delivers insights for your reference via Gmail.</p>
<!--kg-card-begin: html-->
<script>
  workflowBanner(2110, document.currentScript);
</script>
<!--kg-card-end: html-->
<h2 id="ai-workflow-automation-tools-faqs">AI workflow automation tools: FAQs</h2><h3 id="what-is-the-most-customizable-ai-automation-tool">What is the most customizable AI automation tool?</h3><p>n8n tops the list as the most flexible option, offering source-available code, custom JavaScript and Python code steps, and the ability to self-host. While <a href="https://blog.n8n.io/open-source-zapier/">tools like Zapier</a>, Make, and Lindy are more closed down and abstract complexity behind the scenes &#x2014; making them suitable for beginners &#x2014; they lack the deep customization needed for advanced workflows.</p><p>For all-around needs and ultimate flexibility, choose n8n. If you&apos;re looking for niche tools, check out Gumloop, Agentforce, AirOps, and Workato.</p><h3 id="what-vendor-provides-the-most-extensible-ai-automation-platform">What vendor provides the most extensible AI automation platform?</h3><p>n8n leads in extensibility with custom coding options in addition to preconfigured nodes for major integration partners. Zapier offers the most pre-built connectors (8000+), but is more closed down. Most other tools on this list offer a mix of limited integrations alongside varying degrees of customization features.</p><h3 id="who-offers-the-best-pricing-for-ai-powered-workflow-tools">Who offers the best pricing for AI-powered workflow tools?</h3><p>n8n is the only tool that offers a free self-hosting option. Paid cloud plans start at $20/month for 2,500 workflow executions with unlimited steps &#x2014; <a href="https://blog.n8n.io/n8n-execution-advantage/">notably different from variable credit-based pricing</a>. Most tools on this list offer limited free plans, though ChatGPT Agent Builder requires a paid subscription.</p><p>Workato and Agentforce are priced for enterprise, with significantly higher costs. n8n and Zapier charge by workflow execution, while others charge by workflow steps, which can make budgeting challenging as costs scale unpredictably.</p><h3 id="what-platforms-support-visual-workflow-design-using-ai">What platforms support visual workflow design using AI?</h3><p>All tools on this list offer some level of visual workflow design, with n8n, Zapier, Make, Lindy, and Gumloop being the most approachable. Rather than hopping between platforms to leverage different strengths, you&apos;re better off choosing a tool flexible enough to handle both simple and advanced workflows &#x2014; this avoids the overhead of learning multiple platforms and UIs.</p><h3 id="what-is-the-most-flexible-ai-automation-solution-for-developers">What is the most flexible AI automation solution for developers?</h3><p>n8n is the clear choice for developers because it offers real code fallback in JavaScript and Python in addition to preconfigured integration nodes, plus source-available licensing. Unlike closed systems like Agentforce, n8n gives developers the freedom to customize at any level they need.</p><h3 id="what-is-the-most-secure-ai-workflow-automation-platform">What is the most secure AI workflow automation platform?</h3><p>Workato stands out for enterprise security with SOC 2 Type II compliance, comprehensive RBAC, centralized governance, and guaranteed SLAs. n8n also offers strong security features, including SOC 2 compliance, source-available code for auditing, secret management via AWS/GCP/Azure/Vault, and enterprise-grade RBAC.</p><h3 id="what-is-the-best-ai-workflow-automation-tool-for-enterprise-teams">What is the best AI workflow automation tool for enterprise teams?</h3><p>Workato is purpose-built for enterprise with robust security, compliance, and governance features. However, n8n is also an excellent choice for enterprise teams, offering the security features enterprises require while providing superior flexibility for cross-departmental workflows. Most tools on this list offer enterprise tiers, but features vary significantly.</p><h3 id="who-provides-the-best-scalable-ai-automation-for-technical-teams">Who provides the best scalable AI automation for technical teams?</h3><p>n8n and Workato both provide <a href="https://blog.n8n.io/the-n8n-scalability-benchmark/">excellent scalability</a> for technical teams, though Workato has a more narrow focus on sales and marketing use cases. n8n&apos;s combination of technical flexibility, execution-based pricing (vs. credit-per-step), and source-available code makes it ideal for technical teams building complex, scalable automations.</p><h2 id="wrap-up">Wrap up</h2><p>There are numerous AI workflow automation tools available on the market, catering to a wide range of specialty use cases, business types, and business sizes.</p><p>n8n offers the most flexibility, while Make, Zapier, Gumloop, and Lindy are good options for beginners and non-technical users seeking a more managed experience. AirOps, Workato, and Agentforce offer specialized solutions for sales and marketing needs, while ChatGPT Agent Builder is an exciting new option for existing ChatGPT paid subscribers.</p><p>Picking the right tool for your needs comes down to just a few factors:</p><ul><li>Your level of technical expertise and genuine interest in learning new technologies.</li><li>The specific tools you want to integrate and the amount of effort you&#x2019;re willing to put forth for the ideal setup.</li><li>The nature of your workflows and desired flexibility, as well as non-negotiable security and compliance features.</li><li>Your industry and the importance of using a tool tailored specifically for it.</li></ul><p>But most importantly: just get started! The best way to understand which tool fits your needs is hands-on experience.</p><p>Beyond each platform&apos;s native resources, explore third-party courses (like LinkedIn Learning&apos;s n8n course by Morten Rand-Hendriksen), join communities where builders share workflows and insights (such as Lenny&apos;s Community, which has an n8n partnership), and connect with fellow AI enthusiasts on LinkedIn or in tool-specific Slack channels. Sharing what you&apos;re building not only accelerates learning but often leads to unexpected opportunities and collaborations.</p><p><a href="https://app.n8n.cloud/register">Try n8n for free</a> and experience the difference in automation.</p>]]></content:encoded></item><item><title><![CDATA[Build a fast, deep research automation flow with Oxylabs and n8n]]></title><description><![CDATA[Learn about web scraping and how to to build an automated web intelligence workflow.]]></description><link>https://blog.n8n.io/build-a-fast-deep-research-automation-flow-with-oxylabs-and-n8n/</link><guid isPermaLink="false">6911f229b522e10001500177</guid><category><![CDATA[By partners]]></category><dc:creator><![CDATA[Marketing team]]></dc:creator><pubDate>Mon, 24 Nov 2025 16:48:29 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/11/oxylabs-dl.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-9c831db6-f6fe-4c6e-82e1-8f035a5cdf2a.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="854" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-9c831db6-f6fe-4c6e-82e1-8f035a5cdf2a.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-9c831db6-f6fe-4c6e-82e1-8f035a5cdf2a.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-9c831db6-f6fe-4c6e-82e1-8f035a5cdf2a.png 1600w" sizes="(min-width: 720px) 720px"></figure><img src="https://blog.n8n.io/content/images/2025/11/oxylabs-dl.png" alt="Build a fast, deep research automation flow with Oxylabs and n8n"><p><strong><em>This Verified Node Spotlight was written by </em>Vytenis Kaubr&#x117;<em>, Content Researcher and Technical Copywriter for Oxylabs.</em></strong></p><p>Web scraping sounds simple until you hit anti-bot systems, CAPTCHA challenges, and IP blocks. Building reliable pipelines demands expertise HTTP request strategies, HTML parsing, and infrastructure scaling. The technical overhead can prevent many automation projects from launching.</p><p>Read this article to learn how to build a fast, deep research agent using Oxylabs AI Studio in n8n. No coding required. Automatically handle sophisticated anti-scraping systems, parse any website, and scale reliably for production use.</p><h2 id="what-is-web-scraping">What Is Web Scraping?</h2><p>Web scraping is the automated extraction of public data from websites. Instead of manually copying information, scripts or tools programmatically retrieve and structure the data you need. Traditionally, this is done by writing scripts in programming languages like Python, JavaScript, or C#, though modern solutions offer simpler approaches.</p><p><strong>Common use cases:</strong></p><ul><li><strong>E-commerce:</strong> Track competitor pricing and product availability in real-time</li><li><strong>AI development:</strong> Collect large datasets for training machine learning models</li><li><strong>Market research:</strong> Gather consumer behavior insights and industry trends</li><li><strong>Brand protection:</strong> Monitor online platforms for counterfeit products and trademark violations</li><li><strong>SEO monitoring:</strong> Track keyword rankings and analyze competitor performance</li><li><strong>Travel aggregation:</strong> Collect flight prices, hotel rates, and customer reviews</li><li><strong>Ad verification:</strong> Ensure ads display correctly across different platforms</li></ul><h2 id="top-challenges-of-web-scraping">Top Challenges of Web Scraping</h2><p>Extracting public data from the web presents several obstacles:</p><ul><li><strong>Technical expertise barrier:</strong> Requires proficiency in programming, HTTP protocols, and HTML/CSS selectors</li><li><strong>Anti-bot defenses:</strong> Websites deploy CAPTCHAs, IP blocking, and fingerprinting that require sophisticated bypassing techniques</li><li><strong>Data extraction complexity:</strong> Each website structures HTML differently, demanding custom parsers that break when sites redesign</li><li><strong>Infrastructure scaling:</strong> Large operations need distributed systems, proxy rotation, and monitoring that divert resources from core business goals</li></ul><h2 id="building-a-fast-deep-research-flow-in-n8n">Building a Fast Deep Research Flow in n8n</h2><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-cb506111-6d45-4be3-ac19-278c81a9c552.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="775" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-cb506111-6d45-4be3-ac19-278c81a9c552.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-cb506111-6d45-4be3-ac19-278c81a9c552.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-cb506111-6d45-4be3-ac19-278c81a9c552.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>These challenges don&apos;t mean web scraping is impossible. The right tools can handle the complexity for you, which is exactly what this workflow does.</p><p><a href="https://oxylabs.go2cloud.org/aff_c?offer_id=7&amp;aff_id=1826&amp;url_id=176" rel="noreferrer"><u>Oxylabs AI Studio</u></a> is a <strong>low-code web scraping solution</strong> that eliminates technical barriers. Instead of writing code, you <strong>describe what data you need in plain English</strong>. It has built-in proxy servers, automatic data parsing, a headless browser, and scales on demand. Oxylabs AI Studio integrates with n8n as both a node and a tool, providing purpose-built resources for different scraping needs:</p><ul><li><a href="https://aistudio.oxylabs.io/apps/search"><strong><u>Search</u></strong></a><strong>:</strong> Scrape Google Search and optionally extract content from each result</li><li><a href="https://aistudio.oxylabs.io/apps/scrape"><strong><u>Scraper</u></strong></a><strong>:</strong> Scrape any website and get Markdown or structured JSON/CSV by describing your needs in plain English</li><li><a href="https://aistudio.oxylabs.io/apps/crawl"><strong><u>Crawler</u></strong></a><strong>:</strong> Crawl entire websites to find relevant pages using natural language prompts</li><li><a href="https://aistudio.oxylabs.io/apps/browser_agent"><strong><u>Browser Agent</u></strong></a><strong>: </strong>Control a web browser with natural language and extract data</li></ul><h3 id="project-overview">Project Overview</h3><p>This deep research workflow:</p><ol><li>Analyzes the user&apos;s question and generates 3 strategic Google search queries</li><li>Scrapes Google search results for each query</li><li>Identifies the most relevant and authoritative sources for analysis</li><li>Scrapes and summarizes each source in parallel</li><li>Produces a comprehensive analysis report combining all insights</li></ol><h3 id="prerequisites">Prerequisites</h3><p>You&apos;ll need just two things in addition to your n8n instance:</p><ul><li><strong>Oxylabs AI Studio API key</strong> &#x2013;<a href="https://aistudio.oxylabs.io/register"> <u>Get a free API key</u></a> with 1000 credits</li><li><strong>OpenAI API key</strong> (or alternatives like Claude, Gemini, or local Ollama LLMs)</li></ul><div class="kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    " data-layout="minimal">
            
            <div class="kg-cta-content">
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p><span style="white-space: pre-wrap;">Follow along this tutorial in n8n.</span></p>
                        </div>
                    
                    
                        <a href="https://n8n.io/workflows/10504-automate-web-research-and-analysis-with-oxylabs-and-gpt-for-comprehensive-reports/" class="kg-cta-button kg-style-accent" style="color: #FFFFFF;">
                            Download the workflow template
                        </a>
                        
                    </div>
                
            </div>
        </div><h3 id="step-1-create-google-search-queries">Step 1: Create Google Search Queries</h3><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-60527341-aba0-4a6c-ba80-79594f42070e.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="473" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-60527341-aba0-4a6c-ba80-79594f42070e.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-60527341-aba0-4a6c-ba80-79594f42070e.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-60527341-aba0-4a6c-ba80-79594f42070e.png 1600w" sizes="(min-width: 720px) 720px"></figure><ol><li>Add the <strong>When chat message received</strong> trigger node</li><li>Add the <strong>OpenAI</strong> node &gt; <strong>Message a model</strong> action</li><li>Add the <strong>Split Out</strong> node</li></ol><p>This setup lets you <strong>input any message</strong> through n8n&apos;s chat interface. The LLM then analyzes your message and <strong>generates strategic Google Search queries</strong> to uncover different aspects of your topic. The system prompt creates 3 search queries by default (adjust this number as needed).</p><h3 id="step-2-scrape-google-select-relevant-urls">Step 2: Scrape Google &amp; Select Relevant URLs</h3><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-14c9f216-b5ac-440f-875b-ca9dcdd75f80.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="472" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-14c9f216-b5ac-440f-875b-ca9dcdd75f80.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-14c9f216-b5ac-440f-875b-ca9dcdd75f80.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-14c9f216-b5ac-440f-875b-ca9dcdd75f80.png 1600w" sizes="(min-width: 720px) 720px"></figure><ol><li>Add the <strong>Oxylabs AI Studio</strong> node &gt; <strong>Search</strong> resource</li><li>Add the <strong>OpenAI</strong> node &gt; <strong>Message a model</strong> action</li><li>Add the <strong>Split Out</strong> node</li></ol><p>Here, we scrape Google SERPs using the generated search queries, then filter for the most <strong>relevant and authoritative sources</strong>.</p><p>If you haven&apos;t already, install the Oxylabs AI Studio node as shown<a href="https://n8n.io/integrations/oxylabs-ai-studio"> <u>on this page</u></a> and add it to your workflow. Remember, you can claim a<a href="https://aistudio.oxylabs.io/api-key"> <strong><u>free Oxylabs AI Studio API key</u></strong></a> with 1000 credits.</p><p>The Oxylabs AI Studio <strong>Search</strong> resource offers powerful capabilities:</p><ul><li><strong>Limit:</strong> Returns up to 50 search results per query</li><li><strong>Return Contents:</strong> Extracts content from each search result</li><li><strong>Render JavaScript:</strong> Uses a headless browser to capture dynamic content</li></ul><h3 id="step-3-scrape-summarize-content-in-parallel">Step 3: Scrape &amp; Summarize Content in Parallel</h3><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-2715bf73-aa07-42ef-a1b3-7585063344f9.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="860" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-2715bf73-aa07-42ef-a1b3-7585063344f9.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-2715bf73-aa07-42ef-a1b3-7585063344f9.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-2715bf73-aa07-42ef-a1b3-7585063344f9.png 1600w" sizes="(min-width: 720px) 720px"></figure><ol><li>Add the <strong>Oxylabs AI Studio</strong> node &gt; <strong>Scraper</strong> resource</li><li>Add the <strong>OpenAI</strong> node &gt; <strong>Message a model</strong> action</li><li><a href="https://docs.n8n.io/data/data-tables/"><u>Create a data table</u></a></li><li>Add the <strong>Data table</strong> node &gt; <strong>Insert row</strong> action</li><li>Convert these <strong>3 nodes</strong> into a <strong>sub-workflow</strong></li><li>Add the <strong>Wait</strong> node</li></ol><p>In this step, Oxylabs AI Studio scrapes each selected URL and returns content in clean Markdown format instead of raw HTML. To ensure quality analysis, we summarize each piece of content to extract key insights, then save each summary as a row in n8n&apos;s data table.</p><p>To dramatically speed up processing, save the 3 nodes as a sub-workflow. This will ensure that each URL is scraped and analyzed in parallel at the same time, instead of one by one. After creating the sub-workflow, <strong>enable parallel processing</strong> with these settings:</p><ul><li><strong>Mode:</strong> Run once for each item</li><li><strong>Options</strong> &gt; <strong>Add Option</strong> &gt; <strong>disable</strong> &#x201C;Wait For Sub-Workflow Completion&#x201D;</li></ul><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-bb146be9-636a-4fa7-8a94-42f8f322d26c.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1500" height="1522" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-bb146be9-636a-4fa7-8a94-42f8f322d26c.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-bb146be9-636a-4fa7-8a94-42f8f322d26c.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-bb146be9-636a-4fa7-8a94-42f8f322d26c.png 1500w" sizes="(min-width: 720px) 720px"></figure><h3 id="step-4-wait-for-data">Step 4: Wait for Data</h3><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-48518bac-ec6a-40bd-97b1-36612d4b33e7.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="910" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-48518bac-ec6a-40bd-97b1-36612d4b33e7.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-48518bac-ec6a-40bd-97b1-36612d4b33e7.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-48518bac-ec6a-40bd-97b1-36612d4b33e7.png 1600w" sizes="(min-width: 720px) 720px"></figure><ol><li>Add the <strong>Edit Fields (Set)</strong> node</li><li>Add the <strong>Data table</strong> node &gt; <strong>Get row(s)</strong> action</li><li>Add the <strong>Aggregate</strong> node</li><li>Add the <strong>If</strong> node</li><li>Add the <strong>Wait</strong> node</li><li><strong>Loop </strong>through 2, 3, 4, and 5 nodes</li></ol><p>Since the sub-workflow runs in parallel, we need to wait for completion. While you could add a fixed <strong>Wait</strong> node that waits for 2-3 minutes, the dynamic approach is better. It checks whether the <strong>expected number of results exists in the data table</strong> by comparing the number of URLs sent to the sub-workflow against the last row ID. If they don&apos;t match, it waits 10 seconds and checks again. When they match, the loop exits for final processing.</p><h3 id="step-5-create-final-analysis">Step 5: Create Final Analysis</h3><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-cdc87d52-8deb-4b21-90ca-e091b1230d45.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="542" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-cdc87d52-8deb-4b21-90ca-e091b1230d45.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-cdc87d52-8deb-4b21-90ca-e091b1230d45.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-cdc87d52-8deb-4b21-90ca-e091b1230d45.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>Connect these nodes to the <strong>True</strong> branch of the <strong>If</strong> node:</p><ol><li>Add the <strong>Data table</strong> node &gt; <strong>Get row(s)</strong> action</li><li>Add the <strong>OpenAI</strong> node &gt; <strong>Message a model</strong> action</li></ol><p>Once all summaries are ready, we read the entire table and pass the data to AI for synthesis. It creates a comprehensive and actionable report structured in Markdown. Here&apos;s an example output snippet about building a summer house:</p><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/11/data-src-image-607901f2-ff51-4a1b-8c07-ea73352cd32e.png" class="kg-image" alt="Build a fast, deep research automation flow with Oxylabs and n8n" loading="lazy" width="1600" height="656" srcset="https://blog.n8n.io/content/images/size/w600/2025/11/data-src-image-607901f2-ff51-4a1b-8c07-ea73352cd32e.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/11/data-src-image-607901f2-ff51-4a1b-8c07-ea73352cd32e.png 1000w, https://blog.n8n.io/content/images/2025/11/data-src-image-607901f2-ff51-4a1b-8c07-ea73352cd32e.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>To reuse this workflow, remember to clear the data table after the final analysis by adding a <strong>Data table</strong> node with the <strong>Delete row(s)</strong> action.</p><h2 id="next-steps">Next steps</h2><p>The combination of n8n and Oxylabs AI Studio eliminates technical barriers: no proxy management, no anti-bot workarounds, no parsers and scrapers to maintain. Feel free to use this workflow as a foundation for your own automation pipelines.</p><div class="kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    " data-layout="minimal">
            
            <div class="kg-cta-content">
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p><span style="white-space: pre-wrap;">Try it now by downloading the template from n8n.</span></p>
                        </div>
                    
                    
                        <a href="https://n8n.io/workflows/10504-automate-web-research-and-analysis-with-oxylabs-and-gpt-for-comprehensive-reports/" class="kg-cta-button kg-style-accent" style="color: #FFFFFF;">
                            Get the workflow
                        </a>
                        
                    </div>
                
            </div>
        </div><p><strong>Ways to expand:</strong></p><ul><li>Connect nodes like Google Sheets, Notion, Airtable, or webhooks to route results where you need them</li><li>Explore other AI Studio apps like Browser Agent for interactive browser control or Crawler for mapping entire websites</li><li>Adjust the system prompts in LLM nodes to fit your specific research goals</li><li>Scale up by processing more search queries, increasing results per query beyond 10, and selecting additional relevant URLs</li></ul><h2 id="useful-resources">Useful resources</h2><p><a href="https://aistudio.oxylabs.io/apps"><u>Oxylabs AI Studio applications</u></a></p><p><a href="https://github.com/oxylabs/n8n-nodes-oxylabs-ai-studio"><u>AI Studio node in n8n</u></a></p><p><a href="https://oxylabs.io/resources/integrations/n8n"><u>Oxylabs and n8n tutorial</u></a></p><div class="kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    " data-layout="minimal">
            
            <div class="kg-cta-content">
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p><span style="white-space: pre-wrap;">Interested in building your own node so others can install it directly from the n8n Editor? </span></p>
                        </div>
                    
                    
                        <a href="https://docs.n8n.io/integrations/creating-nodes/build/reference/verification-guidelines/" class="kg-cta-button kg-style-accent" style="color: #FFFFFF;">
                            Become a verified node partner
                        </a>
                        
                    </div>
                
            </div>
        </div>]]></content:encoded></item><item><title><![CDATA[n8n Expands Whats Possible in Enterprise Automation with Microsoft Agent 365]]></title><description><![CDATA[A new integration with Microsoft Agent 365 lets teams build agentic systems that connect n8ns AI orchestration layer and automation ecosystem with Microsofts productivity suite.]]></description><link>https://blog.n8n.io/n8n-expands-whats-possible-in-enterprise-automation-with-microsoft-agent-365/</link><guid isPermaLink="false">6916137ef24a700001f9f87f</guid><dc:creator><![CDATA[Marketing team]]></dc:creator><pubDate>Tue, 18 Nov 2025 16:31:39 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/11/image--67-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/11/image--67-.png" alt="n8n Expands What&#x2019;s Possible in Enterprise Automation with Microsoft Agent 365"><p>n8n is expanding what&#x2019;s possible for AI orchestration and automation in the enterprise. Today, we&#x2019;re excited to announce a new integration with Microsoft Agent 365. Agents built on n8n will soon gain extended capabilities offered by Microsoft Agent 365 when developers include the Microsoft Agent 365 node in their workflows.&#xA0;&#xA0;</p><p><strong>Expanding the Boundaries of Enterprise Automation</strong></p><p>With this new integration, agents built on n8n can act within Microsoft environments such as Microsoft Word and Outlook using their own company-issued identity and permissions. Managed similarly to employee accounts, they can be provisioned to securely collaborate, assist, and perform actions within the Microsoft technology stack.</p><p>n8n agents can access a broad AI orchestration and automation layer that includes modular connections to LLMs, vector stores, and enterprise tools like Jira, PagerDuty, ServiceNow, and ElevenLabs, along with more than 1,200 built-in integrations and 6,000+ community workflow templates. Beyond these connections, agents can use advanced n8n capabilities such as invoking other agents as tools, launching sub-workflows, and combining reasoning, deterministic logic, and human steps to build intelligent systems with guardrails. Enterprises can further ensure reliability through built&#x2011;in evaluation and testing features, with error workflows that trigger automatically when something goes wrong. These capabilities help teams keep agentic workflows reliable, accountable, and scalable across environments.</p><blockquote>&#x201C;At n8n, we help teams put AI orchestration and automation to work reliably inside their businesses. Our platform connects agents to the business-critical tools and data they rely on, while enabling AI monitoring and human oversight. This integration with Microsoft Agent 365 allows developers using n8n capabilities to create agents that can be part of Microsoft agent management and identity ecosystem and collaborate more seamlessly with Microsoft offerings, where many enterprises already run their most important work.&#x201D; &#x2014; Jan Oberhauser, Founder and CEO of n8n</blockquote><p>For n8n builders, the integration also opens doors into Microsoft&#x2019;s enterprise ecosystem. Workflows can now reach end users directly in the tools they use every day, like Word and Outlook, enabling richer automations and agentic use cases that engage employees where work actually happens. This shift not only expands what builders can deliver but also reinforces Microsoft&#x2019;s focus on giving organizations choice, security, and control in how they extend automation and AI across their business.</p><blockquote>&#x201C;We&#x2019;re pleased to welcome n8n to the Microsoft Agent 365 ecosystem and collaborate to help customers innovate faster and stay secure, bringing agents into the same trusted environment as users with familiar infrastructure, apps, and protections.&#x201D; &#xA0;&#x2014; Nirav Shah, Corporate Vice President, Microsoft Agent 365</blockquote><p><strong>Where Agentic Workflows Meet Work</strong></p><p>The impact of agentic automation is clearest in daily work. By connecting directly into Microsoft 365 applications, agents built with n8n and enabled with Microsoft Agent 365 engage employees where they already collaborate, communicate, and make decisions. Workflows surface in Outlook, SharePoint, and Teams to deliver updates, trigger actions, and coordinate tasks in context, reducing the manual effort of tracking information across systems and ensuring that the right people see the right information at the right time.</p><p>Consider an engineering manager who once spent hours tracking customer reported incidents, high&#x2011;priority bugs, and upcoming releases across multiple systems. Each day began with digging through tickets, gathering updates, and chasing the right people for context. The work kept projects moving but left little time for improvement or strategy.</p><p>Now she deploys an agent enabled with Microsoft Agent 365 built with n8n. Acting as a digital colleague, the agent has permission to access relevant Microsoft 365 applications and internal data to stay on top of priorities. Through its n8n&#x2011;powered workflow, the agent connects to Jira and PagerDuty, combining deterministic logic with agentic reasoning to generate daily summaries and highlight long-standing incidents. It can then post updates in Teams, and schedule follow-up meetings with ticket owners. What once took hours of manual coordination is now managed automatically with context, intelligence, and the right level of autonomy.</p><p><strong>How It Works in n8n</strong></p><p>At this time, agents built with n8n are capable of being provisioned with a Microsoft Agent 365 license using the <strong>Webhook Trigger</strong> and <strong>AI Agent nodes</strong>. This setup authenticates the agent within a Microsoft tenant and enables communication between Microsoft 365 services. Once configured, the agent can securely exchange information between Microsoft 365 and n8n workflows.</p><p>Soon, a dedicated <strong>Microsoft A365 Agent Trigger node</strong> will simplify this process, removing the need for manual setup and bringing a more streamlined experience directly inside n8n.</p><p><a href="https://learn.microsoft.com/en-us/microsoft-agent-365/developer/samples?tabs=nodejs" rel="noreferrer">Learn more</a> and&#xA0;explore&#xA0;the <a href="https://github.com/microsoft/Agent365-Samples/tree/main/nodejs/n8n/sample-agent" rel="noreferrer">n8n Sample Agent</a>.</p><p><strong>Unlocking Value Where Work Happens</strong></p><p>This integration makes agents created with n8n&#x2019;s source available, code&#x2011;extensible AI orchestration and automation platform capable of being provisioned with a Microsoft Agent 365 license, enabling them to act directly within the Microsoft tools enterprises rely on. The result is agentic automation that fits naturally into the flow of everyday work while giving organizations greater value from their AI agents with governance and visibility built in. </p><p></p>]]></content:encoded></item><item><title><![CDATA[Implementing Rerankers in Your AI Workflows]]></title><description><![CDATA[Boost your RAG pipelines accuracy with rerankers that reorder results by true semantic relevancedeliver smarter, faster, and more precise AI search!]]></description><link>https://blog.n8n.io/implementing-rerankers-in-your-ai-workflows/</link><guid isPermaLink="false">68f0ec722493b40001f20173</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Andrew Green]]></dc:creator><pubDate>Thu, 16 Oct 2025 13:18:51 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/10/workflow-template--2---1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/10/workflow-template--2---1-.png" alt="Implementing Rerankers in Your AI Workflows"><p>In a Retrieval-Augmented Generation (RAG) pipeline, a retriever fetches a set of candidate documents using a coarse filter via a vector similarity search. When these documents are retrieved, they are not ordered, so a less relevant document may be used before a more relevant one.&#xA0;</p><p>This is where we apply reranking models, which offer a second pass at the retrieval process to sort the retrieved text by semantic relevance with respect to the user&#x2019;s query.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-6fedfd5d-48f2-4ba0-8ae8-db93c5d2564b.png" class="kg-image" alt="Implementing Rerankers in Your AI Workflows" loading="lazy" width="1600" height="1131" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-6fedfd5d-48f2-4ba0-8ae8-db93c5d2564b.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-6fedfd5d-48f2-4ba0-8ae8-db93c5d2564b.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-6fedfd5d-48f2-4ba0-8ae8-db93c5d2564b.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Source: https://www.pondhouse-data.com/blog/advanced-rag-colbert-reranker</em></i></figcaption></figure><p>You can get an intuitive understanding of rerankers by thinking of search engines. When searching for something on Google, we expect that the most relevant results will be displayed on the first page. As we go through the following pages, the relevance of the results with respect to the query would decrease.</p><h2 id="how-rerankers-improve-retrieval-quality">How rerankers improve retrieval quality</h2><p>The reranker can significantly improve the search quality because it operates at a sub-document and sub-query level. It uses a more precise model, such as a transformer that jointly reads the query and the document to scrutinize the candidate texts.&#xA0;</p><p>The reranking process is straightforward:</p><ul><li>Retrieve: First, a search system uses vector embeddings to find a broad set of potentially relevant documents based on the user&apos;s query.</li><li>Analyze: Then, the reranker takes these results and analyzes their semantic content considering the nuances of how the query terms interact with the document content.</li><li>Reorder: Lastly, the model reorders the search results, placing the ones it deems most relevant at the top, based on this deeper analysis.</li></ul><p>The AI ecosystem offers a range of open-source and commercial tools that can help improve RAG pipelines using rerankers. In the following section, we will present some options for deploying rerankers, followed by open-source and commercial tools.</p><h2 id="how-to-deploy-rerankers">How to deploy rerankers</h2><p>Rerankers are standalone models that must be run independently. You can either choose to deploy and run the model directly or consume it in an as-a-service fashion. As such, you have three options:</p><ol><li>An -aaS delivery via an API</li><li>A cloud-hosted option, or</li><li>A self-hosted option for local deployments.</li></ol><h3 id="option-1-as-a-service-aas">Option 1: as-a-Service (aaS)</h3><p><strong>The API model</strong> is perhaps the most straightforward approach to implementing rerankers. These are available from commercial solutions such as Cohere and Jina. This method allows developers to integrate reranking capabilities into their RAG pipelines with minimal infrastructure overhead. The commercial solutions expose API endpoints where users submit a query along with a list of retrieved documents, and the service returns these documents reordered from most to least semantically relevant. The underlying architecture typically processes user input by chunking documents and computing relevance scores for each segment, with the final document score determined by the highest-scoring chunk.&#xA0;</p><h3 id="option-2-cloud-hosted-deployments">Option 2: Cloud-hosted deployments</h3><p><strong>Cloud-hosted deploymen</strong>t involves deploying reranker models through major cloud providers&apos; AI platforms, combining the robustness and scalability of cloud infrastructure with the performance of commercial reranking models. This deployment method is particularly beneficial for organizations that require consistent performance, automatic scaling, and integration with existing cloud-based data pipelines. While not as convenient as the API option, hosting the model in your cloud minimizes dependency on the third-party vendor and can deliver on any security mandates, compliance certifications, and service level agreements.</p><h3 id="option-3-self-hosted-deployments">Option 3: Self-hosted deployments</h3><p><strong>Self-hosted deployments </strong>allow enterprises to run reranker models within their own infrastructure, such that no data has to be processed by a third party. It also offers flexibility to customize deployment configurations, optimize for specific hardware, and integrate with existing enterprise systems. While this approach requires more technical expertise and infrastructure management, it delivers the benefits of real-time reranking with minimal latency while maintaining full control over data privacy and security protocols.</p><h2 id="open-source-reranking-tools">Open source reranking tools</h2><p>Some of the most notable open source tools for reranking include the following:</p><p><a href="https://github.com/stanford-futuredata/ColBERT"><u>ColBERT</u></a> is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds. It relies on fine-grained contextual late interaction, encoding each passage into a matrix of token-level embeddings. At search time, it embeds every query into another matrix and efficiently finds passages that contextually match the query using scalable vector-similarity (MaxSim) operators.</p><p><a href="https://github.com/PrithivirajDamodaran/FlashRank"><u>FlashRank</u></a> uses Pairwise or Listwise rerankers. It&#x2019;s a Python library that adds re-ranking to your existing search &amp; retrieval pipelines.&#xA0;</p><p><a href="https://arxiv.org/pdf/2312.02724"><u>RankZephyr</u></a> is an open-source large language model (LLM) for listwise zero-shot reranking. It&#x2019;s built on the 7-billion parameter Zephyr-&#x3B2; model (based on Mistral), and uses instruction fine-tuning to distill reranking capabilities from both RankGPT-3.5 and RankGPT-4 without requiring human-annotated query-passage relevance pairs.&#xA0;</p><h2 id="commercial-reranking-providers">Commercial reranking providers</h2><p>Some examples of commercial rerankers include Cohere and Jina.&#xA0;</p><p><strong>Cohere&apos;s</strong> reranker model employs cross-attention mechanisms for fine-grained ranking, enabling direct query-document comparison that significantly improves result quality for complex and under-specified queries. The model offers multilingual capabilities, supporting over 100 languages and delivering accurate retrieval across language boundaries from international and multilingual datasets. It can handle complex enterprise data formats, ranking multi-aspect and semi-structured documents&#x2014;including emails, tables, JSON, and code&#x2014;with the same precision as traditional long-form text. The solution is designed for enterprise deployment flexibility, offering private deployment options in virtual private clouds or on-premises environments for maximum data privacy and security control, while also being available through Cohere&apos;s platform and trusted cloud providers.&#xA0;</p><p><strong>Jina Reranke</strong>r offers comprehensive multilingual retrieval capabilities across over 100 languages, enabling effective document retrieval regardless of the query language used. The model features specialized function-calling support and advanced code search capabilities, allowing it to rank code snippets and function signatures based on natural language queries, making it particularly well-suited for Agentic RAG applications. Additionally, Jina Reranker v2 provides robust tabular and structured data support, effectively ranking the most relevant tables based on natural language queries and helping to sort different table schemas to identify the most appropriate one before SQL query generation, making it a versatile solution for enterprise environments with diverse data formats and multilingual requirements.</p><h2 id="how-reranking-works">How reranking works</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-507fecff-dde2-4a7b-b7a2-cdfee0c26a48.png" class="kg-image" alt="Implementing Rerankers in Your AI Workflows" loading="lazy" width="479" height="267"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Source: https://www.sbert.net/examples/cross_encoder/applications/README.html</em></i></figcaption></figure><p>Bi-encoders and cross-encoders are two architectures used in natural language processing (NLP) for tasks like text similarity, retrieval, or ranking.<strong> Cross-Encoders can be used</strong> whenever you have a pre-defined set of sentence pairs you want to score. For example, you have 100 sentence pairs and you want to get similarity scores for these 100 pairs.</p><p><strong>Bi-Encoders are used</strong> whenever you need a sentence embedding in a vector space for efficient comparison. Applications are for example, Information Retrieval / Semantic Search or Clustering.&#xA0;</p><p><strong>Cross-Encoder achieve higher performance than Bi-Encoders</strong>, however, they do not scale well for large datasets. Clustering 10,000 sentences with CrossEncoders would require computing similarity scores for about 50 million sentence combinations, which takes about 65 hours. With a Bi-Encoder, you compute the embedding for each sentence, which takes only 5 seconds.</p><p>You can <strong>combine Cross- and Bi-Encoders</strong>. For example, you can use an efficient Bi-Encoder to retrieve the top-100 most similar sentences for a query, then a Cross-Encoder to re-rank these 100 hits by computing the score for every (query, hit) combination.</p><h3 id="llm-based-reranking">LLM-based reranking</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-d2afe3ea-21be-4aa7-9e58-fe5e54e8178d.png" class="kg-image" alt="Implementing Rerankers in Your AI Workflows" loading="lazy" width="1600" height="1544" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-d2afe3ea-21be-4aa7-9e58-fe5e54e8178d.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-d2afe3ea-21be-4aa7-9e58-fe5e54e8178d.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-d2afe3ea-21be-4aa7-9e58-fe5e54e8178d.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Source: </em></i><a href="https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6"><u><i><em class="italic underline" style="white-space: pre-wrap;">https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6</em></i></u></a></figcaption></figure><p>Just as when evaluating RAG, you can use an LLM to rerank retrieved documents. LLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost.&#xA0; At a high-level, this approach uses the LLM to decide which documents and chunks are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.</p><p>If you are using LlamaInde, the LLMRerank is a module baked framework as part of the NodePostprocessor abstraction.</p><h2 id="reranking-in-n8n">Reranking in n8n</h2><p>You can easily implement reranking in n8n using<a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.rerankercohere/"><u> the Reranker Cohere node.</u></a> It allows users to rerank the resulting chunks from a vector store. You can connect this node to a vector store. The reranker reorders the list of documents retrieved from a vector store for a given query in order of descending relevance.</p><p>This<a href="https://n8n.io/workflows/6401-document-based-ai-chatbot-with-rag-openai-and-cohere-reranker/"><u> Intelligent AI chatbot with RAG and Cohere Reranker</u></a> workflow template creates an intelligent AI assistant that combines RAG with Cohere&apos;s reranking technology to ensure the most relevant information is prioritized.</p>
<!--kg-card-begin: html-->
<div class="content-banner">
  <div>
    <h3>Smarter RAG workflows</h3>
    <p>Prioritize the most relevant answers automatically with reranking.</p>
  </div>
  <a href="https://app.n8n.cloud/register" class="global-button blog-banner-signup">Start building</a>
</div>
<!--kg-card-end: html-->
<h4 id></h4>]]></content:encoded></item><item><title><![CDATA[AI Agent Orchestration Frameworks: Which One Works Best for You?]]></title><description><![CDATA[Discover AI agent orchestration frameworks like n8n, LangGraph, CrewAI that power scalable, multi-agent AI orchestration in 2025.]]></description><link>https://blog.n8n.io/ai-agent-orchestration-frameworks/</link><guid isPermaLink="false">68ed1b4eb2896000019a2308</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Yulia Dmitrievna]]></dc:creator><pubDate>Thu, 16 Oct 2025 12:51:31 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/10/User-Meetups---Hangouts_1920--1080_03--1---1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/10/User-Meetups---Hangouts_1920--1080_03--1---1-.png" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?"><p>Your customer service chatbot handles basic inquiries. Your data analysis agent processes reports. Your scheduling system books meetings.</p><p>But oftentimes, your agent needs to do all three things at once.</p><p>The traditional approach of cramming more tools and writing more complex system prompts isn&#x2019;t ideal. As LLM token usage increases, you need the most capable (expensive) models to handle complexity, and there&#x2019;s a practical limit to how many tasks a single agent can effectively manage.</p><p>An alternative approach is splitting large agents into smaller, specialized ones. Instead of one overloaded agent, you coordinate multiple focused agents - each expert in their domain. They work together, sharing context and handing off tasks as needed.</p><p>This coordination requires proper tooling. AI agent orchestration frameworks manage communication between agents, maintain shared state across workflows, and handle task delegation between specialized components.</p><p>In this guide, we examine the essential components that AI agent orchestration frameworks should provide, then review 11 frameworks that approach multi-agent coordination differently - from visual workflow builders to enterprise-grade managed platforms.</p><ul>
<li><a href="#what-are-ai-agent-orchestration-frameworks">What are AI agent orchestration frameworks?</a>
<ul>
<li><a href="#essential-ai-agent-orchestration-components">Essential AI agent orchestration components</a></li>
</ul>
</li>
<li><a href="#11-ai-agent-orchestration-frameworks-of-2025">11 AI agent orchestration frameworks of 2025</a>
<ul>
<li><a href="#n8n">n8n</a></li>
<li><a href="#flowise">Flowise</a></li>
<li><a href="#zapier-agents">Zapier Agents</a></li>
<li><a href="#langgraph">LangGraph</a></li>
<li><a href="#crewai">CrewAI</a></li>
<li><a href="#openai-agentkit">OpenAI AgentKit</a></li>
<li><a href="#amazon-bedrock-agents">Amazon Bedrock Agents</a></li>
<li><a href="#google-agent-development-kit">Google Agent Development Kit</a></li>
<li><a href="#vertex-ai-agent-builder">Vertex AI Agent Builder</a></li>
<li><a href="#microsoft-semantic-kernel-agent-framework">Microsoft Semantic Kernel Agent Framework</a></li>
<li><a href="#azure-ai-foundry-agent-service">Azure AI Foundry Agent Service</a></li>
</ul>
</li>
<li><a href="#benefits-of-using-ai-agent-orchestration-frameworks">Benefits of using AI agent orchestration frameworks</a></li>
<li><a href="#wrap-up">Wrap up</a></li>
<li><a href="#whats-next">What&apos;s next?</a></li>
</ul>
<h2 id="what-are-ai-agent-orchestration-frameworks">What are AI agent orchestration frameworks?</h2><p><strong><em>AI agent orchestration frameworks coordinate multiple specialized agents to accomplish complex workflows that single agents struggle with.</em></strong></p><p>The key difference from traditional AI tools lies in coordination complexity. Instead of managing one conversation with one model, you&apos;re managing state across multiple agents, handling handoffs between specialized systems, and ensuring context doesn&apos;t get lost as tasks flow between agents.</p><h3 id="essential-ai-agent-orchestration-components">Essential AI agent orchestration components</h3><p>Effective frameworks provide five core capabilities:</p><ul><li><strong>State management</strong>: Persistent memory that survives across agent interactions. When your data analysis agent finishes processing and hands off to your scheduling agent, the context needs to transfer seamlessly.</li><li><strong>Communication protocols</strong>: Standardized ways for agents to talk to each other. Whether through structured handoffs, shared chat threads, or event-driven messages.</li><li><strong>Orchestration patterns</strong>: <a href="https://blog.n8n.io/ai-agentic-workflows/"><u>Different coordination approaches</u></a> - sequential pipelines for predictable workflows, parallel execution for speed, or hierarchical structures where supervisor agents manage worker teams.</li><li><strong>Tool integration</strong>: Connecting agents to external systems, APIs, and data sources while managing permissions and error handling across the chain.</li><li><strong>Error recovery</strong>: When one agent fails or produces unexpected results, the framework needs mechanisms to retry, route to alternative agents, or gracefully degrade the workflow.</li></ul><p>These components determine whether your multi-agent system works reliably in production or becomes a complex debugging nightmare.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Read more about the basics of <a href="https://blog.n8n.io/ai-orchestration/"><u>AI orchestration, best practices, and tools</u></a>. The article covers architectural principles and implementation strategies in detail.</div></div><h2 id="11-ai-agent-orchestration-frameworks-of-2025">11 AI agent orchestration frameworks of 2025</h2><p>We&apos;ve organized our analysis into three categories that reflect how the market has evolved:</p><ul><li><strong>Visual and low-code tools</strong> (n8n, Flowise, Zapier Agents) come first &#x2013; these platforms let business teams and developers build agent coordination through graphical interfaces while maintaining technical flexibility.</li><li><strong>Code-first SDKs</strong> (LangGraph, CrewAI, OpenAI Agents SDK, Google ADK, MS Semantic Kernel Agent Framework) follow next &#x2013; code-first frameworks that give technical teams precise control over agent behavior, state management, and communication patterns.</li><li><strong>Enterprise infrastructure platforms</strong> (Amazon Bedrock Agents, Vertex AI Agent Builder and Azure AI Agent Service) conclude our analysis. Here we see an interesting trend: major cloud providers now offer both open-source SDKs for development AND managed infrastructure services for deployment. We&apos;ve separated Google&apos;s and Microsoft&apos;s SDK offerings from their infrastructure services to illustrate this dual-approach strategy.</li></ul><table>
<thead>
<tr>
<th>Framework</th>
<th>Type</th>
<th>Key Features</th>
<th>Prog.<br>Language</th>
<th>Open<br>source?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#n8n">n8n</a></td>
<td>Low-code<br>tool</td>
<td>1000+ integrations,<br>Agent-to-Agent workflows,<br>MCP support,<br>custom JavaScript</td>
<td>JavaScript<br>TypeScript</td>
<td>Fair-code</td>
</tr>
<tr>
<td><a href="#flowise">Flowise</a></td>
<td>Low-code<br>tool</td>
<td>Based on LangChain,<br>visual multi-agent builder,<br>RAG capabilities</td>
<td>JavaScript</td>
<td>&#x2705;</td>
</tr>
<tr>
<td><a href="#zapier-agents">Zapier<br>Agents</a></td>
<td>No-code<br>tool</td>
<td>8000+ app integrations,<br>Agent web browsing tool,<br>business automation focus</td>
<td>N/A</td>
<td>&#x274C;</td>
</tr>
<tr>
<td><a href="#langgraph">LangGraph</a></td>
<td>SDK</td>
<td>Graph-based<br>state management,<br>LangChain ecosystem,<br>human-in-the-loop</td>
<td>Python</td>
<td>&#x2705;</td>
</tr>
<tr>
<td><a href="#crewai">CrewAI</a></td>
<td>SDK</td>
<td>Role-based teams,<br>built from scratch<br>(no LangChain),<br>autonomous collaboration</td>
<td>Python</td>
<td>&#x2705;</td>
</tr>
<tr>
<td><a href="#openai-agentkit">OpenAI<br>AgentKit</a></td>
<td>Low-code<br>SDK</td>
<td>Visual Agent Builder,<br>ChatKit managed hosting,<br>Agents SDK export,<br>OpenAI-only models</td>
<td>Python <br>TypeScript</td>
<td>&#x274C;</td>
</tr>
<tr>
<td><a href="#amazon-bedrock-agents">Amazon<br>Bedrock<br>Agents</a></td>
<td>Infra</td>
<td>Fully managed,<br>multi-model support<br>(Nova, Claude),<br>AWS ecosystem</td>
<td>N/A</td>
<td>&#x274C;</td>
</tr>
<tr>
<td><a href="#google-agent-development-kit">Google<br>Agent<br>Dev. Kit</a></td>
<td>SDK</td>
<td>Vertex AI integration,<br>A2A protocol,<br>Gemini models,<br>enterprise deployment</td>
<td>Python</td>
<td>&#x2705;</td>
</tr>
<tr>
<td><a href="#vertex-ai-agent-builder">Vertex AI<br>Agent<br>Builder</a></td>
<td>Infra</td>
<td>Google Cloud native,<br>compatible with<br>OSS frameworks<br>(LangChain, AG2 or Crew.ai)</td>
<td>N/A</td>
<td>&#x274C;</td>
</tr>
<tr>
<td><a href="#microsoft-semantic-kernel-agent-framework">Microsoft<br>Semantic<br>Kernel</a></td>
<td>SDK</td>
<td>Multi-language,<br>skill-based architecture<br>(functions &amp; plugins),<br>Azure integration</td>
<td>C#<br>Python<br>Java</td>
<td>&#x2705;</td>
</tr>
<tr>
<td><a href="#azure-ai-foundry-agent-service">Azure<br>AI Agent<br>Service</a></td>
<td>Infra</td>
<td>Fully managed,<br>MS 365 integration,<br>enterprise security &amp;<br>compliance</td>
<td>N/A</td>
<td>&#x274C;</td>
</tr>
</tbody>
</table>
<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><a href="https://blog.n8n.io/ai-agents-examples/"><u>15 practical AI agent examples</u></a> show how businesses use these frameworks to solve real-world problems across industries.</div></div><h3 id="n8n">n8n</h3><p><strong>When to use:</strong> &#x441;hoose n8n for its unique mix of low-code visual building, extensive integrations, and developer flexibility for custom code.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-8d6e903e-a28c-43d7-b029-e4a7e48af6c4.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1300" height="550" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-8d6e903e-a28c-43d7-b029-e4a7e48af6c4.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-8d6e903e-a28c-43d7-b029-e4a7e48af6c4.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-8d6e903e-a28c-43d7-b029-e4a7e48af6c4.png 1300w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">n8n strikes the balance between visual workflow building and developer-grade flexibility&#xA0;</em></i></figcaption></figure><p><u>n8n</u> is a source-available AI workflow automation platform that combines <a href="https://n8n.io/ai-agents/" rel="noreferrer">agentic</a> capabilities with traditional business process automation. It offers a unique hybrid approach where users can build sophisticated AI workflows visually while maintaining the ability to add custom JavaScript code when needed. The platform serves as a bridge between no-code simplicity and developer-grade flexibility.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Visual AI agent builder:</strong> drag-and-drop interface for building multi-agent systems with native AI nodes <a href="https://docs.n8n.io/advanced-ai/examples/understand-tools/"><u>including tools</u></a>, <a href="https://docs.n8n.io/advanced-ai/examples/understand-memory/"><u>memory</u></a>, <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/"><u>structured outputs</u></a> and more.</li><li><strong>Agent-to-Agent workflows:</strong> connect Agents sequentially, in parallel or use <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/tools-agent/"><u>Agent Tool node</u></a>, for hierarchical task delegation. Build agents from scratch or via <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/"><u>LangChain nodes</u></a> drop-ins.</li><li><strong>Extensive integration:</strong> <a href="https://n8n.io/integrations/"><u>1000+ pre-built integrations</u></a> with business tools, APIs, and services, approved partner nodes and thousands of community-built integrations.</li><li><strong>Custom code support:</strong> JavaScript integration through a general <a href="https://docs.n8n.io/code/code-node/"><u>Code node</u></a> and a specialized <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/"><u>LangChain Code node</u></a> for advanced customization.</li><li><strong>Memory management:</strong> multiple memory options including <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymongochat/"><u>Mongodb</u></a>, <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryredischat/"><u>Redis</u></a> and <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorypostgreschat/"><u>PostgreSQL Chat</u></a> as well as <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"><u>Simple Buffer Memory</u></a>. Flexible context management with <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/"><u>Memory Manager node</u></a> and dynamic session keys.</li><li><strong>Debugging features</strong>: use <a href="https://docs.n8n.io/advanced-ai/evaluations/overview/#what-are-evaluations"><u>Evals</u></a> to thoroughly test changes in agents, add <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.sendemail/"><u>human-in-the-loop</u></a> nodes for manual feedback at critical checkpoints, connect n8n with <a href="https://docs.n8n.io/advanced-ai/langchain/langsmith/"><u>LangSmith</u></a> and <a href="https://community.n8n.io/t/langfuse-integration/135162"><u>LangFuse</u></a> to see the detailed traces.</li><li><strong>Model flexibility:</strong> support for <a href="https://n8n.io/integrations/openai/"><u>OpenAI</u></a>, <a href="https://n8n.io/integrations/anthropic-chat-model/"><u>Anthropic</u></a>, <a href="https://n8n.io/integrations/azure-openai-chat-model/"><u>Azure</u></a>, <a href="https://n8n.io/integrations/deepseek-chat-model/"><u>DeepSeek</u></a>, <a href="https://n8n.io/integrations/mistral-cloud-chat-model/"><u>Mistral</u></a>, <a href="https://n8n.io/integrations/openrouter-chat-model/"><u>OpenRouter</u></a>, and local models via <a href="https://n8n.io/integrations/ollama-chat-model/"><u>Ollama</u></a>.</li><li><strong>MCP Integration:</strong> native support for Model Context Protocol as both <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolmcp/"><u>client</u></a> and <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/"><u>server</u></a>.</li><li><strong>Self-Hosting options:</strong> free community edition with unlimited executions or Business / Enterprise on-prem editions with extended features.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Cloud tiers starting from </strong>&#x20AC;20/month ($24/month) - 2,500 workflow executions</li><li><strong>Business and Enterprise:</strong> extended team collaboration features, advanced security, SSO, audit logs</li><li><strong>Free Community edition</strong></li></ul><h3 id="flowise">Flowise</h3><p><strong>When to use:</strong> opt for Flowise to rapidly prototype and deploy AI agents via its drag-and-drop interface built on top of LangChain.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-f49e8f35-3b21-4a7a-a4da-c73554f2bb70.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1200" height="650" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-f49e8f35-3b21-4a7a-a4da-c73554f2bb70.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-f49e8f35-3b21-4a7a-a4da-c73554f2bb70.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-f49e8f35-3b21-4a7a-a4da-c73554f2bb70.png 1200w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Flowise simplifies LangChain&apos;s power through drag-and-drop interfaces</em></i></figcaption></figure><p><u>Flowise</u> is an open-source, low-code platform that provides visual building blocks for creating AI agents, multi-agent systems, and simple LLM workflows. Built on top of LangChain and LlamaIndex, it offers three main builders: Assistant (beginner-friendly), Chatflow (single-agent systems), and Agentflow (multi-agent orchestration).</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Three building approaches:</strong> assistant for beginners, Chatflow for single agents, Agentflow for complex multi-agent systems.</li><li><strong>Multi-Agent systems:</strong> with Agentflow V2 users can connect agents in multiple ways</li><li><strong>RAG capabilities:</strong> built-in support for document processing, vector databases, and knowledge retrieval.</li><li><strong>LLM support:</strong> integration with OpenAI, Claude, Cohere, and other providers</li><li><strong>API &amp; SDK access:</strong> REST APIs, JavaScript / Python SDKs, and embedded chat widgets.</li><li><strong>Deployment flexibility:</strong> self-hosted options and cloud deployment with enterprise features.</li><li><strong>Evaluation system:</strong> built-in datasets and evaluators.</li><li><strong>MCP integration:</strong> Model Context Protocol client / server nodes.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Cloud tiers </strong>starting from $35/month - unlimited flows, 10,000 predictions/month, 1GB storage</li><li><strong>Enterprise:</strong> contact for pricing - on-premise deployment, SSO, SAML, audit logs</li></ul><h3 id="zapier-agents">Zapier Agents</h3><p><strong>When to use:</strong> use Zapier Agents to add simple, AI-powered decision-making to your existing business automations across its 8,000+ app ecosystem.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-ba9958f7-14db-498c-a76b-7879bde1383d.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1200" height="640" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-ba9958f7-14db-498c-a76b-7879bde1383d.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-ba9958f7-14db-498c-a76b-7879bde1383d.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-ba9958f7-14db-498c-a76b-7879bde1383d.png 1200w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Zapier Agents add AI decision-making to business process automation</em></i></figcaption></figure><p><u>Zapier Agents</u> extend Zapier&apos;s automation platform by adding AI-powered decision-making capabilities to traditional workflow automations. Agents can browse the web, analyze data, and interact with the vast Zapier ecosystem to automate complex business processes.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>AI-Powered automation:</strong> creating agents in Zapier is straightforward, but comes with significant limitations. Users need to access a separate UI for creating agents. Unlike other visual builders, Zapier allows configuring the agents mostly via prompting and a few drop-downs, such as a limited selection of tools or a simple knowledge base.</li><li><strong>Built-in tools:</strong> agents have access only to a few selected tools, such as Web Browsing or attached documents for the agent&apos;s knowledge base.</li><li><strong>8000+ app integrations:</strong> explore Zapier&apos;s extensive integration ecosystem.</li><li><strong>Chrome extension:</strong> direct interaction with agents through a browser extension</li><li><strong>Team collaboration:</strong> shared agent pools for Team accounts.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Agents are priced separately</strong> from the main Zapier platform<strong>:</strong> $50/month for 1,500 activities/month, up to 40 activities per run.</li><li><strong>Free limited tier:</strong> 400 activities/month, up to 10 activities per run</li><li><strong>Advanced:</strong> custom pricing with a custom number of activities per month</li></ul><h3 id="langgraph">LangGraph</h3><p><strong>When to use:</strong> select LangGraph when you are a developer needing precise, stateful control over complex agent workflows using its graph-based architecture.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-74b46470-161d-4d75-b9ab-765ce31d11b4.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1200" height="841" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-74b46470-161d-4d75-b9ab-765ce31d11b4.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-74b46470-161d-4d75-b9ab-765ce31d11b4.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-74b46470-161d-4d75-b9ab-765ce31d11b4.png 1200w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">LangGraph trades learning complexity for precise control over agent workflows</em></i></figcaption></figure><p><u>LangGraph</u> is a low-level library for creating stateful, graph-based agent workflows that extends LangChain. It provides fine-grained control over agent behavior through state machines and directed graphs, making it suitable for complex applications designed for live deployment and requiring explicit workflow control. Unlike n8n and Flowise, which offer convenient GUI over LangChain modules, LangGraph requires deep programming knowledge and familiarity with LangChain concepts.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Graph-based architecture:</strong> model agent workflows as directed graphs with nodes (Python functions) and edges (decision logic).</li><li><strong>State management:</strong> central persistence layer with checkpointing and memory across conversations.</li><li><strong>Human-in-the-loop:</strong> built-in interruption and resumption capabilities for human intervention.</li><li><strong>LangGraph studio:</strong> specialized IDE for visualization, debugging, and real-time agent interaction.</li><li><strong>Streaming support:</strong> real-time streaming of outputs and intermediate steps.</li><li><strong>Multi-agent orchestration:</strong> hierarchical, collaborative, and handoff patterns for agent coordination.</li><li><strong>Production features:</strong> background runs, burst handling, interrupt management.</li><li><strong>LangChain integration:</strong> deep integration with other components of the LangChain ecosystem, such as LangSmith for tracing, evaluation, and monitoring.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Plus plan:</strong> starts from $39 / month and offers both LangSmith and LangGraph Platform cloud deployment</li><li><strong>Enterprise plan:</strong> custom pricing - all deployment options, dedicated support</li><li><strong>Free Self-Hosted Lite</strong> deployment option with limits</li><li><strong>Usage-based</strong> pricing beyond the paid tier quotas</li></ul><h3 id="crewai">CrewAI</h3><p><strong>When to use:</strong> CrewAI is best for creating collaborative, role-based teams of specialized agents to autonomously work on structured tasks like research or content creation.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-edb5b966-151c-470a-8d94-a48fcd4a964b.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1170" height="650" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-edb5b966-151c-470a-8d94-a48fcd4a964b.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-edb5b966-151c-470a-8d94-a48fcd4a964b.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-edb5b966-151c-470a-8d94-a48fcd4a964b.png 1170w"><figcaption><i><em class="italic" style="white-space: pre-wrap;">CrewAI enables role-based teams for autonomous multi-agent systems</em></i></figcaption></figure><p><u>CrewAI</u> is a lightweight Python framework built from scratch (independent of LangChain) for creating role-based AI agent teams. It enables developers to build &quot;crews&quot; of specialized agents that collaborate autonomously on complex tasks.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Role-based architecture:</strong> define agents with specific roles, goals, backstories, and expertise areas</li><li><strong>Dual framework approach:</strong> CrewAI Crews for autonomous collaboration, CrewAI Flows for event-driven control</li><li><strong>Task management:</strong> sequential and parallel task execution with clear objectives</li><li><strong>Built-in tools:</strong> flexible tool integration and custom API connections</li><li><strong>Process control:</strong> workflow management system defining collaboration patterns</li><li><strong>Enterprise features:</strong> templates, access controls, and deployment tools available</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Paid plans</strong> starting from $99/month (plans are visible only after account creation)</li><li><strong>Limited free plan</strong> with 50 executions/month and 1 live deployed crew</li><li><strong>Enterprise:</strong> custom pricing with advanced features</li></ul><h3 id="openai-agentkit">OpenAI AgentKit</h3><p><strong>When to use</strong>: visual workflow building with OpenAI ecosystem integration &#x2013; suitable for teams wanting a drag-and-drop builder, with managed hosting or Agents SDK code export.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-122bf300-55d8-4203-a713-f0432c3782a2.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1300" height="560" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-122bf300-55d8-4203-a713-f0432c3782a2.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-122bf300-55d8-4203-a713-f0432c3782a2.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-122bf300-55d8-4203-a713-f0432c3782a2.png 1300w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">OpenAI&apos;s recently launched platform combines visual Agent Builder, managed ChatKit deployment, and exportable SDK code</em></i></figcaption></figure><p><u>OpenAI AgentKit</u> is a platform for developing, deploying and optimizing agent workflows. It combines visual Agent Builder, managed deployment options (ChatKit), and code export capabilities (Agents SDK). While offering a polished experience, it represents significant vendor dependency compared to open alternatives.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Visual Agent Builder</strong>: drag-and-drop canvas for agentic workflows with basic templates, live preview, and versioning</li><li><strong>Core primitives</strong>: agents, guardrails, and sessions with automatic tool calling and conversation management</li><li><strong>Connector registry</strong>: centralized admin panel that includes pre-built connectors (like Dropbox, Google Drive, Sharepoint, and Microsoft Teams), as well as third-party MCPs</li><li><strong>Deployment options</strong>: ChatKit managed hosting, embeddable chat widgets, or exported SDK code for self-hosting</li><li><strong>Built-in evaluation</strong>: datasets, trace grading, automated prompt optimization, and performance measurement</li><li><strong>OpenAI-native integration</strong>: deep coupling with GPT models, embeddings, and OpenAI&#x2019;s evaluation tools. However, without any support for Anthropic Claude, local models via Ollama, or other providers</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Platform access</strong>: free when creating agents (either visually with Agent Builder or via Agents SDK)</li><li><strong>Usage costs</strong>: based on underlying OpenAI API consumption (GPT models, embeddings, function calls)</li></ul><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">If your enterprise requires risk-managed, compliant AI adoption, our guide on <a href="https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/"><u>AI workflows for the cautious enterprise</u></a> discusses several strategies beyond single-vendor solutions.</div></div><h3 id="amazon-bedrock-agents">Amazon Bedrock Agents</h3><p><strong>When to use:</strong> Amazon Bedrock Agents is the choice for fully managed, enterprise-scale agent deployment within the AWS ecosystem, offering automatic scaling and security.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-2ebea2f5-c407-4832-b1f4-b98822ad8247.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1497" height="636" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-2ebea2f5-c407-4832-b1f4-b98822ad8247.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-2ebea2f5-c407-4832-b1f4-b98822ad8247.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-2ebea2f5-c407-4832-b1f4-b98822ad8247.png 1497w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Amazon Bedrock Agents offers fully managed agent deployment within AWS ecosystem</em></i></figcaption></figure><p><u>Amazon Bedrock Agents</u> is a fully managed service for building and deploying autonomous agents that integrate with AWS services and external APIs. It provides orchestration designed for large-scale operations, with automatic prompt engineering, memory management, and security.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Multi-agent collaboration:</strong> supervisor-based architecture with specialized agent coordination.</li><li><strong>Fully managed:</strong> no infrastructure management, automatic scaling, built-in security.</li><li><strong>Foundation model choice:</strong> support for multiple models including Amazon Nova and GPT-oss.</li><li><strong>Action groups:</strong> OpenAPI schema integration for external system connectivity.</li><li><strong>Knowledge base integration:</strong> built-in RAG capabilities with vector database support.</li><li><strong>Advanced prompts:</strong> customizable prompt templates for each orchestration step.</li><li><strong>CloudFormation support:</strong> infrastructure as Code deployment and team templates.</li><li><strong>Observability:</strong> built-in monitoring, tracing, and debugging console</li></ul><p><strong>Pricing</strong></p><ul><li><strong>On-demand:</strong> pay per token usage, varies by model (e.g., Nova Micro: $0.000035/1k input tokens)</li><li><strong>Provisioned throughput:</strong> discounted rates with 1-month or 6-month commitments</li><li><strong>Batch mode:</strong> reduced costs for large-scale processing</li><li><strong>Additional services:</strong> knowledge bases, guardrails priced separately</li></ul><h3 id="google-agent-development-kit">Google Agent Development Kit</h3><p><strong>When to use:</strong> ADK is a code-first Python framework for building production-ready agents that require deep integration with Google Cloud and Vertex AI.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-574adf45-8812-4beb-b845-48714cc007c5.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1200" height="560" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-574adf45-8812-4beb-b845-48714cc007c5.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-574adf45-8812-4beb-b845-48714cc007c5.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-574adf45-8812-4beb-b845-48714cc007c5.png 1200w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Google ADK extends BaseAgent class into reasoning (LLM) and orchestration (Sequential/Parallel/Loop) agents</em></i></figcaption></figure><p>Google&apos;s <u>Agent Development Kit</u> (ADK) is a code-first Python framework for building multi-agent applications intended for real-world deployment and integrated with Google Cloud services. It provides flexible orchestration patterns and deep integration with Gemini models and Vertex AI.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><p><strong>Flexible orchestration:</strong> workflow agents (Sequential, Parallel, Loop) and LLM-driven dynamic routing.</p><ul><li><strong>Google Cloud integration:</strong> native Vertex AI deployment, Gemini model access, Cloud services integration</li><li><strong>Multi-agent systems:</strong> support for complex agent coordination and collaboration patterns.</li><li><strong>Production-ready:</strong> deployment options suitable for business applications, with scaling capabilities.</li><li><strong>Rich tooling:</strong> comprehensive development tools (Google search, code execution, automatic tool schema generation) and debugging capabilities.</li><li><strong>Agent-to-Agent protocol:</strong> support for A2A protocol for inter-agent communication.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>ADK framework:</strong> free to use (open source Python framework)</li><li><strong>Usage costs:</strong> based on underlying Google Cloud services (Vertex AI, Gemini models) consumption</li></ul><h3 id="vertex-ai-agent-builder">Vertex AI Agent Builder</h3><p><strong>When to use:</strong> use this managed, no-code platform to quickly build and deploy conversational agents that are integrated with your enterprise data on Google Cloud.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-32b8e842-8fd6-4d0c-8aac-b150c8610640.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1300" height="735" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-32b8e842-8fd6-4d0c-8aac-b150c8610640.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-32b8e842-8fd6-4d0c-8aac-b150c8610640.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-32b8e842-8fd6-4d0c-8aac-b150c8610640.png 1300w" sizes="(min-width: 1200px) 1200px"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Vertex AI Agent Builder transforms enterprise data into conversational agents without coding</em></i></figcaption></figure><p><u>Vertex AI Agent Builder</u> is Google&apos;s platform for creating AI agents integrated with enterprise data sources. It provides a visual interface for assembling no-code agents that can access knowledge bases and connect to business systems.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>No-code development:</strong> visual interface for assembling conversational agents (based on the agents pre-built with ADK)</li><li><strong>Conversational design:</strong> advanced dialogue management and context handling</li><li><strong>Enterprise data integration:</strong> connect to databases, documents, and business systems</li><li><strong>Built-in RAG:</strong> automatic retrieval-augmented generation from knowledge sources</li><li><strong>Multi-framework support:</strong> in addition to ADK, developers can deploy agents built with popular open-source frameworks like LangChain, LangGraph, AG2 or Crew.ai</li><li><strong>Google Cloud integration:</strong> native access to Google services and APIs</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Usage-based:</strong> pay per conversation and API calls</li><li><strong>Integration costs:</strong> additional charges for data source connections and storage</li><li><strong>Enterprise edition:</strong> premium pricing for advanced features</li></ul><h3 id="microsoft-semantic-kernel-agent-framework">Microsoft Semantic Kernel Agent Framework</h3><p><strong>When to use:</strong> this framework is designed for multi-language enterprise development (C#, Python, Java) within the Microsoft ecosystem, featuring deep Azure integration.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-0d4fb9d0-4288-4c69-95f1-fa8c28a6730d.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="1011" height="451" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-0d4fb9d0-4288-4c69-95f1-fa8c28a6730d.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/data-src-image-0d4fb9d0-4288-4c69-95f1-fa8c28a6730d.png 1000w, https://blog.n8n.io/content/images/2025/10/data-src-image-0d4fb9d0-4288-4c69-95f1-fa8c28a6730d.png 1011w"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Microsoft Semantic Kernel supports various multi-agent orchestration patterns. Source: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns</em></i></figcaption></figure><p>Semantic Kernel&#x2019;s <u>Agent Framework</u> is Microsoft&apos;s multi-language orchestration platform supporting .NET, Python, and Java for building production-grade agent systems. It provides skill-based planning and deep Azure AI services integration.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Multi-language support:</strong> native C#, Python, and Java implementations</li><li><strong>Agent orchestration:</strong> hierarchical and collaborative <a href="https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns"><u>agent patterns</u></a> with state management</li><li><strong>Azure AI integration:</strong> deep integration with Azure OpenAI, Cognitive Services</li><li><strong>Built for corporate use: </strong>includes production deployment patterns with monitoring and scaling</li><li><strong>Memory management:</strong> advanced conversation and context management</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Open source:</strong> free framework (MIT license)</li><li><strong>Usage costs:</strong> based on underlying Azure AI services consumptio</li></ul><h3 id="azure-ai-foundry-agent-service">Azure AI Foundry Agent Service</h3><p><strong>When to use:</strong> opt for this fully managed service to deploy agents on Azure with enterprise-grade security, compliance, and native Microsoft 365 integration.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/10/data-src-image-423f06a7-dcac-4c32-899e-99d68babae1f.png" class="kg-image" alt="AI Agent Orchestration Frameworks: Which One Works Best for You?" loading="lazy" width="819" height="583" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/data-src-image-423f06a7-dcac-4c32-899e-99d68babae1f.png 600w, https://blog.n8n.io/content/images/2025/10/data-src-image-423f06a7-dcac-4c32-899e-99d68babae1f.png 819w"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Azure AI Foundry Agent Service processes agents through enterprise deployment pipeline with built-in safety controls</em></i></figcaption></figure><p><u>Azure AI Foundry Agent Service</u> is Microsoft&apos;s fully managed platform for deploying and scaling agent applications built with Semantic Kernel or other frameworks. It provides enterprise-grade infrastructure with built-in security and compliance features.</p><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x2699;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Key features</strong></b></div></div><ul><li><strong>Fully managed:</strong> no infrastructure management required.</li><li><strong>Advanced security and compliance:</strong> built-in data governance and access controls tailored for corporate needs.</li><li><strong>Microsoft 365 integration:</strong> native integration with Office, Teams, and SharePoint.</li><li><strong>Auto-scaling:</strong> automatic resource allocation and performance optimization.</li><li><strong>Monitoring &amp; analytics:</strong> built-in observability and performance tracking.</li><li><strong>Multi-tenant support:</strong> isolated environments for different organizational units.</li></ul><p><strong>Pricing</strong></p><ul><li><strong>Usage-based:</strong> pay per agent execution and resource consumption</li><li><strong>Enterprise tiers:</strong> volume discounts and dedicated support via Azure partners</li><li><strong>Integration costs:</strong> additional charges for premium Microsoft 365 integrations</li></ul><h2 id="benefits-of-using-ai-agent-orchestration-frameworks">Benefits of using AI agent orchestration frameworks</h2><p>Multi-agent AI orchestration has several advantages over single-agent approaches:</p><ul><li><strong>Task specialization reduces complexity.</strong> Instead of one agent handling everything poorly, specialized agents excel at specific functions. Your data analysis agent focuses solely on processing reports while your scheduling agent handles calendar management - each optimized for their domain.</li><li><strong>Cost efficiency at scale.</strong> You can use smaller, cheaper models for specialized tasks instead of requiring the most capable (and expensive) model for everything.</li><li><strong>Better scalability and performance.</strong> When workload increases, you can scale individual agent types based on demand rather than upgrading entire systems. Parallel agent execution also reduces overall processing time compared to sequential single-agent workflows.</li><li><strong>Easier maintenance and updates.</strong> Modifying one agent&apos;s behavior doesn&apos;t require rebuilding your entire system. You can update, test, and deploy changes to specific agents without disrupting the whole workflow.</li></ul><p>These benefits compound as systems grow more complex beyond simple chatbots.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><a href="https://blog.n8n.io/practical-evaluation-methods-for-enterprise-ready-llms/"><u>Practical evaluation methods for enterprise-ready LLMs</u></a> article explains how to test and validate multi-agent system performance.</div></div><h2 id="wrap-up">Wrap up</h2><p>We&apos;ve explored 11 AI agent orchestration frameworks across three categories and came to this conclusion.</p><ul><li><strong>For visual agent building:</strong> n8n is the clear leader among low-code options. While Zapier Agents offer simplicity, they lack the configurability needed for complex workflows. Flowise provides excellent LangChain integration but misses the traditional automation layer that business workflows require. OpenAI&apos;s Agent Builder is a brand new product that only supports LLMs from OpenAI.</li></ul>
<!--kg-card-begin: html-->
<div id="n8n-dark-banner" class="n8n-signup-banner">
  <h2 class="n8n-banner-heading">Build advanced automations with n8n AI Builder</h2>
  <p class="n8n-banner-subtext">Leverage advanced agent configuration through a convenient visual interface, plus 1000+ integrations for comprehensive business automation.</p>
  <a href="https://app.n8n.cloud/register" class="n8n-banner-button">Try n8n now</a>
</div>
<!--kg-card-end: html-->
<ul><li><strong>For developer-focused projects:</strong> the SDK landscape offers multiple approaches beyond the LangChain ecosystem. CrewAI and OpenAI Agents SDK provide framework-agnostic solutions, while LangGraph gives you precise control within the LangChain ecosystem.</li></ul><p>However, Microsoft Semantic Kernel and Google ADK, despite being open source, are designed and optimized primarily for their respective cloud ecosystems - keep this in mind if you need true vendor flexibility.</p><ul><li><strong>For enterprise deployment:</strong> major cloud providers have strategically separated their open-source SDKs from managed infrastructure services.</li></ul><p>While managed platforms like Amazon Bedrock Agents, Vertex AI Agent Builder, and Azure AI Agent Service offer compelling features and reduced operational overhead, they create vendor lock-in risks. Evaluate these carefully against your long-term flexibility requirements and multi-cloud strategies.</p><h2 id="whats-next">What&apos;s next?</h2><p>Ready to build your own AI agent orchestration system? Here are your next steps:</p><ul><li>Learn <a href="https://blog.n8n.io/ai-agents/"><u>what AI agents are and how they work</u></a> - from theory to practical deployment patterns.</li><li>Follow our <a href="https://blog.n8n.io/how-to-build-ai-agent/"><u>step-by-step guide to building AI agents</u></a> with free workflow templates and build your first agent.</li><li>Explore <a href="https://blog.n8n.io/ai-agent-frameworks/"><u>9 AI agent frameworks</u></a> and why developers choose n8n.</li><li>Read <a href="https://blog.n8n.io/the-n8n-scalability-benchmark/"><u>the n8n scalability benchmark</u></a> to understand performance limits and scaling strategies for production workflows.</li></ul><p>Finally, <a href="https://n8n.io/ai-agents/"><u>try n8n&apos;s AI agent capabilities for free</u></a> and <a href="https://n8n.io/workflows/categories/ai/"><u>explore community AI workflows</u></a> to see production-ready orchestration systems in action.</p>]]></content:encoded></item><item><title><![CDATA[n8n raises $180m to get AI closer to value with orchestration]]></title><description><![CDATA[<blockquote>We just raised $180 million in Series C funding, bringing our total funding to $240 million and our valuation to $2.5 billion. <br><br>The round is led by Accel, with support from Meritech, Redpoint, Evantic and Visionaries Club. Corporate investors NVentures (NVIDIA&#x2019;s venture capital arm) and T.Capital</blockquote>]]></description><link>https://blog.n8n.io/series-c/</link><guid isPermaLink="false">68e760b3dd174c0001c2b213</guid><dc:creator><![CDATA[Jan Oberhauser]]></dc:creator><pubDate>Thu, 09 Oct 2025 07:49:56 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/10/banner.png" medium="image"/><content:encoded><![CDATA[<blockquote>We just raised $180 million in Series C funding, bringing our total funding to $240 million and our valuation to $2.5 billion. <br><br>The round is led by Accel, with support from Meritech, Redpoint, Evantic and Visionaries Club. Corporate investors NVentures (NVIDIA&#x2019;s venture capital arm) and T.Capital also join the round, with previous backers including Felicis Ventures, Sequoia, Highland Europe and HV Capital  making follow-on investments as well</blockquote><img src="https://blog.n8n.io/content/images/2025/10/banner.png" alt="n8n raises $180m to get AI closer to value with orchestration"><p>This investment recognises something fundamental: the AI race isn&apos;t only about smarter models - it&apos;s about who can actually put that intelligence to work reliably, inside actual businesses.</p><p>The AI agent landscape has split into two camps. Some platforms put everything in the hands of AI, you write prompts and hope for the best, with the entire logic determined by the model&apos;s interpretation. Others require strict, rule-based routing which is powerful for engineers who code every pathway, but impractical for business users who need to iterate quickly.</p><p>We&apos;ve learnt from our community that neither extreme serves businesses well. Pure autonomy creates magic when it works but proves too unpredictable for business-critical workflows. Pure rule-based routing offers predictability but demands more time and often developers for every change.</p><p><strong>n8n was built for the reality in between: giving flexible control over where your agents sit on this spectrum</strong>. You choose the balance - how much autonomy to grant, how much logic to enforce, and crucially, how to adjust that balance as you learn what works.</p><p>But controlling this balance is only the foundation. Getting agents into production requires two more crucial elements:</p><p><strong>Orchestration</strong>: Connecting agents to your actual tools and data sources, building in human oversight where needed, and establishing the monitoring and triggers that keep everything running</p><p><strong>Coordination</strong>: Bringing together the people who understand the business need with the builders who can make it work - on the same platform, in real time</p><p>Without both, organisations get stuck in endless development cycles. Engineers build in isolation, business users test and provide feedback, iterations drag on. The agent never reaches production because the people closest to the work can&apos;t collaborate effectively with those building the solution.</p><p><strong>The formula we&apos;ve proven is straightforward: combine AI, code, and humans in the same process, on the same platform</strong>. Technical builders handle architecture whilst domain experts configure and refine. That&apos;s coordination, and it only succeeds when the orchestration layer is flexible enough to evolve with your needs.</p><h2 id="our-principles">Our principles</h2><p>We&apos;ve been building n8n since 2019, first as an automation tool, then as a platform for AI orchestration and cross-team collaboration. Now we&apos;re the platform our community and enterprises tell us finally helps them deploy AI in production.&#xA0;</p><p>We&apos;ve built this alongside a fast-growing community contributing videos, templates, nodes, and more. <strong>A community we&apos;ll never stop caring about or restrict access to. Ever.</strong></p><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/10/Series-C-Photo-Grid-2.png" class="kg-image" alt="n8n raises $180m to get AI closer to value with orchestration" loading="lazy" width="1920" height="1920" srcset="https://blog.n8n.io/content/images/size/w600/2025/10/Series-C-Photo-Grid-2.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/10/Series-C-Photo-Grid-2.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/10/Series-C-Photo-Grid-2.png 1600w, https://blog.n8n.io/content/images/2025/10/Series-C-Photo-Grid-2.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Flexibility will always be a top product priority. Flexibility to pick any model, connect any tool, and deploy anywhere: our cloud, your server, a Raspberry Pi, or bare metal. Flexibility in the product so it&apos;s easy enough to make a quick start but robust enough to handle orchestration&apos;s natural complexity.</p><p>From tinkerers automating lights at home to the United Nations running mission-critical workflows at scale, our ambition is clear: <strong>n8n becomes the default platform to build with AI</strong>. And more importantly, to deploy AI.</p><h2 id="what%E2%80%99s-next">What&#x2019;s next</h2><p>We&apos;ve made so much progress this year alone: 6x user growth, 10x revenue growth, and major new features like <a href="https://docs.n8n.io/advanced-ai/evaluations/overview/">Evaluations</a>, <a href="https://docs.n8n.io/data/data-tables/">Data Tables</a>, and <a href="https://docs.n8n.io/release-notes/">many more</a> improvements. Yet it still feels early.</p><p>The industry is moving fast, and so are we. This funding accelerates our roadmap: expanding our integrations, empowering the ecosystem to build their own nodes and share them globally, and evolving n8n beyond the canvas into new interfaces that match how different teams work. We&apos;re making the platform easier to start with whilst more powerful at scale - because that&apos;s what production AI demands. (<a href="https://n8n.io/careers/">We&apos;re hiring!</a>)</p><p>Our community is already pushing n8n from a platform into an ecosystem. We&apos;re rapidly seeing people build their own businesses around n8n, and we want to support the community to take this further: with education, early access to features, commercial partnerships, and fun <a href="https://luma.com/n8n-events">events</a> to bring everyone together.</p><p>I started n8n to remove repetitive tasks and focus on what I actually enjoyed. Now I see a world where building with AI, leveraging agents to scale yourself, and becoming a 10x operator becomes table stakes, just as using Excel is. The first people who knew Excel were special, but now it&apos;s a requirement for many roles.</p><p>The same will happen with AI. And my ambition is that n8n becomes the default platform to build with it.</p>]]></content:encoded></item><item><title><![CDATA[The n8n Scalability Benchmark]]></title><description><![CDATA[<p><strong>Ever wondered just how hard you can push n8n before it starts waving the white flag? We pushed n8n to the limits, with impressive results.&#xA0;</strong></p><p>When you&#x2019;re running mission-critical workflows, you need to know your limits. So we recently put different n8n deployments through their paces &#x2013;</p>]]></description><link>https://blog.n8n.io/the-n8n-scalability-benchmark/</link><guid isPermaLink="false">68b7fed9d319380001f3777a</guid><dc:creator><![CDATA[Charley Mann]]></dc:creator><pubDate>Wed, 24 Sep 2025 16:11:54 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/09/scalability.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/09/scalability.png" alt="The n8n Scalability Benchmark"><p><strong>Ever wondered just how hard you can push n8n before it starts waving the white flag? We pushed n8n to the limits, with impressive results.&#xA0;</strong></p><p>When you&#x2019;re running mission-critical workflows, you need to know your limits. So we recently put different n8n deployments through their paces &#x2013; simulating heavy traffic and maxing out resources to see which setups come out on top.</p><p>Whether you&apos;re running a side hustle or managing engineering for a multi-national organization, stress testing goes a long way to preventing downtime, bottlenecks, and broken promises. This benchmark blog and <a href="https://youtu.be/YvOCJzya9wU?feature=shared"><u>video</u></a> will show you exactly how far n8n can go, and where it starts to fall apart!</p><h3 id="a-workout-for-your-workflow"><strong>A Workout for your Workflow</strong></h3><p><strong>We stress tested n8n across two AWS instance types &#x2013; C5.large and C5.4xlarge &#x2013; </strong>using both n8n&#x2019;s Single and <a href="https://docs.n8n.io/hosting/configuration/environment-variables/queue-mode/"><u>Queue</u></a> modes (multi-threaded, queue-based architecture). We used K6 for load testing, Beszel for live resource monitoring, and n8n&#x2019;s own benchmarking workflows to&#xA0; automatically trigger each stress test scenario.</p><p>This workflow used a spreadsheet to iterate through different virtual user (VUs) levels, running each test and recording the results as it went. Once the data was logged, we turned it into a graph that revealed key performance indicators. Plus, in real-time we could see how well the system performed under varying loads &#x2013; how fast it responded, how reliably it executed, and where it started to crack.&#xA0;</p><p>Here&#x2019;s how we set it up:</p><p>The <strong>C5.large AWS</strong> instance comprised:</p><ul><li>1 vCPUs</li><li>2 Threads</li><li>4 GB RAM</li><li>10 Gbps bandwidth</li></ul><p>When we scaled up to the <strong>C5.4xlarge</strong>, we added 16 vCPUs + 32 GB RAM.</p><p>We ran three critical benchmarking scenarios:</p><ul><li><strong>Single Webhook</strong>: one flow triggered repeatedly</li><li><strong>Multi Webhook</strong>: 10 workflows triggered in parallel</li><li><strong>Binary Data:</strong> large file uploads and processing</li></ul><p>Each test scaled from 3 to 200 virtual users to measure:</p><ul><li>Requests per second</li><li>Average response time</li><li>Failure rate under load</li></ul><p>If you&#x2019;re keen to set up your own stress testing, we&#x2019;ve included all the tools you need to get started at the end of this blog, including the n8n <a href="https://github.com/n8n-io/n8n/tree/master/packages/%40n8n/benchmark"><u>Benchmark Scripts</u></a>.</p><h3 id="single-webhook"><strong>Single Webhook</strong></h3><p>We started small with a single webhook. This mimicked sending a web hook request to an n8n server and sending a response back that we received that webhook call. This was just one workflow, and one endpoint, gradually ramping up traffic to see how far a single n8n instance can be pushed.</p><p>Using a C5.large AWS instance the n8n Single mode deployment handled the pressure surprisingly well, as you can see from the comparison table below. While this instance held up to 100 VUs, <strong>once we reached 200 VUs we hit the ceiling for what a single-threaded setup can manage</strong>, with response times up to 12 seconds and a 1% failure rate.&#xA0;</p><p>When we enabled Queue mode, n8n&#x2019;s more scalable architecture that decouples webhook intake from workflow execution, performance jumped to 72 requests per second, latency dropped under three seconds, and the system handled 200 virtual users with zero failures.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Single-Webhook-Benchmark---AWS-c5.large-Single-Instance-2.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Single-Webhook-Benchmark---AWS-c5.large-Single-Instance-2.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Single-Webhook-Benchmark---AWS-c5.large-Single-Instance-2.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Single-Webhook-Benchmark---AWS-c5.large-Single-Instance-2.png 1600w, https://blog.n8n.io/content/images/2025/09/Single-Webhook-Benchmark---AWS-c5.large-Single-Instance-2.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Single webhook benchmark AWS c5.large</span></figcaption></figure><p>Scaling up to the C5.4xlarge (16 vCPUs, 32 GB RAM) we saw some impressive gains. In single mode, throughput rose slightly to 16.2 requests per second with modest latency improvements.&#xA0;</p><p>But it was Queue mode that really stole the show. We hit a consistent 162 requests per second and maintained that across a full 200 VU load, with latency under 1.2 seconds and zero failures. <strong>That&#x2019;s a 10x throughput gain just by scaling vertically and choosing the right architecture.</strong></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Single-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Single-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Single-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Single-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1600w, https://blog.n8n.io/content/images/2025/09/Single-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Single webhook benchmark AWS c5.4xlarge</span></figcaption></figure><h3 id="multiple-webhooks"><strong>Multiple Webhooks</strong></h3><p>For the next test, we wanted to simulate enterprise-grade multitasking to better reflect real-world n8n deployments,<strong> so we set up 10 distinct workflows, each triggered by its own webhook.</strong></p><p>On the<strong> C5.large in single mode, performance fell off quickly</strong>. At 50 VUs, response time spiked above 14 seconds with an 11% failure rate. At 100 VUs, latency reached 24 seconds with a 21% failure rate. And at 200 VUs, the failure rate hit 38% and response time stretched to 34 seconds &#x2013; essentially a meltdown.</p><p><strong>Switching to Queue mode changed the game.</strong> It sustained 74 requests per second consistently from three to 200 VUs, with latency within acceptable bounds, and a 0% failure rate. Same hardware, totally different outcome.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance-1-2.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance-1-2.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance-1-2.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance-1-2.png 1600w, https://blog.n8n.io/content/images/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance-1-2.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Multi webhook benchmark AWS c5.large</span></figcaption></figure><p><strong>Once again, the C5.4xlarge took things to another level.</strong> In Single mode, it peaked at 23 requests per second with a 31% failure rate. But in Queue mode, we hit and maintained 162 requests per second across all loads, with zero failures. <strong>Even under max stress, latency stayed around 5.8 seconds. </strong>Multitasking at scale demands more muscle and Queue mode absolutely delivers.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1600w, https://blog.n8n.io/content/images/2025/09/Multi-Webhook-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Multi webhook benchmark AWS c5.4xlarge</span></figcaption></figure><h3 id="binary-file-uploads"><strong>Binary File Uploads</strong></h3><p>Finally, we wanted to test the most RAM-hungry and disk-heavy tasks we could, so we set up a binary data benchmark with workflows that deal with large file uploads like images, PDFs, and media.</p><p>On a C5.large in single mode, the cracks appeared early. At just three virtual users we managed only three requests per second. At 200 VUs, response times ballooned and 74% of requests failed. That&#x2019;s not just a slowdown, that&#x2019;s total operational failure.</p><p>Queue mode offered a little more resilience, delaying the breakdown. But by 200 VUs, it too collapsed with an 87% failure rate and incomplete payloads.&#xA0;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Binary-Data-Benchmark---AWS-c5.large-Single-Instance-4.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Binary-Data-Benchmark---AWS-c5.large-Single-Instance-4.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Binary-Data-Benchmark---AWS-c5.large-Single-Instance-4.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Binary-Data-Benchmark---AWS-c5.large-Single-Instance-4.png 1600w, https://blog.n8n.io/content/images/2025/09/Binary-Data-Benchmark---AWS-c5.large-Single-Instance-4.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Binary data benchmark AWS c5.large</span></figcaption></figure><p>Then we turned to the C5.4xlarge. With this larger instance in single mode, we reached 4.6 requests per second, trimmed response time by a third, and reduced the failure rate from 74% to just 11%. Vastly improved, but not perfect.</p><p><strong>Then in Queue mode, we peaked at 5.2 requests per second and, crucially, held a 0% failure rate across the entire test. Every</strong> large file was successfully received, processed, and responded to. This test made it clear &#x2013; it&#x2019;s not just about architecture. Binary-heavy workflows demand serious CPU, RAM, and disk throughput.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/Binary-Data-Benchmark---AWS-c5.4xlarge-Single-Instance.png" class="kg-image" alt="The n8n Scalability Benchmark" loading="lazy" width="1920" height="1080" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/Binary-Data-Benchmark---AWS-c5.4xlarge-Single-Instance.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/Binary-Data-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/Binary-Data-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1600w, https://blog.n8n.io/content/images/2025/09/Binary-Data-Benchmark---AWS-c5.4xlarge-Single-Instance.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Binary data benchmark AWS c5.4xlarge</span></figcaption></figure><h3 id="key-takeaways"><strong>Key Takeaways</strong></h3><p>So what did all these tests tell us?</p><ol><li><strong>Queue mode isn&#x2019;t optional.</strong> It&#x2019;s the first step toward real scalability. Even on entry-level hardware, it massively boosts performance with minimal setup.</li><li><strong>Hardware matters.</strong> Upgrading to a C5.4xlarge more than doubles throughput, cuts latency in half, and eliminates failure rates entirely.</li><li><strong>Binary data breaks everything&#x2014;unless you&#x2019;re prepared</strong>. You&#x2019;ll need more RAM, faster disk, shared storage like S3, and parallel workers to manage it all.</li></ol><p>If you&#x2019;re building automation for internal teams, backend systems, or customer-facing apps, don&#x2019;t wait for bottlenecks to force an upgrade. Plan for scale from the beginning. Use Queue mode to separate intake from processing, scale horizontally with workers for concurrent processing, and size your hardware to match your workload. Simple flows need less, but binary data and multitasking need more. <strong>n8n is built to scale, but like any engine, it needs the right fuel and the right track to reach full power.</strong></p><p>Want to test your own setup?</p><ul><li><a href="https://docs.n8n.io/hosting/scaling/performance-benchmarking/"><u>n8n Benchmarking Guide</u></a></li><li><a href="https://docs.n8n.io/hosting/scaling/queue-mode/"><u>Queue Mode Setup</u></a></li><li><a href="https://docs.n8n.io/hosting/installation/docker/"><u>Docker Installation Guide</u></a></li><li><a href="https://k6.io/"><u>K6 Load Testing</u></a></li><li><a href="https://www.beszel.dev/"><u>Beszel Monitoring</u></a></li><li><a href="https://github.com/n8n-io/n8n/tree/master/packages/%40n8n/benchmark"><u>n8n Benchmark Scripts on GitHub</u></a></li></ul><p>And here&apos;s the full benchmarking video.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/YvOCJzya9wU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="We Maxed Out n8n - Here&#x2019;s When It Broke"></iframe></figure>]]></content:encoded></item><item><title><![CDATA[Practical Evaluation Methods for Enterprise-Ready LLMs]]></title><description><![CDATA[Discover practical evaluation methods for enterprise-ready LLMs. Learn how to measure accuracy, safety, and reliabilityand see how n8ns built-in evaluation tools make it easy to test and improve AI workflows.]]></description><link>https://blog.n8n.io/practical-evaluation-methods-for-enterprise-ready-llms/</link><guid isPermaLink="false">68addbd6d319380001f376c1</guid><category><![CDATA[AI]]></category><dc:creator><![CDATA[Andrew Green]]></dc:creator><pubDate>Wed, 03 Sep 2025 17:39:41 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/08/meteric-based-evaluations--1-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/08/meteric-based-evaluations--1-.jpg" alt="Practical Evaluation Methods for Enterprise-Ready LLMs"><p>Evaluations for LLMs are the equivalent of performance monitoring for enterprise IT systems. While the applications may work without them, they will not be suitable for production deployments.&#xA0;</p><p>In this article, we&#x2019;ll describe today&#x2019;s most common LLM evaluation methods to help you bring your AI implementations to an enterprise-grade standard.</p><p>We&apos;ll also show how n8n&apos;s native evaluation capabilities make it easy to implement these methods directly in your workflows.</p><h2 id="matching-evaluation-methods-to-the-llm%E2%80%99s-purpose">Matching evaluation methods to the LLM&#x2019;s purpose</h2><p>Evaluations tell us whether an LLM output is suitable for its intended purpose. As such, the most important aspect to determine before exploring the evaluation options is the LLM&#x2019;s intended purpose.&#xA0;</p><p>Some of those include:</p><ul><li>Providing a chat interface to consumers</li><li>Writing code</li><li>Using the LLM as a natural language interface for a software product</li><li>Automating internal processes using AI Agents</li><li>Generating descriptions for retail products</li><li>Summarizing unstructured email data into a spreadsheet</li></ul><p>You most likely have an intuitive understanding that somebody using LLMs to write code is interested in a valid JSON output, while those who want to generate descriptions are not. To help, we have collated a range of evaluation methods available today either via open source or commercial products. This document is not a tutorial on how to use evaluations, but rather would help you understand the types of evaluations available today which can best serve your use case.</p><p>We can categorize evaluations into four broad categories:</p>
<!--kg-card-begin: html-->
<table style="border:none;border-collapse:collapse;table-layout:fixed;width:468pt"><colgroup><col><col><col><col></colgroup><tbody><tr style="height:0pt"><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Category</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Typical Methods</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Best for</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Example use case</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Matches and similarity</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Exact Match, Regex, Levenshtein similarity, semantic similarity</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">High fidelity reproduction</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Compliance, Legal, knowledge base search</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Code Evaluations</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">JSON,&#xA0;</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Functional correctness,&#xA0;</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Syntax correctness,&#xA0;</span></p><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Format check</span></p><br></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Code generation, natural language interfaces</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Automation workflows, coding copilots</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">LLM-as-judge</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Helpfulness, Correctness, Factuality</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Flexible, subjective tasks</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Product copilots</span></p></td></tr><tr style="height:0pt"><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Safety</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">PII detection, prompt injection, toxicity detection</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Public-facing apps</span></p></td><td style="border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:11pt;font-family:Arial,sans-serif;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Customer support chatbot</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>The first is<strong> Matches and similarity,</strong> which<strong> </strong>is particularly useful when you already have a ground truth answer and are interested in the LLM reproducing it to some degree of fidelity.</p><p>The second is about <strong>Code Evaluations</strong>. While the most obvious use case for this is code generation, it is also important for instances where the LLM is used as a natural language interface. Imagine a SaaS HR product that has an LLM-based virtual assistant. To interact with the product, the LLM would write a database query, generate a script or call the product&#x2019;s API to execute an action, which would be subject to the code evaluations.</p><p>The third option is <strong>LLM-as-judge</strong>, which, despite being somewhat recursive, is also flexible, highly configurable, and easy to automate. I have my doubts about judge LLMs, as they are just as reliable as the LLMs themselves. You can also imagine scenarios where you implement LLMs to judge the judge LLMs, ad infinitum. So, there must be some deterministic component as part of the LLM-as-Judge approach.</p><p>Lastly, <strong>Safety </strong>evaluations are the basis of guardrails, and measure whether the LLM output is toxic or contains sensitive information.</p><p>In addition to evaluating the LLM output, scoring the LLM&#x2019;s context, especially with respect to context supplied via RAG, is highly important, and we explore this more in <a href="https://blog.n8n.io/evaluating-rag-aka-optimizing-the-optimization/"><u>our blog post on <em>Evaluations for Retrieval Augmented Generation (RAG) systems.</em>&#xA0;</u></a></p><h2 id="matches-and-similarity">Matches and similarity</h2><p>In this section, we talk about two types of evaluations. Matches and similarity, which we&#x2019;ve grouped together because they often require ground truth, i.e. a pre-determined source of truth. These may be technical docs, contracts, laws, medical documents, etc.</p><p><strong>Matches can either be</strong> <strong>exact</strong>, where the output is exactly equal to the target; or they can be <strong>based on regex</strong>, checking that the specified regular expression can be found in the output.&#xA0;</p><p>Matches are important for use cases such as re-generating content verbatim from technical documentation. Imagine a user asking an LLM assistant &#x201C;how to open a support ticket&#x201D;. An exact match would recreate the same content from the technical documentation page, while a regex-based match would tolerate some additional content from the LLM, like your typical &#x201C;Sure! Here is how to open a support ticket&#x201D;.</p><p>Going from matches to similarity, it&#x2019;s important to note the <strong>Levenshtein Similarity Ratio,</strong> which measures <strong>string similarity</strong>. It&#x2019;s the difference between two strings as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.</p><p>Similarity is more complex but also more useful. <strong>Semantic similarity</strong> <a href="https://arxiv.org/abs/2401.17072"><u>embeds words into vectors to compare them in a numerical 0-1 value</u></a>, where words that are similar score higher, and words that are not score lower. As such, we can measure how similar an LLM output is to an original source based on what the content means.</p><h2 id="code-evaluations">Code evaluations</h2><p>Compared to normal text, code also has a functional component. Evaluating code generated by an LLM means to determine whether the code 1) runs, and 2) runs as intended. As discussed earlier, I also want to expand this area to also include natural language interfaces that generate code to interact with a software product.</p><ul><li><strong>JSON validity </strong>- Check that the output is valid JSON by first ensuring the output is JSON, and then checking the schema conforms to a structure.</li><li><strong>Functional correctness</strong> evaluates the accuracy of NL-to-code generation tasks when the LLMs is tasked with generating code for a specific task in natural language. In this context, functional correctness evaluation is used to assess whether the generated code produces the desired output for a given input. Natural-language prompts can be paired with a suite of <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00694/125031/NoviCode-Generating-Programs-from-Natural-Language"><u>unit tests to check semantic behavior</u></a> covering correct outputs, edge cases, control-flow handling, API usage, etc.</li><li><strong>Syntax correctness</strong>: This metric measures whether the generated code conforms to the syntax rules of the programming language being used. This metric can be evaluated using a set of rules that check for common syntax errors. Some examples of common syntax errors are missing semicolons, incorrect variable names, or incorrect function calls.</li><li><strong>Format check</strong> evaluates whether generated is using good formatting practices, such as indentation, line breaks, and whitespace.</li></ul><h2 id="llm-as-judge">LLM-as-judge</h2><p>Independent LLMs can be used to evaluate whether responses are satisfactory. Some examples of evaluations that can be run by judge LLMs include helpfulness, correctness, query equivalence, and factuality.</p><p><strong>Helpfulness</strong> evaluates whether an LLM&apos;s output is relevant to the original query. It uses a combination of embedding similarity and LLM evaluation to determine relevance. It uses an LLM to generate potential questions that the output could be answering, then compares these questions with the original query using embedding similarity, and lastly calculates a relevance score based on the similarity scores.</p><p><strong>Correctness</strong> evaluates whether the AI&apos;s response is faithful to the provided context, checking for hallucinations or unsupported claims. It does so by <strong>analyzing</strong> the relationship between the provided context and the AI&apos;s response, identifying claims in the response that are not supported by the context.</p><p><strong>SQL Query Equivalence</strong> checks if the SQL query is equivalent to a reference one by using an LLM to infer if it would generate the same results given the table schemas.</p><p>Factuality evaluates the factual consistency between an LLM output and a reference answer. <a href="https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml"><u>OpenAI&apos;s evals</u></a> are used by multiple providers to determine if the output is factually consistent with the reference. The factuality checker compares outputs based on the following:</p><ul><li>Output is a subset of the reference and is fully consistent</li><li>Output is a superset of the reference and is fully consistent</li><li>Output contains all the same details as the reference</li><li>Output and reference disagree</li><li>Output and reference differ, but differences don&apos;t matter for factuality</li></ul><p>In n8n, LLM-as-judge evaluations are included in the built-in helpfulness and correctness metrics. Users can also create a custom metric and include an LLM as Judge in a sub-workflow that generates scores on outputs and passes them back as metrics.</p><h2 id="safety">Safety&#xA0;</h2><p>Safety evaluations check whether the LLM response contains personal identifiable information, prompt injection attempts, or toxic content. These are particularly important when exposing the LLM application to consumers or other external use cases.</p><ul><li><strong>PII Detection</strong> finds and sanitizes personally identifiable information in text, including phone numbers, email addresses, and social security numbers. It allows customization of the detection threshold and the specific types of PII to check.</li><li><strong>Prompt Injection and Jailbreak Detection</strong> identifies attempts made by users to&#xA0; jailbreak the system and produce unintended output. attempts in the input.</li><li><strong>Content Safety</strong> detects potentially unsafe content in text, including hate speech, self-harm, sexual content, and violence.&#xA0;</li></ul><h2 id="metric-based-evaluations-in-n8n">Metric-based evaluations in n8n</h2><p><a href="https://docs.n8n.io/advanced-ai/evaluations/overview/"><u>In n8n, evaluations</u></a> are a native part of workflows and can be used to understand the LLM&#x2019;s behavior against a test dataset. <a href="https://docs.n8n.io/release-notes/#webhook-html-responses"><u>Metric-based evaluations</u></a> can assign one or more scores to each test run, which can be compared to previous runs to see how the metrics change and drill down into the reasons for those changes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/data-src-image-f0166321-cfc0-45b4-ae32-c7016dd70418.png" class="kg-image" alt="Practical Evaluation Methods for Enterprise-Ready LLMs" loading="lazy" width="1050" height="482" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/data-src-image-f0166321-cfc0-45b4-ae32-c7016dd70418.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/data-src-image-f0166321-cfc0-45b4-ae32-c7016dd70418.png 1000w, https://blog.n8n.io/content/images/2025/09/data-src-image-f0166321-cfc0-45b4-ae32-c7016dd70418.png 1050w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Metric-based evaluations in n8n</span></figcaption></figure><p>Evaluations are simply implemented in workflows with the Evaluations Trigger. It acts as a separate execution that does not affect your production workflow in any way. It is manually triggered and automatically pulls datasets from the assigned Google Sheet. Lastly, you need to populate the output column(s) of your dataset when the evaluation runs by inserting the &apos;Set outputs&apos; action of the evaluation node and wiring it up to your workflow after it has produced the outputs you&apos;re evaluating.&#xA0;</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/QkciQpotQBQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="From Prompt to Production: Smarter AI with Evaluations"></iframe></figure><p>n8n supports both deterministic and LLM-based evaluations and can measure whether the output&apos;s meaning is consistent with a reference output, if it exactly matches the expected output, whether the answer addresses the question, tools used, and determine how close the output is to a reference output. Users can also create custom metrics.</p><p>Here&#x2019;s a <u>workflow example of an LLM-based evaluation</u>, where the workflow collects the agent&apos;s response and the documents retrieved, and then uses an LLM to assess if the former is based on the latter. A high score indicates LLM adherence and alignment whereas a low score could signal inadequate prompt or model hallucination.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/screenshot-BX2AB7zk.png" class="kg-image" alt="Practical Evaluation Methods for Enterprise-Ready LLMs" loading="lazy" width="1169" height="724" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/screenshot-BX2AB7zk.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/screenshot-BX2AB7zk.png 1000w, https://blog.n8n.io/content/images/2025/09/screenshot-BX2AB7zk.png 1169w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Evaluate RAG Response Accuracy with OpenAI: Document Groundedness Metric</span></figcaption></figure>
<!--kg-card-begin: html-->
<script>
  workflowBanner(4426, document.currentScript);
</script>
<!--kg-card-end: html-->
<p>Another <u>workflow example uses the RAGAS methodology,</u> which is useful when the agent&apos;s response is allowed to be more verbose and conversational. The agent&apos;s response is classified in three buckets: True Positive (in answer and ground truth), False Positive (in answer but not ground truth) and False Negative (not in answer but in ground truth).A high score indicates the agent is accurate whereas a low score could indicate the agent has incorrect training data or is not providing a comprehensive enough answer.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/screenshot-tHfUlNjB.png" class="kg-image" alt="Practical Evaluation Methods for Enterprise-Ready LLMs" loading="lazy" width="1297" height="546" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/screenshot-tHfUlNjB.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/screenshot-tHfUlNjB.png 1000w, https://blog.n8n.io/content/images/2025/09/screenshot-tHfUlNjB.png 1297w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Evaluate AI Agent Response Correctness with OpenAI and RAGAS Methodology</span></figcaption></figure>
<!--kg-card-begin: html-->
<script>
  workflowBanner(4424, document.currentScript);
</script>
<!--kg-card-end: html-->
<h2 id="wrap-up">Wrap up</h2><p>Implementing evaluations as part of your AI workflows can help bring your automation logic to an enterprise-grade level. The built-in metrics within n8n give you all the tools to test the performance of your AI models without the need for external libraries or applications. Learn more about<a href="https://docs.n8n.io/advanced-ai/evaluations/overview/"><u> metric-based evaluations here.</u></a></p>
<!--kg-card-begin: html-->
<script>
  workflowBanner([5523, 4273, 4428, 4425], document.currentScript, { 
      workflowsHeader: "Or start with Evaluation workflows right away!"
  });
</script>
<!--kg-card-end: html-->
]]></content:encoded></item><item><title><![CDATA[Agentic RAG: A Guide to Building Autonomous AI Systems]]></title><description><![CDATA[Standard RAG is accurate but inflexible. Agentic RAG is the upgrade, using smart AI agents in dynamic workflows. These agents choose the right tool for any query, like a database or web search, and verify their own answers. This guide explains what Agentic RAG is and shows you practical examples.]]></description><link>https://blog.n8n.io/agentic-rag/</link><guid isPermaLink="false">68b43322d319380001f376c8</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Mihai Farcas]]></dc:creator><pubDate>Wed, 03 Sep 2025 09:11:49 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/09/workflow-template--1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/09/workflow-template--1-.png" alt="Agentic RAG: A Guide to Building Autonomous AI Systems"><p>If you&#x2019;ve worked with any Large Language Model (LLM) applications, you&apos;ve likely struggled with the inherent challenges of these powerful systems. At their core, LLMs are prone to hallucinations (i.e., confident yet incorrect outputs) and suffer from knowledge cut-off dates, meaning they lack access to real-time or proprietary information unless explicitly provided. They can also produce inconsistent responses and often miss nuanced context, processing language based on learned patterns rather than true understanding.</p><p>To overcome these limitations, developers turned to Retrieval-Augmented Generation (RAG), a technique that connects LLMs to external data sources. This allows the model to fetch relevant, up-to-date information before formulating a response, dramatically improving accuracy. RAG was a significant step forward, but it&apos;s fundamentally a static, linear process: retrieve information, then generate an answer.</p><p>But what if the system could be more intelligent? What if it could autonomously decide the best way to find an answer, which tools to use, and even critique its own response for completeness? This is the promise of Agentic RAG, the next evolution of this framework. By integrating LLM-powered agents, we transform the simple RAG pipeline into a dynamic, intelligent workflow.</p><p>In this article, we&apos;ll explore what Agentic RAG is, how it moves beyond the limitations of its predecessor, and why it is set to redefine how we build sophisticated AI applications.</p><h2 id="what-is-an-agentic-rag">What is an agentic RAG?</h2><p>At its core, Agentic RAG upgrades the standard retrieval framework by integrating LLM-powered agents to introduce autonomous decision-making. Instead of following a rigid set of instructions, the system can perceive its environment, make decisions, and execute actions to achieve a goal.</p><p>While this intelligence is applied across the entire workflow, the most fundamental shift occurs during indexing. In traditional RAG, indexing is a predefined and often manual process. With Agentic RAG, this becomes a dynamic and context-aware operation driven by the AI itself. An agent can autonomously decide not just what information to add to the vector store, but also how to do it most effectively.</p><p>For example, an agent can intelligently parse complex documents to extract richer, more useful metadata and also decide on the optimal chunking strategy for different types of content. This transforms indexing from a static setup task into an ongoing process of knowledge-building, laying the foundation for more accurate and relevant results down the line.</p><h2 id="what-is-the-difference-between-simple-rag-and-agentic-rag">What is the difference between simple RAG and agentic RAG?</h2><p>The primary difference between simple (or naive) RAG and Agentic RAG lies in their operational workflow and intelligence. While both aim to enhance Large Language Models (LLMs) with external data, their approaches and capabilities differ significantly. Simple RAG is a linear and static process, whereas Agentic RAG is dynamic, adaptive, and autonomous.</p><p>To better understand the key distinctions, here is a direct comparison:</p><table>
<thead>
<tr>
<th>Feature</th>
<th>Simple RAG</th>
<th>Agentic RAG</th>
</tr>
</thead>
<tbody>
<tr>
<td>Workflow</td>
<td>Fixed &#x201C;retrieve then<br>read&#x201D; sequence</td>
<td>Dynamic, multi-step<br>process (query rewriting,<br>multi-source retrieval,<br>or skipping retrieval)</td>
</tr>
<tr>
<td>Decision-making</td>
<td>None; path<br>is predetermined</td>
<td>Agent makes decisions<br> (routing, tool use,<br>self-critique)</td>
</tr>
<tr>
<td>Data Sources &amp; Tools</td>
<td>Single, unstructured<br>knowledge base</td>
<td>Multiple sources<br>(vector stores, SQL,<br>web APIs, etc.)</td>
</tr>
<tr>
<td>Adaptability</td>
<td>Rigid; same process<br>for every query</td>
<td>Adaptive; adjusts<br>retrieval steps for complex,<br>multi-hop queries</td>
</tr>
</tbody>
</table>
<p>In essence, while simple RAG provides an LLM with passive access to external knowledge, agentic RAG gives it an active framework for intelligent operation. This framework enables the system to solve complex problems by dynamically choosing tools and data sources. This intelligence also extends to the knowledge base itself; an agent can autonomously update and maintain its own information, deciding what to store and how to index it for optimal relevance and accuracy.</p><h2 id="what-is-the-structure-of-agentic-rag">What is the structure of agentic RAG?</h2><p>As we previously saw, agentic RAG fundamentally changes how a system stores, retrieves, and uses information. Instead of a rigid pipeline, it introduces a three-staged lifecycle where agents make decisions at every step to improve the quality and relevance of the final answer. Building such a system requires three key components:</p><h3 id="intelligent-storage-deciding-what-and-how-to-index">Intelligent storage: deciding what and how to index</h3><p>Before any information can be retrieved, it must be stored. In a traditional RAG system, this indexing process is static. An Agentic RAG system, however, turns this into an active, intelligent process.</p><p>An agent can analyze incoming data and decide if it should be indexed at all. More importantly, it decides the most effective way to store it. This includes performing high-precision parsing of complex documents, creating rich metadata for better filtering, choosing the optimal chunking strategy, and even selecting the most appropriate embedding model for the context of the data. This ensures the knowledge base is not just a passive repository but an optimized and strategically organized source of information.</p><h3 id="dynamic-retrieval-using-the-right-tool-for-the-right-data">Dynamic retrieval: using the right tool for the right data</h3><p>When a user asks a question, an agentic system excels at finding the right information from the best possible source. It is not limited to searching a single vector store.</p><p>Using a component often called a <em>Retriever Router</em>, an LLM agent analyzes the incoming query and decides the best course of action. This might mean querying a SQL database, using a web search API, or searching internal product documentation. By being equipped with a variety of tools, the system can interact with multiple, diverse data sources, ensuring it can retrieve the most relevant context, no matter where it lives.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A basic retriever router prompt could look like this:<br><br><i><em class="italic" style="white-space: pre-wrap;">&quot;You are a router. Your job is to select the best tool to answer the user&apos;s query. You have two tools:</em></i><br><i><em class="italic" style="white-space: pre-wrap;">1. SQL_database_tool: Use for questions about sales, revenue, or specific metrics.</em></i><br><i><em class="italic" style="white-space: pre-wrap;">2. document_vector_store_tool: Use for questions about company policies or general information.&quot;</em></i></div></div><h3 id="verified-generation-composing-and-critiquing-the-answer">Verified generation: composing and critiquing the answer</h3><p>Once the information is retrieved, the process isn&apos;t over, using an <em>Answer Critic</em> function, the system checks if the retrieved information has correctly and completely answered the user&apos;s original question. If the answer is incomplete or incorrect, the critic can generate a new, more specific question to retrieve the missing information and trigger another round of retrieval. This iterative process of generating and critiquing ensures the final response is accurate and comprehensive before it is ever presented to the user.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A basic answer critic prompt could look like this:<br><br><i><em class="italic" style="white-space: pre-wrap;">&quot;You are an Answer Critic. Evaluate if the GENERATED_ANSWER fully addresses the USER_QUERY. If it is incomplete, state what is missing and generate a new INTERNAL_QUERY to find the missing information.&quot;</em></i></div></div><h2 id="3-agentic-rag-use-cases">3 agentic RAG use cases</h2><p>Let&apos;s see how these principles work in practice through a few concrete examples. The following workflows illustrate how <a href="https://n8n.io/ai/" rel="noreferrer">n8n</a>&apos;s visual, node-based interface is perfectly suited for designing and orchestrating the complex, multi-step logic that Agentic RAG systems require.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">To learn more about the underlying concepts, check out our documentation on <a href="https://docs.n8n.io/advanced-ai/rag-in-n8n/" target="_blank" rel="noopener">advanced AI with RAG in n8n</a>.</div></div><h3 id="adaptive-rag-choosing-the-right-retrieval-strategy">Adaptive RAG (choosing the right retrieval strategy)</h3><p>Not all questions are the same. Some ask for a simple fact, while others require a deep analysis. A simple RAG system treats them all identically, which can lead to poor results. This workflow demonstrates a more advanced, adaptive RAG by first analyzing the user&apos;s intent and then choosing the best retrieval strategy for that specific type of question.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/screenshot-1ZYDAgCl@2x.png" class="kg-image" alt="Agentic RAG: A Guide to Building Autonomous AI Systems" loading="lazy" width="1744" height="1044" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/screenshot-1ZYDAgCl@2x.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/screenshot-1ZYDAgCl@2x.png 1000w, https://blog.n8n.io/content/images/size/w1600/2025/09/screenshot-1ZYDAgCl@2x.png 1600w, https://blog.n8n.io/content/images/2025/09/screenshot-1ZYDAgCl@2x.png 1744w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">n8n Adaptive RAG (choosing the right retrieval strategy)</span></figcaption></figure>
<!--kg-card-begin: html-->
<script>
  workflowBanner(5111, document.currentScript);
</script>
<!--kg-card-end: html-->
<p>This workflow is built around a multi-stage process where agents make decisions to tailor the retrieval and generation process.</p><ol><li>Query classification: When a user submits a query, the first AI agent doesn&apos;t try to answer it. Its only job is to classify the user&apos;s intent into one of four categories: Factual, Analytical, Opinion, or Contextual.</li><li>Strategic routing: A Switch node directs the flow to one of four distinct paths based on the classification. Each path is a specialized strategy for handling that type of query.</li><li>Query adaptation: On each path, another AI agent adapts the original query to optimize it for retrieval.<ol><li>For factual queries, the agent rewrites the question to be more precise.</li><li>For analytical queries, the agent breaks the question down into several sub-questions to ensure broad coverage.</li><li>For opinion queries, the agent identifies different viewpoints to search for.</li></ol></li><li>Tailored retrieval and generation: The adapted query is used to retrieve relevant documents from a vector store. Finally, a concluding agent generates the answer using a system prompt specifically designed for the original query type (e.g., &quot;be precise&quot; for factual, &quot;present diverse views&quot; for opinion).</li></ol><p>This workflow is a prime example of Agentic RAG because it moves beyond simply routing to different data sources and instead routes to different information retrieval strategies.</p><p>The initial classification agent acts as a sophisticated Retriever Router. It&apos;s making an autonomous decision about the user&apos;s intent, which dictates the entire subsequent workflow. A simple RAG system lacks this understanding and uses a one-size-fits-all approach.</p><p>The agents in each of the four paths actively transform the user&apos;s query. They aren&apos;t just passing it along, they are working to improve it based on the initial classification.</p><h3 id="ai-agent-with-a-dynamic-knowledge-source">AI Agent with a dynamic knowledge source</h3><p>This workflow demonstrates a core principle of Agentic RAG: dynamic source selection. Instead of relying on a single knowledge base, we&apos;ll build an AI agent that can intelligently choose between two different information sources: a static RAG database for foundational knowledge and a live search engine for current events.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/09/screenshot-Xi4WlZS6@2x.png" class="kg-image" alt="Agentic RAG: A Guide to Building Autonomous AI Systems" loading="lazy" width="1472" height="902" srcset="https://blog.n8n.io/content/images/size/w600/2025/09/screenshot-Xi4WlZS6@2x.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/09/screenshot-Xi4WlZS6@2x.png 1000w, https://blog.n8n.io/content/images/2025/09/screenshot-Xi4WlZS6@2x.png 1472w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">AI Agent with a dynamic knowledge source</span></figcaption></figure>
<!--kg-card-begin: html-->
<script>
  workflowBanner(5398, document.currentScript);
</script>
<!--kg-card-end: html-->
<p>The main component of this workflow is the AI Agent node. This agent is connected to two distinct &quot;tools&quot; that it can use to answer questions:</p><ol><li>A RAG MCP server: This server is connected to a traditional RAG database containing specific, pre-loaded information (in this case, about the Model Context Protocol).</li><li>A search engine MCP server: This server gives the agent the ability to perform real-time web searches, providing access to up-to-the-minute information.</li></ol><p>Why is this considered &quot;Agentic RAG&quot;? This setup goes beyond simple RAG because the AI isn&apos;t just retrieving information; it&apos;s making a decision. When a user asks a question, the agent must first analyze the query and decide which tool is best suited to answer it.</p><p>This is the &quot;Retriever Router&quot; concept in action. The Model Context Protocol (MCP) acts as the communication layer that allows the agent to understand its available tools (the two servers) and choose one.</p><p>For example, if you ask, &quot;What is Model Context Protocol?&quot;, the agent will recognize this as a foundational question and route it to the RAG MCP Server. However, if you ask, &quot;Who won the Formula 1 race last weekend?&quot;, the agent understands this requires current information and will use the Search Engine MCP Server to find the answer.</p><p>This autonomous decision-making is what makes the workflow &quot;agentic&quot;.</p><h3 id="ai-agent-for-tabular-and-unstructured-data-sql-graphrag">AI agent for tabular and unstructured data (SQL + GraphRAG)</h3><p>This advanced workflow addresses one of the most significant challenges for traditional RAG systems: handling structured, tabular data from sources like Excel files or Google Sheets. While standard RAG excels at searching text, it often fails when asked to perform precise calculations or comparisons on relational data because the chunking process breaks the table&apos;s structure.</p><p>This system solves that problem by creating a hybrid agent that can choose between SQL queries for tabular data and GraphRAG for unstructured documents.</p><p>Check out the original YouTube video here: </p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/BTxghC1qHbw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="The ULTIMATE n8n Agentic RAG System (SQL + GraphRAG)"></iframe></figure><p>The workflow is built around an intelligent data ingestion process that treats data differently based on its type. The process begins when a new file is added to a designated Google Drive folder. An initial step in the n8n workflow checks the file type to determine the correct processing path.</p><p>For tabular data (Excel/Sheets), the system executes a series of steps to properly structure it for querying:</p><ol><li>The file is downloaded and its contents are extracted.</li><li>A code node then creates a new PostgreSQL table in a database like Supabase. It dynamically generates a database schema from the file&apos;s headers.</li><li>Finally, it populates the new table with the data, handling various data types like text, numbers, and dates.</li></ol><p>Unstructured data (PDFs, Word documents) are routed to the GraphRAG system for a more sophisticated ingestion process using a library called <a href="https://github.com/HKUDS/LightRAG"><u>LightRAG</u></a>. In short:</p><ol><li>Instead of simply chunking the text, an LLM first analyzes the document&apos;s content to identify key entities (like people, companies, or concepts) and the relationships that connect them.</li><li>These extracted entities and relationships are then used to build a structured <strong>knowledge graph</strong>. This graph represents the core information from the document and is usually stored in a dedicated graph database.</li></ol><p>Why is this considered &quot;Agentic RAG&quot;? The system doesn&apos;t just index all incoming data the same way. An agent makes a decision based on the file type, choosing a different, more effective storage strategy for tabular data (SQL database) versus unstructured documents (GraphRAG).</p><p>It also decides which knowledge source is appropriate, based on the user&apos;s question. If the query is best answered with tabular data, it generates an SQL query and uses the execute SQL query tool to get the answer directly from the database. If the question is about document content, it routes the query to the GraphRAG tool instead.</p><h2 id="faqs">FAQs</h2><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">What is the difference between self RAG and agentic RAG?</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">While both represent advancements over simple RAG, they focus on improving the process in different ways. The key difference is that Self-RAG builds decision-making into the Language Model itself, while Agentic RAG builds decision-making into the workflow around the model.</span><br><br><span style="white-space: pre-wrap;">Self-RAG is a specific framework that fine-tunes a model to make its own retrieval decisions during generation. It uses special &quot;reflection tokens&quot; to internally decide if it needs to search for information, if the retrieved documents are relevant, and if its own answer is well-supported by the facts. It&#x2019;s about giving the model the ability to self-correct and self-assess its own process.</span><br><br><span style="white-space: pre-wrap;">Agentic RAG, as we&apos;ve discussed, is a broader architectural pattern. It uses LLM-powered agents to manage an external workflow. This includes analyzing a user&apos;s intent to choose the right tool, adapting the query to a specific strategy, and critiquing the final answer.</span></p></div>
        </div><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">What is the difference between graph RAG and agentic?</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">Agentic RAG is about the intelligence and autonomy of the system&apos;s decision-making and workflow execution. Graph RAG is about the structure and richness of the underlying knowledge base, using knowledge graphs to enable more precise, relational, and multi-hop information retrieval. An Agentic RAG system might incorporate a Graph RAG as one of its specialized &quot;retriever agents&quot; or tools for querying structured data, demonstrating how these variants can complement each other.</span><br><br><span style="white-space: pre-wrap;">The key differentiator between traditional RAG and Graph RAG is the underlying database. Graph RAG tipically involves querying a graph database, for example Neo4j, ArangoDB etc. In contrast traditional RAG involves querying a vector database (vector stores), for example Pinecone, Qdrant, Milvus etc.</span></p></div>
        </div><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">What is the difference between RAG and multi-model RAG?</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">Multi-model RAG is an approach where different, specialized AI models are used at various stages of the RAG pipeline to improve performance and handle complex tasks. This can involve several strategies:</span></p><p><span style="white-space: pre-wrap;">Diverse LLM utilization: In complex applications, a &quot;multi-LLM strategy&quot; can be used to assign different Large Language Models (LLMs) to the jobs they perform best. This might involve specialized task-specific models like:</span><br><span style="white-space: pre-wrap;">1. Named Entity Recognition (NER) models to extract specific entities for metadata filtering.</span><br><span style="white-space: pre-wrap;">2. Hallucination-detection and moderation models to ensure the quality and safety of the final answer.</span></p><p><span style="white-space: pre-wrap;">Agentic RAG is a great example of a multi-model approach. This setup uses multiple LLM agents that work together to solve a problem. These agents can have specific &quot;profiles&quot; (like a &quot;coder&quot; agent and a &quot;tester&quot; agent), coordinate their actions, and provide feedback to each other to tackle complex, multi-step tasks.</span></p></div>
        </div><h2 id="wrap-up">Wrap up</h2><p>As we&apos;ve covered, Agentic RAG is a big step up from traditional RAG systems. It moves away from the simple &quot;retrieve-then-read&quot; process and uses AI agents to create a smarter, more flexible workflow.</p><p>This shift means that agents make their own decisions at every step of the information lifecycle. In the storage phase, they can intelligently figure out how to index information, choosing the best chunking strategy or metadata to make the knowledge base more effective. During retrieval, they act as a smart router, choosing the best tool for a specific query, whether that&apos;s a vector database, a SQL database, or a live web search. Finally, in the generation phase, they don&apos;t just give an answer, they can also review their own work for accuracy, triggering more search rounds if the first answer isn&apos;t good enough.</p><p>As the <a href="https://n8n.io/workflows/categories/ai-rag/" rel="noreferrer">n8n RAG workflow examples</a> have shown, these capabilities are not just theories but are practical tools you can use today to build the next generation of powerful and trustworthy AI applications.</p><h2 id="what%E2%80%99s-next">What&#x2019;s next?</h2><p>The next step is to move from theory to practice. Think about your own data and the challenges you face. Could an agent that chooses between a database and a web search improve your results? Could adapting the retrieval strategy based on a user&apos;s query provide more relevant answers?</p><p>Check out these step-by-step video guides on how to build Agentic RAG systems with n8n:</p><ul><li><a href="https://www.youtube.com/watch?v=mQt1hOjBH9o" rel="noopener">I Built the ULTIMATE n8n RAG AI Agent Template</a></li><li><a href="https://www.youtube.com/watch?v=BhGaGFH0jR4" rel="noopener">Store All Data Types with Agentic RAG in n8n</a></li><li><a href="https://www.youtube.com/watch?v=BTxghC1qHbw" rel="noopener">The ULTIMATE n8n Agentic RAG System (SQL + GraphRAG)</a></li><li><a href="https://www.youtube.com/watch?v=rh2JRWsLGfg" rel="noopener">Building with Reasoning LLMs | n8n Agentic RAG Demo + Template</a></li></ul><p>For a deeper look into the fundamentals, explore this tutorial on <a href="https://blog.n8n.io/rag-chatbot/" rel="noreferrer">building a custom knowledge RAG chatbot using n8n</a>. You don&apos;t have to start from scratch, browse the <a href="https://n8n.io/workflows/categories/ai/" rel="noopener">pre-built AI workflows from the n8n community</a> to use as a starting point for your own projects.</p>]]></content:encoded></item><item><title><![CDATA[Evaluating RAG, aka Optimizing the Optimization]]></title><description><![CDATA[RAG isnt foolproof. Explore common hallucinations, evaluation metrics, and how to improve RAG accuracy in n8n.]]></description><link>https://blog.n8n.io/evaluating-rag-aka-optimizing-the-optimization/</link><guid isPermaLink="false">68a6ef54d319380001f37695</guid><category><![CDATA[AI]]></category><dc:creator><![CDATA[Andrew Green]]></dc:creator><pubDate>Thu, 21 Aug 2025 10:17:35 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/08/workflow-template--1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/08/workflow-template--1-.png" alt="Evaluating RAG, aka Optimizing the Optimization"><p>Retrieval augmented generation is often positioned as the <a href="https://platform.openai.com/docs/guides/optimizing-llm-accuracy/"><u>go-to solution for optimizing LLMs</u></a>. But despite the integration of RAG in agentic systems, LLMs may still present unsupported or contradictory claims to the retrieved contents.</p><p>Imagine a business analyst at a logistics company using an internal AI assistant powered by RAG to interact with financial reports. When the analyst asks, <em>&#x201C;What is our Q2 performance?&#x201D;</em>, the assistant responds: <em>&#x201C;Our Q2 revenue decreased by 15% compared to Q1 due to supply chain disruptions following the Suez Canal blockage.&#x201D;</em> </p><p>While the system correctly retrieved the financial report, noting a 15% revenue drop, it fabricated a justification by attributing the decline to the Suez Canal blockage&#x2014;an explanation not present in the source material.</p><p>Retrieving documents doesn&#x2019;t guarantee accuracy, so RAG itself must be optimized. This means tuning the search to return the right results, including less noise, and aligning the LLM response with the context retrieved.</p><p>That&#x2019;s why in this article, we&#x2019;ll discuss how RAG systems can still hallucinate, and provide a framework for evaluating RAG applications using the Ragas framework. Lastly, we&#x2019;ll present how to implement RAG evaluations in n8n.</p><h2 id="four-types-of-rag-hallucinations">Four types of RAG hallucinations</h2><p>Hallucinations have a slightly different definition in the context of RAG. We use the term to indicate a response is not supported by or aligned with the retrieved context. It is considered a hallucination when the LLM does not generate content based on the textual data provided to it as part of the RAG retrieval process, but rather generates content based on its pre-trained knowledge.</p><p><a href="https://www.vectara.com/blog/hhem-2-1-a-better-hallucination-detection-model"><u>Vectara, the creators of the HHEM evaluation models</u></a>, give the following example: if the retrieved context states <em>&quot;The capital of France is Berlin&quot;</em>, and the LLM outputs <em>&quot;The capital of France is Paris&quot;</em>, then the LLM response is hallucinated, despite it being correct.</p><p>We can categorize RAG-specific hallucinations into four categories, as described in the paper titled <a href="https://arxiv.org/pdf/2401.00396"><u>RAGTruth</u></a>:</p><ul><li><strong>Evident Conflict:</strong> for when generative content presents direct contraction or opposition to the provided information. These conflicts are easily verifiable without extensive context, often involving clear factual errors, misspelled names, incorrect numbers, etc.</li><li><strong>Subtle Conflict</strong>: for when generative content presents a departure or divergence from the provided information, altering the intended contextual meaning. These conflicts often involve substitution of terms that carry different implications or severity, requiring a deeper understanding of their contextual applications.&#xA0;</li><li><strong>Evident Introduction of Baseless Information:</strong> for when generated content includes information not substantiated in the provided information. It involves the creation of hypothetical, fabricated, or hallucinatory details lacking evidence or support.&#xA0;</li><li><strong>Subtle Introduction of Baseless Information</strong>: is when generated content extends beyond the provided information by incorporating inferred details, insights, or sentiments. This additional information lacks verifiability and might include subjective assumptions or commonly observed norms rather than explicit facts.&#xA0;</li></ul><h2 id="the-two-pillar-rag-evaluation-framework">The two-pillar RAG evaluation framework</h2><p>A good RAG implementation can validate two things:</p><ol><li>Making sure RAG retrieves the right information. This is RAG Document Relevance.</li><li>Ensuring LLM answers are consistent with the context retrieved via RAG. This is RAG Groundedness</li></ol><p>Most tools available today use the <a href="https://docs.ragas.io/en/stable/concepts/metrics/overview/"><u>Ragas library</u></a>, which provides a set of RAG-specific evaluation functions. We use the evaluations available in the Ragas library in the descriptions below.</p><h3 id="rag-document-relevance-retrieving-the-right-context">RAG Document Relevance: Retrieving the right context</h3><p>The<strong> Context Recall </strong>evaluation measures how many of the relevant documents were successfully retrieved. Higher recall means fewer relevant documents were left out.<strong> Context Precision</strong> is a metric that measures the proportion of relevant chunks in the retrieved contexts. Calculating context recall always requires a reference to compare against.&#xA0;</p><p>Both recall and precision can be calculated using a judge LLM or by using deterministic calculations.</p><p><strong>LLM-based Context Recall</strong> is computed using three variables - the user input, the reference and the retrieved contexts.&#xA0; To estimate context recall from the reference, the reference is broken down into claims, each claim in the reference answer is analyzed to determine whether it can be attributed to the retrieved context or not.&#xA0;</p><p><strong>LLM-based Context Precision</strong> is used to estimate if a retrieved context is relevant or not by having an LLM compare each of the retrieved contexts or chunks present in retrieved contexts with the response.</p><p><strong>Non-LLM-Based</strong>the <strong> Context Recall</strong> <strong>and Precision</strong> compare retrieved contexts or chunks with the reference contexts. These metrics use measures such as semantic similarity, Levenshtein Similarity Ratio and string comparison metrics to determine if a retrieved context is relevant or not.&#xA0;</p><h3 id="rag-groundedness-evaluating-responses-against-retrieved-context">RAG Groundedness: Evaluating responses against retrieved context</h3><p><strong>Faithfulness </strong>determines how factually consistent a response is with the retrieved context. A response is considered faithful if all its claims can be supported by the retrieved context. <a href="https://huggingface.co/vectara/hallucination_evaluation_model"><u>Vectara&apos;s HHEM-2.1-Open</u></a> is an open source classifier model that is trained to detect hallucinations from LLM generated text. It can be used to cross-check claims with the given context to determine if it can be inferred from the context.&#xA0;</p><p><strong>Response Relevancy</strong> measures how relevant a response is to the user input. An answer is considered relevant if it directly and appropriately addresses the original question. This metric focuses on how well the answer matches the intent of the question, without evaluating factual accuracy. It penalizes answers that are incomplete or include unnecessary details.</p><h2 id="rag-evaluations-in-n8n">RAG Evaluations in n8n</h2><p>You can evaluate RAG performance in n8n without external libraries or calls. The evaluations natively available include both RAG document relevance and answer groundedness. These calculate whether the documents retrieved are relevant to the question and if the answer is grounded in the documents retrieved. RAG evaluations are run against a test dataset, and results can be compared across runs to see how the metrics change and drill down into the reasons for those changes.</p><p>The<a href="https://n8n.io/workflows/4426-evaluate-rag-response-accuracy-with-openai-document-groundedness-metric/"><u> Evaluate RAG Response Accuracy with OpenAI workflow</u></a> template uses LLM-based response relevancy to assess whether the response is based on the retrieved documents. A high score indicates LLM adherence and alignment whereas a low score could signal inadequate prompt or model hallucination.</p><p>The <a href="https://n8n.io/workflows/4273-evaluation-metric-example-rag-document-relevance/"><u>RAG document relevance</u></a> workflow uses an LLM-based context recall to calculate a retrieval score for each input and determine whether the workflow is performing well or not.</p><p>To understand more about evaluation in n8n, check out the <a href="https://blog.n8n.io/introducing-evaluations-for-ai-workflows/"><u>Introducing Evaluations for AI Workflows blog post</u></a>, and our<a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.evaluation/"><u> technical documentation on the evaluation node.</u></a></p>
<!--kg-card-begin: html-->
<script>
  workflowBanner([4271, 5523, 4424, 4428], document.currentScript, { 
      workflowsHeader: "More n8n workflows for AI Evaluations"
  });
</script>

<!--kg-card-end: html-->
]]></content:encoded></item><item><title><![CDATA[12 Best Autonomous AI Agents  2025s Top Picks]]></title><description><![CDATA[Tired of doing repetitive tasks? These 12 AI agents handle complex workflows independently while you focus on the fun stuff. Plus n8n tutorials for ultimate customization!]]></description><link>https://blog.n8n.io/best-autonomous-ai-agents/</link><guid isPermaLink="false">689cec0bd319380001f375e1</guid><category><![CDATA[AI]]></category><category><![CDATA[Guide]]></category><dc:creator><![CDATA[Yulia Dmitrievna]]></dc:creator><pubDate>Thu, 21 Aug 2025 09:54:35 GMT</pubDate><media:content url="https://blog.n8n.io/content/images/2025/08/autonomous-ai-agents--1-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.n8n.io/content/images/2025/08/autonomous-ai-agents--1-.jpg" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks"><p>The promise of fully autonomous AI agents sounds appealing: just set a goal and let AI handle everything.</p><p>In reality, complete autonomy often comes with a hidden cost.</p><p>While it&#x2019;s relatively straightforward to build human-in-the-loop agents for specific tasks, creating truly autonomous systems means surrendering control to LLMs. And despite their capabilities, LLMs can deviate from the optimal path in countless unexpected ways.</p><p>This leads to a fundamental tradeoff that every business faces: how much autonomy do you actually want, and how much oversight can you realistically provide?</p><p>The answer isn&#x2019;t universal. A legal firm reviewing contracts needs different guardrails than a sales team enriching prospect data. Some workflows benefit from full autonomy, while others require strategic human checkpoints.</p><p>In this guide, we review 12 autonomous AI agents that handle this tradeoff in different ways &#x2013; from simple no-code builders to sophisticated systems that operate independently for hours on end.</p><p>We also show you how platforms like <a href="https://n8n.io/ai-agents/"><u>n8n</u></a> let you build custom autonomous workflows where you control exactly how much independence you want to grant your AI agents.</p><h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#what-are-autonomous-ai-agents">What are autonomous AI agents?</a>
<ul>
<li><a href="#core-characteristics-of-autonomous-ai-agents">Core characteristics of autonomous AI agents</a></li>
<li><a href="#difference-from-traditional-ai-agents">Difference from traditional AI agents</a></li>
<li><a href="#key-autonomous-ai-agents-use-cases">Key autonomous AI agents use cases</a></li>
</ul>
</li>
<li><a href="#12-top-autonomous-ai-agents-of-2025">12 top autonomous AI agents of 2025</a>
<ul>
<li><a href="#lindy-ai">Lindy AI</a></li>
<li><a href="#relevance-ai">Relevance AI</a></li>
<li><a href="#harvey-ai">Harvey AI</a></li>
<li><a href="#clay">Clay</a></li>
<li><a href="#hubspot-breeze">HubSpot Breeze</a></li>
<li><a href="#salescloser-ai">Salescloser AI</a></li>
<li><a href="#vapi">VAPI</a></li>
<li><a href="#box-ai-agents">Box AI Agents</a></li>
<li><a href="#browserbase-director">Browserbase Director</a></li>
<li><a href="#legacy-use">Legacy-use</a></li>
<li><a href="#droidrun">Droidrun</a></li>
<li><a href="#claude-code">Claude Code</a></li>
</ul>
</li>
<li><a href="#building-fully-autonomous-ai-agents-with-n8n">Building fully autonomous AI agents with n8n</a>
<ul>
<li><a href="#flexible-autonomy-your-way">Flexible autonomy, your way</a></li>
<li><a href="#recent-n8n-updates-make-autonomous-agents-more-reliable">Recent n8n updates make autonomous agents more reliable</a></li>
<li><a href="#building-your-first-autonomous-agent-in-n8n">Building your first autonomous agent in n8n</a></li>
</ul>
</li>
<li><a href="#wrap-up">Wrap up</a></li>
<li><a href="#what%E2%80%99s-next">What&#x2019;s next?</a></li>
</ul>
<h2 id="what-are-autonomous-ai-agents">What are autonomous AI agents?</h2><p><strong><em>The main difference between traditional AI agents and autonomous ones lies in the level of human oversight required to define and execute objectives.</em></strong></p><p>Consider a customer service chatbot that follows your predefined rules and forwards complex issues to humans. It&#x2019;s helpful, but it operates within the strict boundaries you&#x2019;ve set.</p><p>Now imagine an AI system tasked with &#x201C;increasing customer retention by 15%.&#x201D; A truly autonomous version would independently analyze your customer data, identify at-risk segments, design retention campaigns, and execute them across multiple channels &#x2013; without asking for permission at every step.</p><p>This level of independence can deliver impressive results, but it also leads to unpredictability. The agent might choose strategies you wouldn&#x2019;t have considered, access data in unexpected ways, or interpret &#x201C;success&#x201D; differently than you intended.</p><p>Most successful implementations today operate somewhere in the middle&#x2014;autonomous enough to handle complex multi-step tasks, but with strategic human oversight at critical decision points.</p><h3 id="core-characteristics-of-autonomous-ai-agents">Core characteristics of autonomous AI agents</h3><p>Autonomous AI agents share several key characteristics that distinguish them from traditional automation:</p><ul><li><strong>Goal-driven behavior</strong>: they work toward their objectives rather than simply following predefined steps;</li><li><strong>Multi-step planning</strong>: they can break down complex goals into actionable subtasks;</li><li><strong>Environmental adaptation</strong>: they adjust their approach based on real-time feedback and changing conditions;</li><li><strong>Tool integration</strong>: they can connect and coordinate multiple systems to accomplish their goals.</li></ul><h3 id="difference-from-traditional-ai-agents">Difference from traditional AI agents</h3><table>
<thead>
<tr>
<th>Feature</th>
<th>AI Agents</th>
<th>Autonomous AI Agents</th>
</tr>
</thead>
<tbody>
<tr>
<td>Autonomy Level</td>
<td>Partial</td>
<td>Full or near-full</td>
</tr>
<tr>
<td>Task Complexity</td>
<td>Single or simple workflows</td>
<td>Complex, evolving<br>multi-step plans</td>
</tr>
<tr>
<td>Human Supervision</td>
<td>Frequent or required</td>
<td>Minimal or optional</td>
</tr>
<tr>
<td>Adaptability</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Planning &amp; Reasoning</td>
<td>Rule-based or reactive</td>
<td>Goal-directed and strategic</td>
</tr>
</tbody>
</table>
<h3 id="key-autonomous-ai-agents-use-cases">Key autonomous AI agents use cases</h3><p>Autonomous AI agents excel in scenarios where human supervision would slow down complex, multi-step processes. Here are the most common areas of application:</p><ul><li><strong>Research and competitive intelligence</strong>: long-running research projects with source data cross-referencing and synthesis into the final reports, including graphics, code snippets and bibliography.</li><li><strong>Complex workflow automation</strong>: complete business processes from lead qualification to customer onboarding, adapting to each prospect&#x2019;s characteristics.</li><li><strong>Web navigation and data extraction</strong>: navigate website pages, handle dynamic content, and extract structured data while adapting to changes.</li><li><strong>Strategic decision-making</strong>: analyze conditions, evaluate options and execute decisions within predefined parameters.</li></ul>
<!--kg-card-begin: html-->
<script> workflowBanner ([2651, 2324, 2682, 2315 ], document.currentScript, {workflowsHeader: "n8n AI agents examples with different architectures and use cases" }); </script>
<!--kg-card-end: html-->
<h2 id="12-top-autonomous-ai-agents-of-2025">12 top autonomous AI agents of 2025</h2><p>The autonomous AI agent landscape includes hundreds of specialized tools. We&#x2019;ve curated 12 agents that represent different approaches to the autonomy challenge.</p><p>Our selection follows a progression: we start with user-friendly business-focused agents that anyone can deploy quickly. Then we move into specialized agents built for specific industries like legal or sales. Finally, we explore infrastructure-level tools that developers use to build custom autonomous systems from the ground up.</p><p>This selection reflects the variety of approaches currently being tested across different industries and use cases.</p><table>
<thead>
<tr>
<th>Tool</th>
<th>Category</th>
<th>Target Audience</th>
<th>Primary Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#lindy-ai">Lindy AI</a></td>
<td>No-Code agent builder</td>
<td>Business users</td>
<td>Workflow automation,<br>customer service,<br>lead generation</td>
</tr>
<tr>
<td><a href="#relevance-ai">Relevance AI</a></td>
<td>Multi-agent platform</td>
<td>Enterprise teams</td>
<td>Multi-agent workflows,<br>business process automation</td>
</tr>
<tr>
<td><a href="#harvey-ai">Harvey AI</a></td>
<td>Legal AI agents</td>
<td>Legal professionals</td>
<td>Legal document review,<br>contract analysis,<br>compliance</td>
</tr>
<tr>
<td><a href="#clay">Clay</a></td>
<td>Sales enrichment</td>
<td>Sales teams</td>
<td>Sales prospecting,<br>data enrichment,<br>outreach personalization</td>
</tr>
<tr>
<td><a href="#hubspot-breeze">HubSpot<br>Breeze</a></td>
<td>CRM AI agents</td>
<td>HubSpot users</td>
<td>Marketing, sales,<br>customer service<br>automation within HubSpot</td>
</tr>
<tr>
<td><a href="#salescloser-ai">SalesCloser AI</a></td>
<td>Sales automation</td>
<td>Sales teams</td>
<td>Automated sales conversations,<br>lead qualification</td>
</tr>
<tr>
<td><a href="#vapi">VAPI</a></td>
<td>Voice AI infrastructure</td>
<td>Developers</td>
<td>Voice AI applications,<br>phone automation,<br>conversational interfaces</td>
</tr>
<tr>
<td><a href="#box-ai-agents">Box AI Agents</a></td>
<td>Document AI agents</td>
<td>Enterprise<br>document<br>management</td>
<td>Document analysis,<br>content management,<br>enterprise search</td>
</tr>
<tr>
<td><a href="#browserbase-director">Browserbase<br>Director</a></td>
<td>Browser automation</td>
<td>Developers/<br>automation teams</td>
<td>Web scraping,<br>browser automation,<br>testing</td>
</tr>
<tr>
<td><a href="#legacy-use">legacy-use</a></td>
<td>Legacy system<br>integration</td>
<td>Enterprise IT</td>
<td>Legacy system<br>API modernization</td>
</tr>
<tr>
<td><a href="#droidrun">Droidrun</a></td>
<td>Mobile automation</td>
<td>Mobile app developers</td>
<td>Android device automation,<br>mobile testing</td>
</tr>
<tr>
<td><a href="#claude-code">Claude Code</a></td>
<td>Coding assistant</td>
<td>Software developers</td>
<td>Autonomous coding,<br>code review,<br>development automation</td>
</tr>
</tbody>
</table>
<h3 id="lindy-ai">Lindy AI</h3><p>A no-code platform that lets you build AI agents by describing tasks in English, with a drag-and-drop interface.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdpyBK-OV1YhyHnxGsGA2-54AJ4fM6Ca550z0JMR1qkVlU9hzxo3shBXFpHlWsx0o83xdFW9Hlt6NFOIi_2AAxhfswL6m6JSwdLUPHblwubxOqzrBG2-HFnXV_oJxspaMUBsiftfg?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="287"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Lindy allows creating simple agents with basic business features</em></i></figcaption></figure><p><strong>Distinctive features</strong>: <a href="https://www.lindy.ai/"><u>Lindy</u></a> allows you to create AI-employees in minutes and add components with typical tasks such as a phone call, email or calendar. The platform also offers access to a template marketplace where you can select pre-built agents for common tasks.</p><p><strong>Strengths</strong>:</p><ul><li>Beginner-friendly setup,</li><li>strong customer service automation capabilities,</li><li>over 100 native integrations, including CRMs, email platforms, and calendars.</li></ul><p><strong>Ideal users</strong>: Small businesses and non-technical teams who want automation without the complexity - perfect for customer support, lead nurturing and meeting scheduling.</p><p><strong>Pricing</strong>: Starting at $50/month for 5,000 credits, with a free tier that offers 400 credits per month for basic actions to test the waters.</p><h3 id="relevance-ai">Relevance AI</h3><p>A multi-agent platform where teams of specialized AI agents collaborate on complex workflows, applying advanced chain-of-thought reasoning.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeaa7vxRltooLLY8EVWVdu-X2ncDx0y5ifr2ZkqljL3x4ecE842CvDKZqb8oOyVFXh75GfdYoyzkpJi5oF-tXqlfykEj0gDxEJv7U-UV6XPYfur5NG_4KV7UyNI2-hwmrocii0krQ?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="351"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Relevance AI focuses on multi-agent interactions</em></i></figcaption></figure><p><strong>Distinctive features</strong>: Unlike single-agent platforms, <a href="https://relevanceai.com/"><u>Relevance AI</u></a> deploys multiple agents that work together - think of it as hiring an entire AI department rather than just one assistant. The platform includes agent-to-agent communication protocols and dynamic task allocation among team members.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Superior multi-agent coordination,&#xA0;</li><li>enterprise-grade security with SOC2 compliance,&#xA0;&#xA0;</li><li>comprehensive analytics to monitor the performance of your AI team.</li></ul><p><strong>Ideal users</strong>: Enterprise teams handling complex business processes that require multiple specialized skills - like research pipelines, content generation workflows, or multi-stage sales processes.</p><p><strong>Pricing</strong>: Starting at $19/month for 10,000 credits, with enterprise plans offering custom pricing and dedicated infrastructure.</p><h3 id="harvey-ai">Harvey AI</h3><p>Legal-specific AI agents designed exclusively for law firms, with built-in understanding of legal processes and jurisdiction variations.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfZensEm44ukgsM5QGrbyUxHAKOMGyzml9-_mZX-qX6LCCgCUs14ce8qOOmrwCjh4rL2JSkVSBr358cqX_tjQteu6t4hf_ySGCOI5qiB8rakiWQWNkM1dNkv2Nik5qNk9N3zvgJow?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="331"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Harvey offers several features optimized for lawyers</em></i></figcaption></figure><p><strong>Distinctive features</strong>: Unlike general-purpose platforms such as Lindy or Relevance AI, <a href="https://www.harvey.ai/"><u>Harvey</u></a> is specifically designed for legal tasks. It understands attorney-client privilege, ensures compliance across different jurisdictions, and integrates directly into legal practice management systems.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>In-depth legal expertise that generic AI agents can&#x2019;t match,&#xA0;</li><li>regulatory compliance integrated into every workflow,&#xA0;</li><li>proven ROI at large firms like Allen &amp; Overy and PwC.</li></ul><p><strong>Ideal users</strong>: Law firms and corporate legal departments involved in contract review, legal research, due diligence, and compliance monitoring - areas where legal accuracy is essential.</p><p><strong>Pricing</strong>: Enterprise-only with custom pricing.</p><h3 id="clay">Clay</h3><p>Sales intelligence platform where AI agents autonomously research prospects and enrich data by combining information from 50+ sources.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeMbfq7P76elysMwfqFs_JTAxxnwM2HJb0ruY4rz3zL4mTfXWrDmQLix6iJbKg-F5lYAsdBNn63fHHr5yxNs-3WVXwLXdCt8OhrwupBUGoVBFYS2UhKj0QU76M8LZHSKUEl4GZrkw?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="233"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Clay works like a smart AI-powered spreadsheet</em></i></figcaption></figure><p><strong>Distinctive features</strong>: While Harvey AI specializes in legal tasks, <a href="https://www.clay.com/"><u>Clay</u></a> focuses entirely on sales intelligence. Its &#x201C;waterfall enrichment&#x201D; feature automatically tries multiple data sources until it finds the information you need and can perform complex web searches that go far beyond what CRM-integrated tools like HubSpot Breeze offer.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Exceptional data quality with comprehensive prospect profiles,&#xA0;</li><li>AI-powered personalization at scale, and integration with popular tools <a href="https://community.clay.com/x/support/o8wav1inzt61/integrating-clay-with-n8n-understanding-http-respo"><u>such as n8n</u></a>.</li></ul><p><strong>Ideal users</strong>: Sales teams and revenue operations that need thorough prospect research and account-based marketing - especially those handling high-value B2B sales where personalization matters.</p><p><strong>Pricing</strong>: Starting at $149/month for 2,000 credits, with usage-based pricing that scales based on your research volume.</p><h3 id="hubspot-breeze">HubSpot Breeze</h3><p>CRM-integrated AI agents that work natively within HubSpot&#x2019;s ecosystem, covering prospecting, customer service, content creation and social media management.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdRPLh4b2kQX_Y3DoIwdw4OIS18c6LFNe39aK1DjebRHw9_6qjiaPBy8xN1hS7fAmJOm24DKPM7PzrY_DLqhW_BI_m_nLIZ5bJi-I9WDe-2_-F20pBrleT64qhoZ8GZZi91dpG7_Q?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="353"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Hubspot Breeze integrates autonomous agents inside the platform UI</em></i></figcaption></figure><p><strong>Distinctive features</strong>: Unlike standalone platforms like Clay or Lindy which require separate integrations, <a href="https://www.hubspot.com/products/artificial-intelligence"><u>Breeze agents</u></a> live directly inside HubSpot. You get multiple specialized agents - for prospecting, customer service, content, and social media - all sharing the same customer data without having to worry about setup.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Zero learning curve for existing HubSpot users,&#xA0;</li><li>seamless data flow across all agents,&#xA0;</li><li>enterprise-grade reliability that matches HubSpot&#x2019;s infrastructure.</li></ul><p><strong>Ideal users</strong>: HubSpot customers who want AI automation without the complexity of managing multiple tools - perfect for marketing and sales teams already invested in the HubSpot ecosystem.</p><p><strong>Pricing</strong>: Breeze uses HubSpot Credits which are included in every seat-based tier. Additional packs begin from $10/mo for 1000 credits.</p><h3 id="salescloser-ai">Salescloser AI</h3><p>Conversational AI that handles complete sales conversations autonomously across email, chat, SMS, and phone channels.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcS_XNggyTA_xp--OmtSlzVP58fQK0Wk4oCq-HppOPGUqor0PimA1Xmy2pdHDIjjlwysdetDobjGYcUuX5jiTqhi9hj40wb-Qu8um3czgooPV8bjk9O5xzc3oqNUYfdu9XscGw2?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="351"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Salescloser has a rather basic UI, where users can describe transitions in plain language</em></i></figcaption></figure><p><strong>Distinctive features</strong>: While HubSpot Breeze works within your existing CRM, <a href="https://salescloser.ai/"><u>Salescloser</u></a> focuses exclusively on actual sales conversations. It can qualify leads through natural dialogue, handle objections, and book appointments - essentially replacing your SDR team rather than just supporting it like Clay&#x2019;s research agents do.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Salescloser claims to offer a 30-40% improvement in lead qualification,&#xA0;</li><li>a natural conversation flow that doesn&#x2019;t feel robotic,&#xA0;</li><li>24/7 availability that never misses a hot lead.</li></ul><p><strong>Ideal users</strong>: SMBs and lead generation companies that need to scale their sales conversations without hiring more staff - especially effective for qualifying inbound leads and scheduling appointments.</p><p><strong>Pricing</strong>: Available upon request.</p><h3 id="vapi">VAPI</h3><p>Voice AI infrastructure platform that provides the building blocks for creating conversational voice applications with a response time of less than 500 ms.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfxJlLUoQwcHZ8-nv8wEz9fF_Q2k0xxcVqDWudtPOcBafm7TExD140R_t3DmsqqiRZnVISq0nHnFXc7TVSZ4qEZZJAhwbGux-m4zQkivk94hjTTmRJYVdZb36XPdcn-e-5hfo4Uxw?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="417"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Vapi allows creating agents within a single screen</em></i></figcaption></figure><p><strong>Distinctive features</strong>: Unlike conversation-oriented tools such as Salescloser which cover specific sales scenarios, <a href="https://vapi.ai/"><u>VAPI</u></a> is purely infrastructure - think of it as the voice equivalent of what n8n does for workflow automation. It offers real-time webhook integration during actual calls, making it perfect for building custom voice agents that can be connected with other automation platforms.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Extremely low-latency speech processing,&#xA0;</li><li>extensive customization options for brand-specific voice experiences,&#xA0;</li><li>direct telephony integration that allows you to build complex phone systems.</li></ul><p><strong>Ideal users</strong>: Developers and businesses building voice-enabled applications - from customer service phone automation to accessibility applications that require a reliable voice infrastructure.</p><p><strong>Pricing</strong>: Usage-based between $0.05-0.20 per minute, depending on the model, so you only pay for actual conversation time.</p><h3 id="box-ai-agents">Box AI Agents</h3><p>AI for enterprise document management that provides intelligent search, automatic classification, and workflow automation across organizational content.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcMvm28tsvepaXQfFVn8STk77ZDnUTYo4OzqQV4FIQx3H7hkqwF_YY9VSItypRPjVj2I7Z9F9Hkz-Q5For-fqoZCSQjSpP7hic5e-xZ4yhOacn4om1OdUgULxmQpuPImTfAva-okw?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="348"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Box.com focuses on agents for company document processing</em></i></figcaption></figure><p><strong>Distinctive features</strong>: <a href="https://www.box.com/ai"><u>Box AI Agents</u></a> specialize entirely in document intelligence within enterprise environments. Unlike general-purpose platforms, these agents understand document hierarchies, compliance requirements and can automatically enforce retention policies across large content repositories.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Deep integration with Box&#x2019;s content management ecosystem,&#xA0;</li><li>enterprise-grade security,&#xA0;</li><li>advanced document understanding that goes beyond simple text extraction.</li></ul><p><strong>Ideal users</strong>: Large organizations with complex document workflows, compliance requirements, and knowledge management needs - particularly useful for contract lifecycle management and regulatory reporting.</p><p><strong>Pricing</strong>: Included when purchasing Box Enterprise Plus / Enterprise Advanced plans, with additional AI units for high-volume usage.</p><h3 id="browserbase-director">Browserbase Director</h3><p>An LLM-powered browser automation platform that generates reproducible scripts from natural language instructions and is built on a scalable cloud infrastructure.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcAhSlEpqFf5fOaawCdBNejK-MY9DaiCKddxLBVNfg2Va1LB2sZtFNq2FI_IU4y0raTFpukkRigi2Ykv95LYMh9yp0fbDeSeReRbtrEu62R0NGztOoWFqO-anaKulkH01Dfyazj?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="347"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Director generates reusable browser automation scripts</em></i></figcaption></figure><p><strong>Distinctive features</strong>: <a href="https://www.director.ai/"><u>Browserbase Director</u></a> is a new product for creating web automations using AI. Unlike traditional browser automation which requires coding skills, Director lets you describe what you want in a text field and generates reusable scripts - bridging the gap between no-code simplicity and developer-grade reliability.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Advanced anti-detection capabilities that bypass bot protection,&#xA0;</li><li>reliable scaling across thousands of browser sessions,&#xA0;</li><li>the ability to turn one-time automations into repeatable workflows.</li></ul><p><strong>Ideal users</strong>: Developers and automation engineers who need sophisticated web scraping or automated testing - especially those building workflows that integrate with platforms like n8n.</p><p><strong>Pricing</strong>: Director is free to create automations, but Browserbase&apos;s standard rates apply for execution: staring at $20/month for the Developer Plan.</p><h3 id="legacy-use">Legacy-use</h3><p>An open-source tool that automatically creates REST APIs for legacy systems without requiring changes to existing applications.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcXAfsYABHZ9lTRZshGXTSPA6c1YKc_TbZ-Db4wsO9cTO1PcueM28AHSjrCmX3Zgp1AOF2qLk0VS09OeHzAjbegihUnyg2BsGCzEv5eKEKS2DBu_muDMm-bZUXevx4v6xnTr8llvw?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="327"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Legacy-use automates old Windows apps inside VM environments</em></i></figcaption></figure><p><strong>Distinctive features</strong>: While other tools focus on modern integrations, <a href="https://github.com/legacy-use/legacy-use"><u>legacy-use</u></a> solves the problem that many enterprises face - connecting decades-old systems with modern AI workflows. It uses multimodal LLMs to understand legacy interfaces and automatically generate API endpoints, making it possible to include old Windows software in your automation pipelines.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>No code changes to existing legacy systems required,&#xA0;</li><li>AI-driven automation that reduces manual configurations,&#xA0;</li><li>a cost-effective approach to modernization that doesn&#x2019;t require expensive system overhauls.</li></ul><p><strong>Ideal users</strong>: Enterprise IT teams and system integrators dealing with legacy applications or older ERP systems that need to be connected with modern tools and workflows.</p><p><strong>Pricing</strong>: Open-source with free community edition, plus commercial support and enterprise features available for complex deployments.</p><h3 id="droidrun">Droidrun</h3><p>Open-source framework that enables LLM-driven automation of Android devices.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcGc_j56CmRnevgGCgp2m7OSFtOjgxbfR-tvZ7WJMZF1sCzICA4qpfrruZaS-65R2H-Wdzz-FYJ2vjjGd0h6W9v6WxbHLMLMDIHxCcA9n8WRjeMsyrIu4qB-SADaLE8QM4zRDYBQA?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="313"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Droidrun allows automating both physical devices or virtual phones</em></i></figcaption></figure><p><strong>Distinctive features</strong>: While legacy-use modernizes old desktop software, <a href="https://github.com/droidrun/droidrun"><u>Droidrun</u></a> brings AI automation to mobile devices. It can control both physical and virtual Android devices using computer vision and natural language processing - essentially giving you an AI assistant that can navigate mobile apps just like a human.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>Native mobile automation capabilities that work with any Android app,&#xA0;</li><li>LLM-powered intelligent interactions that adapt to UI changes,&#xA0;</li><li>support for both testing scenarios and production use cases.</li></ul><p><strong>Ideal users</strong>: Mobile developers and QA engineers who need to automate app testing, user behavior simulation, or accessibility testing across different Android devices and app versions.</p><p><strong>Pricing</strong>: Open-source and free to use, with upcoming cloud platform options for large-scale mobile automation.</p><h3 id="claude-code">Claude Code</h3><p>Anthropic&#x2019;s official autonomous coding agent that handles complete feature development from natural language descriptions, with built-in CI/CD automation via GitHub Actions.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd4zuCkdfKUIT5WgeOp9qgyNeFIjkGJnuz-D5_PQ72XPzFmWZk_kqBof87rrQLTQe84MvFuQWDA1UBOIWUi068naekq1SO8ATNNzkV2hKIHMUEAwpR9zzIlaMfY2M2NzhAvjDbduw?key=dzJ9IAD3M84gmQvwQtl6xg" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="624" height="469"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Claude Code is an advanced CLI utility that can orchestrate multiple coding agents</em></i></figcaption></figure><p><strong>Distinctive features</strong>: While Claude models already support popular coding agents like Cursor and Aider, Anthropic now offers its own CLI tool. Compared to these alternatives,&#xA0; <a href="https://www.anthropic.com/claude-code"><u>Claude Code</u></a> enables more autonomous operation - it requires less manual oversight when handling multi-step development tasks from planning to deployment.</p><p><strong>Strengths</strong>:&#xA0;</p><ul><li>High-quality code generation with documentation,&#xA0;</li><li>reasoning capabilities for complex development tasks,&#xA0;</li><li><a href="https://github.com/anthropics/claude-code-action"><u>GitHub Actions integration</u></a> that automates your entire development pipeline.</li></ul><p><strong>Ideal users</strong>: Software developers and teams who want to automate feature development, code refactoring, and test creation - especially valuable for teams that want to focus on architecture while AI handles the implementation details.</p><p><strong>Pricing</strong>: Included in the Claude Pro subscription for $20/month or the Teams plan for $25/month. Also available via API with usage-based pricing.</p><h2 id="building-fully-autonomous-ai-agents-with-n8n">Building fully autonomous AI agents with n8n</h2><p>The agents we&#x2019;ve reviewed solve certain tasks well, but what if you need something that&#x2019;s better tailored to your business?</p><p>Perhaps you need an agent that processes invoices, updates your CRM, sends Slack notifications, and forwards complex cases to humans &#x2013; all based on your exact business rules. Or maybe you want to combine multiple AI capabilities while maintaining control over when the agent acts independently and when it asks for approval.</p><p>This is where <a href="https://n8n.io/ai-agents/"><u>n8n</u></a> becomes essential for building autonomous agents that work exactly the way you need them to:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.n8n.io/content/images/2025/08/screenshot-NLrv3heH-1.png" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="1156" height="600" srcset="https://blog.n8n.io/content/images/size/w600/2025/08/screenshot-NLrv3heH-1.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/08/screenshot-NLrv3heH-1.png 1000w, https://blog.n8n.io/content/images/2025/08/screenshot-NLrv3heH-1.png 1156w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Building fully autonomous AI agents with n8n</span></figcaption></figure><h3 id="flexible-autonomy-your-way">Flexible autonomy, your way</h3><p>With n8n, you can precisely control how autonomous your AI agents become. You can build anything from fully autonomous systems that run for hours without human intervention to <a href="https://blog.n8n.io/ai-agentic-workflows/"><u>agentic workflows</u></a> that combine AI decision-making with strategic human oversight.</p><p>The platform supports multiple triggers for hundreds of services, as well as <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.webhook/"><u>webhooks</u></a>, <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.scheduletrigger/"><u>schedules</u></a>, <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/"><u>file changes</u></a>, <a href="https://docs.n8n.io/integrations/builtin/trigger-nodes/n8n-nodes-base.postgrestrigger/"><u>database updates</u></a> &#x2013; so your agents can automatically spring into action based on real-world events. And with n8n&#x2019;s visual canvas, you can easily add <a href="https://docs.n8n.io/advanced-ai/examples/human-fallback/"><u>human-in-the-loop checkpoints</u></a> wherever you need them.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Pro tip: n8n also integrates with existing agents through <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolmcp/"><u>MCP (Model Context Protocol)</u></a> servers, and you can <a href="https://n8n.io/integrations/mcp-server-trigger/"><u>expose your workflows as tools</u></a> that external agents can trigger directly.</div></div><h3 id="recent-n8n-updates-make-autonomous-agents-more-reliable">Recent n8n updates make autonomous agents more reliable</h3><p>Several recent n8n features are specifically designed to address the challenges of building production-ready autonomous agents:</p><ul><li><strong>LLM streaming</strong>: <a href="https://community.n8n.io/t/stream-ai-responses-on-http-responses-llm-chains-and-ai-agents-nodes/52084/29"><u>real-time response streaming</u></a> means your agents can provide immediate feedback during long-running tasks, making them more responsive and enabling better monitoring.</li><li><strong>Model selector</strong>: automatic <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.modelselector/"><u>model selection</u></a> allows your agents to dynamically select the optimal LLM for each task or implement fallback mechanisms when a model is unavailable&#x2014;crucial for reliable autonomous operation.</li><li><strong>AI evaluations</strong>: the built-in <a href="https://blog.n8n.io/introducing-evaluations-for-ai-workflows/"><u>evaluation framework</u></a> helps you test and measure your autonomous agents before deployment, turning guesswork into evidence when optimizing performance.</li><li><strong>Sub-agents</strong>: the new <a href="https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolaiagent/"><u>AI Agent Tool</u></a> allows you to build multi-agent teams where a primary agent delegates specialized tasks to other agents&#x2014;all within a single workflow.</li><li><strong>Expanded ecosystem</strong>: thanks to <a href="https://blog.n8n.io/community-nodes-available-on-n8n-cloud/"><u>verified community nodes</u></a>, you can now access specialized integrations directly from the canvas, including tools for <a href="https://qdrant.tech/documentation/platforms/n8n/"><u>domain-specific autonomous workflows</u></a>.</li></ul><h3 id="building-your-first-autonomous-agent-in-n8n">Building your first autonomous agent in n8n</h3><p>Creating an AI agent in n8n is a straightforward process:</p><ol><li>Start with a trigger that fits your use case (webhook, schedule, database change, etc.)</li><li>Add an AI Agent node connected to your chosen LLM</li><li>Equip it with custom tools for specific actions &#x2013; web scrapers, database queries, API calls, file processors</li></ol><p>If necessary:</p><ul><li>Add evaluation checkpoints to monitor performance</li><li>Include <a href="https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.sendemail/#waiting-for-a-response"><u>human-in-the-loop</u></a> feature when business judgment is critical (Send and Wait for a Response operation in various nodes)</li></ul><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Our recommendation</strong></b>: start with a pre-built agent that covers 80% of your use case, then see if the remaining 20% justifies the development of a custom solution. For most businesses, that threshold comes faster than expected.</div></div><p>Here&#x2019;s a 2-part series on creating AI agents:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/yzvLfHb0nqE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Building AI Agents: Chat Trigger, Memory, and System/User Messages Explained [Part 1]"></iframe></figure><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/lkudvrC3AOU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Building AI Agents: AI App Tools and Guardrails explained, Gmail Draft Agent [Part 2]"></iframe></figure><p>Explore community-built agents in the template gallery:</p>
<!--kg-card-begin: html-->
<script> workflowBanner ([2621,2878,6941,5819], document.currentScript, {workflowsHeader: "Community-built n8n AI agents" }); </script>
<!--kg-card-end: html-->
<h2 id="wrap-up">Wrap up</h2><p>We explored 12 autonomous AI agents covering a vast automation spectrum:</p><ul><li><strong>Business-ready solutions</strong>: Lindy AI and Relevance AI offer quick deployment for common workflows, while specialized agents like Clay (sales) and HubSpot Breeze (CRM) excel in their target domains.</li><li><strong>Communication agents</strong>: SalesCloser AI handles sales conversations autonomously, while VAPI provides the infrastructure for building custom voice applications.</li><li><strong>Enterprise tools</strong>: Harvey AI transforms legal workflows, Box AI Agents manage document workflows at scale and are perfect for companies with high compliance requirements.</li><li><strong>Developer infrastructure</strong>: Browserbase Director automates web interactions, legacy-use modernizes old systems, Droidrun controls mobile devices, and Claude Code handles complete feature development.</li></ul><p>Most excel in their niches&#x2014;but what if you need an agent that processes invoices, updates multiple systems, and routes complex cases to the right people?</p><figure class="kg-card kg-image-card"><img src="https://blog.n8n.io/content/images/2025/08/screenshot-NLrv3heH.png" class="kg-image" alt="12 Best Autonomous AI Agents &#x2013; 2025&#x2019;s Top Picks" loading="lazy" width="1156" height="600" srcset="https://blog.n8n.io/content/images/size/w600/2025/08/screenshot-NLrv3heH.png 600w, https://blog.n8n.io/content/images/size/w1000/2025/08/screenshot-NLrv3heH.png 1000w, https://blog.n8n.io/content/images/2025/08/screenshot-NLrv3heH.png 1156w" sizes="(min-width: 720px) 720px"></figure><p></p><p><strong>That&#x2019;s where n8n comes in: </strong>it lets you build fully autonomous agents that combine multiple capabilities with the exact level of human oversight you want, while integrating seamlessly with specialized tools for advanced needs.</p>
<!--kg-card-begin: html-->
<div class="content-banner">
  <div>
    <h3>Ready to experience the difference autonomous agents can make?</h3>
    <p>Build an agent that works exactly how your business needs it to!</p>
  </div>
  <a href="https://app.n8n.cloud/register" class="global-button blog-banner-signup">Try n8n now</a>
</div>
<!--kg-card-end: html-->
<h2 id="what%E2%80%99s-next">What&#x2019;s next?</h2><p>Ready to dive deeper into the world of autonomous AI agents? Here are your next steps:</p><ul><li><strong>Master the foundations</strong>: learn about <a href="https://blog.n8n.io/ai-agent-frameworks/"><u>AI agent frameworks and when to choose each one</u></a> - from no-code builders to programming-first solutions that give you complete control.</li><li><strong>Check out real-world applications</strong>: explore <a href="https://blog.n8n.io/ai-agents-examples/"><u>15 practical AI agent examples</u></a> across industries like finance, healthcare, and e-commerce to develop ideas for your own use cases.</li><li><strong>Build with confidence</strong>: follow our <a href="https://blog.n8n.io/how-to-build-ai-agent/"><u>step-by-step tutorial for building production-ready AI agents</u></a> with evaluation frameworks and reliability patterns that work at scale.</li><li><strong>Go open-source</strong>: discover <a href="https://blog.n8n.io/open-source-ai-tools/"><u>11 game-changing open-source AI tools</u></a> that let you build autonomous agents without vendor lock-in or unpredictable costs.</li><li><strong>Enterprise deployment</strong>: read our guide to <a href="https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/"><u>AI workflows for cautious enterprises</u></a> to implement autonomous agents with proper governance, security, and compliance controls.</li><li><strong>Explore AI integrations</strong>: browse <a href="https://n8n.io/integrations/categories/ai/"><u>n8n&#x2019;s complete AI nodes catalog</u></a> to see all available services and tools you can connect in your autonomous agent workflows.</li></ul><p>Finally, <a href="https://n8n.io/ai/"><u>explore n8n&#x2019;s AI ecosystem</u></a> and <a href="https://n8n.io/workflows/categories/ai/"><u>browse the community AI workflows</u></a> to see what&#x2019;s possible when you have complete control over your autonomous agent architecture.</p>]]></content:encoded></item></channel></rss>